{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 11:48:57,737 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-30 11:49:01,854] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2024-12-30 11:49:01,855] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))\n",
    "    \n",
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))\n",
    "\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/shayan/Desktop/aimet/Examples/torch/quantization/'\n",
    "import sys\n",
    "sys.path.append(\"/home/shayan/Desktop/aimet/\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer\n",
    "from Examples.torch.utils.image_net_data_loader import ImageNetDataLoader\n",
    "\n",
    "sys.path.remove(\"/home/shayan/Desktop/aimet/\")\n",
    "\n",
    "class ImageNetDataPipeline:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_val_dataloader() -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Instantiates a validation dataloader for ImageNet dataset and returns it\n",
    "        \"\"\"\n",
    "        data_loader = ImageNetDataLoader(DATASET_DIR,\n",
    "                                         image_size=image_net_config.dataset['image_size'],\n",
    "                                         batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                         is_training=False,\n",
    "                                         num_workers=image_net_config.evaluation['num_workers']).data_loader\n",
    "        return data_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(model: torch.nn.Module, use_cuda: bool) -> float:\n",
    "        \"\"\"\n",
    "        Given a torch model, evaluates its Top-1 accuracy on the dataset\n",
    "        :param model: the model to evaluate\n",
    "        :param iterations: the number of batches to be used to evaluate the model. A value of 'None' means the model will be\n",
    "                           evaluated on the entire dataset once.\n",
    "        :param use_cuda: whether or not the GPU should be used.\n",
    "        \"\"\"\n",
    "        evaluator = ImageNetEvaluator(DATASET_DIR, image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations=None, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    data_loader = ImageNetDataPipeline.get_val_dataloader()\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for path, input_data, target_data in data_loader:\n",
    "            # if \"cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa\" not in path[0]:\n",
    "            #     continue\n",
    "            # if \"cf\" in path[0]:\n",
    "            print(path)\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 11:49:02,621 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-30 11:49:02,622 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-30 11:49:02,623 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "('/home/shayan/Desktop/aimet/Examples/torch/quantization/val/9/cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa.jpeg',)\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "from copy import deepcopy\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=deepcopy(model),\n",
    "                           quant_scheme=QuantScheme.post_training_tf,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8,\n",
    "                           config_file=\"/home/shayan/Desktop/aimet/my_config.json\")\n",
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim.export(\"./temp\", filename_prefix=\"model\", dummy_input=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "----------------------------------------------------------\n",
      "Layer: conv1\n",
      "  Input[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.231783390045166, max=2.231783390045166, delta=0.017573097559410757, offset=-127.0\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.39387544989585876, max=0.39387544989585876, delta=0.0031013814952429823, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.6932891607284546, max=0.6932891607284546, delta=0.005458969769515391, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-3.4561045169830322, max=3.4561045169830322, delta=0.027213421393567184, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: maxpool\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-3.4561045169830322, max=3.4561045169830322, delta=0.027213421393567184, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.3745041489601135, max=0.3745041489601135, delta=0.002948851566615067, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.1022106409072876, max=1.1022106409072876, delta=0.008678823944151872, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-3.4839067459106445, max=3.4839067459106445, delta=0.02743233658197358, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.7707811594009399, max=0.7707811594009399, delta=0.006069142987408976, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.7853344678878784, max=1.7853344678878784, delta=0.014057751715652586, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.8630332946777344, max=2.8630332946777344, delta=0.02254356924943098, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.2799574136734009, max=0.2799574136734009, delta=0.0022043890840425266, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.2079195976257324, max=1.2079195976257324, delta=0.009511177934060885, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-4.982845306396484, max=4.982845306396484, delta=0.039235002412570746, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer1.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.0473909378051758, max=1.0473909378051758, delta=0.008247172738623432, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.170573115348816, max=1.170573115348816, delta=0.009217111144478865, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-3.725557565689087, max=3.725557565689087, delta=0.029335098942433756, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.21269579231739044, max=0.21269579231739044, delta=0.0016747700182471689, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.7400765419006348, max=0.7400765419006348, delta=0.005827374345674289, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.891108274459839, max=2.891108274459839, delta=0.022764632082360937, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.7241502404212952, max=0.7241502404212952, delta=0.005701970397018072, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.446347951889038, max=1.446347951889038, delta=0.011388566550307386, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.7933433055877686, max=2.7933433055877686, delta=0.021994829177856445, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.0.downsample.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.6922507882118225, max=0.6922507882118225, delta=0.005450793607967106, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.1103509664535522, max=1.1103509664535522, delta=0.008742920995697262, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.690948724746704, max=1.690948724746704, delta=0.013314556887769323, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.3110506236553192, max=0.3110506236553192, delta=0.0024492175090970016, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.8257396221160889, max=0.8257396221160889, delta=0.00650188678831566, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-4.6385064125061035, max=4.6385064125061035, delta=0.036523672539418135, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer2.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.8770784139633179, max=0.8770784139633179, delta=0.006906129243805652, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.159238576889038, max=1.159238576889038, delta=0.009127862810149906, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.471842050552368, max=2.471842050552368, delta=0.01946332323269581, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.23571734130382538, max=0.23571734130382538, delta=0.001856042057510436, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.8612890243530273, max=0.8612890243530273, delta=0.00678180334136242, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.1018495559692383, max=2.1018495559692383, delta=0.016549996503694788, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.563919186592102, max=0.563919186592102, delta=0.004440308555843323, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.6091494560241699, max=0.6091494560241699, delta=0.004796452409639133, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.2017791271209717, max=2.2017791271209717, delta=0.017336843520637572, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.0.downsample.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.4081612229347229, max=0.4081612229347229, delta=0.0032138678971238024, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.3355441093444824, max=0.3355441093444824, delta=0.002642079601137657, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.1659480333328247, max=1.1659480333328247, delta=0.009180693175849013, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.27238303422927856, max=0.27238303422927856, delta=0.0021447483010179416, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.8067159056663513, max=0.8067159056663513, delta=0.006352093745404341, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.718599319458008, max=2.718599319458008, delta=0.02140629385400006, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer3.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.9701208472251892, max=0.9701208472251892, delta=0.007638746828544797, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.061082124710083, max=1.061082124710083, delta=0.008354977359921913, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.9392735958099365, max=2.9392735958099365, delta=0.023143886581180603, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.30162283778190613, max=0.30162283778190613, delta=0.002374982974660678, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.564267635345459, max=0.564267635345459, delta=0.004443052246814638, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-4.400547504425049, max=4.400547504425049, delta=0.03464998034980354, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.1437747478485107, max=1.1437747478485107, delta=0.009006100376759927, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.1255919933319092, max=1.1255919933319092, delta=0.008862929081353615, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-3.4736530780792236, max=3.4736530780792236, delta=0.027351599039993887, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.0.downsample.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.9982079863548278, max=0.9982079863548278, delta=0.007859905404368723, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.7599548697471619, max=0.7599548697471619, delta=0.005983896612182377, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-1.438810110092163, max=1.438810110092163, delta=0.011329213465292623, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.1.conv1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.2932627201080322, max=0.2932627201080322, delta=0.0023091552764411987, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.8511659502983093, max=0.8511659502983093, delta=0.006702094096837081, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.1.relu\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-14.905149459838867, max=14.905149459838867, delta=0.11736338157353439, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: layer4.1.conv2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-3.648308038711548, max=3.648308038711548, delta=0.028726834950484628, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-2.225013017654419, max=2.225013017654419, delta=0.017519787540585977, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-12.381731033325195, max=12.381731033325195, delta=0.09749394514429288, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: avgpool\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-5.364097595214844, max=5.364097595214844, delta=0.04223698893869956, offset=-127.0\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: fc\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.7152369618415833, max=0.7152369618415833, delta=0.005631787101114828, offset=-127.0\n",
      "  -------\n",
      "  Param[bias]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.061649736016988754, max=0.061649736016988754, delta=0.000485430992259754, offset=-127.0\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=True\n",
      "    StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-11.122593879699707, max=11.122593879699707, delta=0.08757947936771422, offset=-127.0\n",
      "  -------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = lambda min, delta: (min) / delta\n",
    "delta = lambda max, min, bitwidth: (max - min) / (2 ** bitwidth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030892192148694805 == 0.0031013814952429823 and -127.5 == -127.0\n",
      "encoding_min=-0.39387544989585876, encoding_max=0.39387544989585876, encoding_delta=0.0031013814952429823\n",
      "encoding_offset=-127.0, encoding_bitwidth=8\n"
     ]
    }
   ],
   "source": [
    "encoding = list(sim._get_qc_quantized_layers(sim.model)[0][1].param_quantizers.values())[0]._encoding[0]\n",
    "encoding_min = encoding.min\n",
    "encoding_max = encoding.max\n",
    "encoding_delta = encoding.delta\n",
    "encoding_offset = encoding.offset\n",
    "encoding_bitwidth = list(sim._get_qc_quantized_layers(sim.model)[0][1].param_quantizers.values())[0].bitwidth\n",
    "\n",
    "my_delta = delta(encoding_max, encoding_min, encoding_bitwidth)\n",
    "my_offset = offset(encoding_min, my_delta)\n",
    "\n",
    "print(f\"{my_delta} == {encoding_delta} and {my_offset} == {encoding_offset}\")\n",
    "print(f\"{encoding_min=}, {encoding_max=}, {encoding_delta=}\")\n",
    "print(f\"{encoding_offset=}, {encoding_bitwidth=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for Grid Search implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabelledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,):\n",
    "        self.dataset = ImageNetDataPipeline.get_val_dataloader().dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 0.0, 'max': 10, 'offset': 0.0, 'delta': 0.0392156862745098}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = lambda min, delta: min / delta\n",
    "delta = lambda max, min, bitwidth: (max - min) / (2 ** bitwidth - 1)\n",
    "\n",
    "def compute_encoding(min, max, bw=8):\n",
    "    enc_delta = delta(max, min, bw)\n",
    "    enc_offset = offset(min, enc_delta)\n",
    "    return {\"min\": min, \"max\": max, \"offset\": enc_offset, \"delta\": enc_delta}\n",
    "\n",
    "def set_encoding_for_layer(layer, weight_encoding: dict, bias_encoding: dict=None, input_encoding: dict=None, output_encoding: dict=None):\n",
    "    if weight_encoding:\n",
    "        layer.param_quantizers['weight'].encoding.max = weight_encoding['max']\n",
    "        layer.param_quantizers['weight'].encoding.min = weight_encoding['min']\n",
    "        layer.param_quantizers['weight'].encoding.delta = weight_encoding['delta']\n",
    "        layer.param_quantizers['weight'].encoding.offset = weight_encoding['offset']\n",
    "\n",
    "    if bias_encoding:\n",
    "        layer.param_quantizers['bias'].encoding.max = bias_encoding['max']\n",
    "        layer.param_quantizers['bias'].encoding.min = bias_encoding['min']\n",
    "        layer.param_quantizers['bias'].encoding.delta = bias_encoding['delta']\n",
    "        layer.param_quantizers['bias'].encoding.offset = bias_encoding['offset']\n",
    "            \n",
    "    if input_encoding:\n",
    "        for input_quantizer in layer.input_quantizers:\n",
    "            input_quantizer.encoding.max = input_encoding['max']\n",
    "            input_quantizer.encoding.min = input_encoding['min']\n",
    "            input_quantizer.encoding.delta = input_encoding['delta']\n",
    "            input_quantizer.encoding.offset = input_encoding['offset']\n",
    "    \n",
    "    if output_encoding:\n",
    "        for output_quantizer in layer.output_quantizers:\n",
    "            output_quantizer.encoding.max = output_encoding['max']\n",
    "            output_quantizer.encoding.min = output_encoding['min']\n",
    "            output_quantizer.encoding.delta = output_encoding['delta']\n",
    "            output_quantizer.encoding.offset = output_encoding['offset']\n",
    "        \n",
    "    return layer\n",
    "    \n",
    "compute_encoding(0.0, 10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quant_wrapper.input_quantizers[0].encoding.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from aimet_torch import utils\n",
    "num_batches = 1\n",
    "\n",
    "def _compute_mse_loss(module: torch.nn.Module, quant_wrapper: torch.nn.Module,\n",
    "                        fp32_model: torch.nn.Module, sim: QuantizationSimModel, verbose=False) -> float:\n",
    "    \"\"\"\n",
    "    Compute MSE loss between fp32 and quantized output activations for each batch, add for\n",
    "    all the batches and return averaged mse loss.\n",
    "\n",
    "    :param module: module from the fp32_model.\n",
    "    :param quant_wrapper: Corresponding quant wrapper from the QuantSim model.\n",
    "    :param fp32_model: PyTorch model.\n",
    "    :param sim: Quantsim model.\n",
    "    :return: MSE loss between fp32 and quantized output activations.\n",
    "    \"\"\"\n",
    "    # output activations collector.\n",
    "    orig_module_collector = utils.ModuleData(fp32_model, module)\n",
    "    quant_module_collector = utils.ModuleData(sim.model, quant_wrapper)\n",
    "    \n",
    "    if verbose:\n",
    "        weight = quant_module_collector._module.param_quantizers['weight']\n",
    "        bias = quant_module_collector._module.param_quantizers['bias']\n",
    "        inp = quant_module_collector._module.input_quantizers\n",
    "        outs = quant_module_collector._module.output_quantizers\n",
    "        \n",
    "        print(\"WEIGHT\")    \n",
    "        print(weight)\n",
    "        print(\"BIAS\")\n",
    "        print(bias)\n",
    "        print(\"INPUT\")\n",
    "        for i in inp:\n",
    "            print(i)\n",
    "        print(\"OUTPUT\")\n",
    "        for i in outs:\n",
    "            print(i)\n",
    "    total = 0\n",
    "    loss = 0.0\n",
    "    batch_index = 0\n",
    "    unlabeled_dataset_iterable = torch.utils.data.DataLoader(UnlabelledDataset(), batch_size=1, shuffle=False)\n",
    "    for model_inputs in unlabeled_dataset_iterable:\n",
    "        assert isinstance(model_inputs, (torch.Tensor, tuple, list))\n",
    "        with torch.no_grad():\n",
    "            _, quantized_out_acts = quant_module_collector.collect_inp_out_data(model_inputs,\n",
    "                                                                                collect_input=False,\n",
    "                                                                                collect_output=True)\n",
    "            _, fp32_out_acts = orig_module_collector.collect_inp_out_data(model_inputs,\n",
    "                                                                            collect_input=False,\n",
    "                                                                            collect_output=True)\n",
    "        loss += torch.nn.functional.mse_loss(fp32_out_acts, quantized_out_acts).item()\n",
    "        total += fp32_out_acts.size(0)\n",
    "        batch_index += 1\n",
    "        if batch_index == num_batches:\n",
    "            break\n",
    "\n",
    "    average_loss = loss/total\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "-----------------------------\n",
      "fc\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "fp32_name, fp32_module = list(dict(model.named_modules()).items())[-1]\n",
    "quant_name, quant_wrapper = list(dict(sim.model.named_modules()).items())[-2]\n",
    "\n",
    "print(fp32_module)\n",
    "print(quant_wrapper)\n",
    "print(\"-----------------------------\")\n",
    "print(fp32_name)\n",
    "print(quant_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-11.122593879699707, max=11.122593879699707, delta=0.08757947936771422, offset=-127.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[k for k in quant_wrapper.__dict__.keys() if \"quantizers\" in k]\n",
    "print(quant_wrapper.output_quantizers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11874568462371826"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_compute_mse_loss(fp32_module, quant_wrapper, model, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Weight, Biases, etc at same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            weight_maximum = 0.7152369618415833 \n",
      " [0.35761848 0.36995015 0.38228182 0.3946135  0.40694517 0.41927684\n",
      " 0.43160851 0.44394018 0.45627185 0.46860353 0.4809352  0.49326687\n",
      " 0.50559854 0.51793021 0.53026189 0.54259356 0.55492523 0.5672569\n",
      " 0.57958857 0.59192024 0.60425192 0.61658359 0.62891526 0.64124693\n",
      " 0.6535786  0.66591027 0.67824195 0.69057362 0.70290529 0.71523696] \n",
      " \n",
      "\n",
      "            bias_maximum = 0.061649736016988754 \n",
      " [0.03082487 0.03188779 0.03295072 0.03401365 0.03507657 0.0361395\n",
      " 0.03720243 0.03826535 0.03932828 0.04039121 0.04145413 0.04251706\n",
      " 0.04357999 0.04464291 0.04570584 0.04676877 0.04783169 0.04889462\n",
      " 0.04995754 0.05102047 0.0520834  0.05314632 0.05420925 0.05527218\n",
      " 0.0563351  0.05739803 0.05846096 0.05952388 0.06058681 0.06164974] \n",
      " \n",
      "\n",
      "            input_maximum = None \n",
      " [ 5.56129694  5.7530658   5.94483466  6.13660352  6.32837238  6.52014124\n",
      "  6.7119101   6.90367896  7.09544782  7.28721668  7.47898554  7.6707544\n",
      "  7.86252326  8.05429212  8.24606098  8.43782984  8.6295987   8.82136756\n",
      "  9.01313642  9.20490528  9.39667414  9.588443    9.78021186  9.97198072\n",
      " 10.16374958 10.35551844 10.5472873  10.73905616 10.93082502 11.12259388] \n",
      " \n",
      "\n",
      "            output_maximum = 11.122593879699707 \n",
      " None \n",
      " \n",
      "\n",
      "            \n",
      "Original loss: 0.11874568462371826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:24<00:00,  2.81s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.81s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.80s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.83s/it]\n",
      "100%|██████████| 30/30 [01:27<00:00,  2.93s/it]\n",
      "100%|██████████| 30/30 [01:26<00:00,  2.90s/it]\n",
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n",
      " 33%|███▎      | 10/30 [14:10<28:31, 85.59s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def make_ranges(max, n=30, max_diff=0.1):\n",
    "    max_diff = max * 0.5\n",
    "    return np.linspace(max-max_diff if max-max_diff >= 0 else 0, max, n)\n",
    "\n",
    "def check_encodings_present(quant_wrapper):\n",
    "    encodings_present = {\"weight\": False, \"bias\": False, \"input\": False, \"output\": False}\n",
    "    if quant_wrapper.param_quantizers['weight']._encoding:\n",
    "        encodings_present['weight'] = True\n",
    "    if quant_wrapper.param_quantizers['bias']._encoding:\n",
    "        encodings_present['bias'] = True\n",
    "    if quant_wrapper.input_quantizers[0]._encoding:\n",
    "        encodings_present['input'] = True\n",
    "    if quant_wrapper.output_quantizers[0]._encoding:\n",
    "        encodings_present['output'] = True\n",
    "    return encodings_present\n",
    "\n",
    "def grid_search(sim, model, fp32_module, quant_wrapper, verbose=False):\n",
    "    encodings_present = check_encodings_present(quant_wrapper)\n",
    "    encoding_max = {k: None for k in encodings_present.keys()}\n",
    "    \n",
    "    # if encodings_present[\"weight\"]:\n",
    "    #     encoding_max[\"weight\"] = quant_wrapper.param_quantizers['weight']._encoding[0].max\n",
    "    # if encodings_present[\"bias\"]:\n",
    "    #     encoding_max[\"bias\"] = quant_wrapper.param_quantizers['bias']._encoding[0].max\n",
    "    # if encodings_present[\"input\"]:\n",
    "    #     encoding_max[\"input\"] = quant_wrapper.input_quantizers[0]._encoding[0].max\n",
    "    # if encodings_present[\"output\"]:\n",
    "    #     encoding_max[\"output\"] = [out._encoding for out in quant_wrapper.output_quantizers if out._encoding]\n",
    "    \n",
    "    weight_maximum = quant_wrapper.param_quantizers['weight']._encoding[0].max \n",
    "    bias_maximum = quant_wrapper.param_quantizers['bias']._encoding[0].max\n",
    "    # input_maximum = quant_wrapper.input_quantizers[0]._encoding[0].max\n",
    "    output_maximum = [out._encoding for out in quant_wrapper.output_quantizers if out._encoding][0][0].max\n",
    "    weight_range = make_ranges(weight_maximum)\n",
    "    bias_range = make_ranges(bias_maximum)\n",
    "    input_range = make_ranges(output_maximum)\n",
    "    \n",
    "    orig_loss = _compute_mse_loss(fp32_module, quant_wrapper, model, sim)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "            weight_maximum = {weight_maximum} \\n {weight_range} \\n \\n\n",
    "            bias_maximum = {bias_maximum} \\n {bias_range} \\n \\n\n",
    "            input_maximum = {None} \\n {input_range} \\n \\n\n",
    "            output_maximum = {output_maximum} \\n None \\n \\n\n",
    "            \"\"\")\n",
    "        print(f\"Original loss: {orig_loss}\")\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_params = {\"weight\": weight_maximum, \"bias\": bias_maximum, \"output\": output_maximum}\n",
    "    all_logs = []\n",
    "    for weight in tqdm(weight_range):\n",
    "        for bias in tqdm(bias_range):\n",
    "            for inp in input_range:\n",
    "                weight_encoding = compute_encoding(-weight, weight)\n",
    "                bias_encoding = compute_encoding(-bias, bias)\n",
    "                input_encoding = compute_encoding(-inp, inp)\n",
    "                set_encoding_for_layer(quant_wrapper, weight_encoding, bias_encoding, output_encoding=input_encoding)\n",
    "                loss = _compute_mse_loss(fp32_module, quant_wrapper, model, sim)\n",
    "                all_logs.append({\"weight\": weight, \"bias\": bias, \"output\": inp, \"loss\": loss})\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_params = {\"weight\": weight, \"bias\": bias, \"output\": inp}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"New best loss: {best_loss} with params: {best_params}\")\n",
    "        if weight_maximum != best_params['weight']:\n",
    "            print(f\"Weight maximum changed from {weight_maximum} to {best_params['weight']}\")\n",
    "        if bias_maximum != best_params['bias']:\n",
    "            print(f\"Bias maximum changed from {bias_maximum} to {best_params['bias']}\")\n",
    "        if output_maximum != best_params['output']:\n",
    "            print(f\"output maximum changed from {output_maximum} to {best_params['output']}\")\n",
    "    \n",
    "    return best_loss, best_params, all_logs\n",
    "    \n",
    "best_loss, best_params, all_logs =  grid_search(sim, model, fp32_module, quant_wrapper, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fc'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "output_dir = \"./logs/\"\n",
    "output_dir = os.path.join(output_dir, fp32_name)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "pd.DataFrame(all_logs).to_csv(os.path.join(output_dir, \"logs.csv\"))\n",
    "\n",
    "with open(os.path.join(output_dir, \"best_params.json\"), \"w\") as f:\n",
    "    best_params.update({\"best_loss\": best_loss})\n",
    "    json.dump(best_params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            weight_maximum = 0.39387544989585876 \n",
      " [0.29387545 0.30101831 0.30816116 0.31530402 0.32244688 0.32958974\n",
      " 0.33673259 0.34387545 0.35101831 0.35816116 0.36530402 0.37244688\n",
      " 0.37958974 0.38673259 0.39387545] \n",
      " \n",
      "\n",
      "            bias_maximum = 0.6932891607284546 \n",
      " [0.59328916 0.60043202 0.60757488 0.61471773 0.62186059 0.62900345\n",
      " 0.6361463  0.64328916 0.65043202 0.65757488 0.66471773 0.67186059\n",
      " 0.67900345 0.6861463  0.69328916] \n",
      " \n",
      "\n",
      "            input_maximum = 2.231783390045166 \n",
      " [2.13178339 2.13892625 2.1460691  2.15321196 2.16035482 2.16749768\n",
      " 2.17464053 2.18178339 2.18892625 2.1960691  2.20321196 2.21035482\n",
      " 2.21749768 2.22464053 2.23178339] \n",
      " \n",
      "\n",
      "            output_maximum = [] \n",
      " None \n",
      " \n",
      "\n",
      "            \n",
      "Original loss: 0.00016707682516425848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 11.42it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.88it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.84it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.99it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.92it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.13it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.20it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.95it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.05it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.86it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.02it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.79it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.96it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 11.93it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00, 12.04it/s]\n",
      "100%|██████████| 15/15 [00:18<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best loss: 4.611624899553135e-05 with params: {'weight': 0.39387544989585876, 'bias': 0.6861463035855975, 'input': 2.1746405329023086}\n",
      "Bias maximum changed from 0.6932891607284546 to 0.6861463035855975\n",
      "Input maximum changed from 2.231783390045166 to 2.1746405329023086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def make_ranges(max, n=15, max_diff=0.1):\n",
    "    return np.linspace(max-max_diff if max-max_diff >= 0 else 0, max, n)\n",
    "\n",
    "def grid_search(sim, model, fp32_module, quant_wrapper, verbose=False):\n",
    "    weight_maximum = quant_wrapper.param_quantizers['weight']._encoding[0].max\n",
    "    bias_maximum = quant_wrapper.param_quantizers['bias']._encoding[0].max\n",
    "    input_maximum = quant_wrapper.input_quantizers[0]._encoding[0].max\n",
    "    output_maximum = [out._encoding for out in quant_wrapper.output_quantizers if out._encoding]\n",
    "    \n",
    "    weight_range = make_ranges(weight_maximum)\n",
    "    bias_range = make_ranges(bias_maximum)\n",
    "    input_range = make_ranges(input_maximum)\n",
    "    \n",
    "    orig_loss = _compute_mse_loss(fp32_module, quant_wrapper, model, sim)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "            weight_maximum = {weight_maximum} \\n {weight_range} \\n \\n\n",
    "            bias_maximum = {bias_maximum} \\n {bias_range} \\n \\n\n",
    "            input_maximum = {input_maximum} \\n {input_range} \\n \\n\n",
    "            output_maximum = {output_maximum} \\n None \\n \\n\n",
    "            \"\"\")\n",
    "        print(f\"Original loss: {orig_loss}\")\n",
    "        \n",
    "    best_loss = float('inf')\n",
    "    best_params = {\"weight\": weight_maximum, \"bias\": bias_maximum, \"input\": input_maximum}\n",
    "    for weight in tqdm(weight_range):\n",
    "        for bias in tqdm(bias_range):\n",
    "            for inp in input_range:\n",
    "                weight_encoding = compute_encoding(-weight, weight)\n",
    "                bias_encoding = compute_encoding(-bias, bias)\n",
    "                input_encoding = compute_encoding(-inp, inp)\n",
    "                set_encoding_for_layer(quant_wrapper, weight_encoding, bias_encoding, input_encoding)\n",
    "                loss = _compute_mse_loss(fp32_module, quant_wrapper, model, sim)\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_params = {\"weight\": weight, \"bias\": bias, \"input\": inp}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"New best loss: {best_loss} with params: {best_params}\")\n",
    "        if weight_maximum != best_params['weight']:\n",
    "            print(f\"Weight maximum changed from {weight_maximum} to {best_params['weight']}\")\n",
    "        if bias_maximum != best_params['bias']:\n",
    "            print(f\"Bias maximum changed from {bias_maximum} to {best_params['bias']}\")\n",
    "        if input_maximum != best_params['input']:\n",
    "            print(f\"Input maximum changed from {input_maximum} to {best_params['input']}\")\n",
    "        \n",
    "    \n",
    "grid_search(sim, model, fp32_module, quant_wrapper, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-30 11:00:24,901 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-12-30 11:00:24,901 - Quant - INFO - Unsupported op type Mean\n",
      "2024-12-30 11:00:24,903 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "('/home/shayan/Desktop/aimet/Examples/torch/quantization/val/9/cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa.jpeg',)\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "from copy import deepcopy\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "tf_enhanced_sim = QuantizationSimModel(model=deepcopy(model),\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8,\n",
    "                           config_file=\"/home/shayan/Desktop/aimet/my_config.json\")\n",
    "\n",
    "tf_enhanced_sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "-----------------------------\n",
      "fc\n",
      "-----------------------------\n",
      "WEIGHT\n",
      "StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf_enhanced, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.7204298377037048, max=0.7204298377037048, delta=0.005672676023095846, offset=-127.0\n",
      "\n",
      "BIAS\n",
      "StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf_enhanced, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-0.06204679608345032, max=0.06204679608345032, delta=0.0004885574453510344, offset=-127.0\n",
      "\n",
      "INPUT\n",
      "StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf_enhanced, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=False\n",
      "    no encoding\n",
      "\n",
      "OUTPUT\n",
      "StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf_enhanced, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-12.294072151184082, max=12.294072151184082, delta=0.09680371731519699, offset=-127.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quant_name, quant_wrapper = list(dict(tf_enhanced_sim.model.named_modules()).items())[-2\n",
    "                                                                                    ]\n",
    "\n",
    "print(quant_wrapper)\n",
    "print(\"-----------------------------\")\n",
    "print(quant_name)\n",
    "print(\"-----------------------------\")\n",
    "print(\"WEIGHT\")\n",
    "print(quant_wrapper.param_quantizers['weight'])\n",
    "print(\"BIAS\")\n",
    "print(quant_wrapper.param_quantizers['bias'])\n",
    "print(\"INPUT\")\n",
    "for i in quant_wrapper.input_quantizers:\n",
    "    print(i)\n",
    "print(\"OUTPUT\")\n",
    "for i in quant_wrapper.output_quantizers:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
