{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 08:15:07,651 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-07 08:15:11,725] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-07 08:15:11,726] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))\n",
    "    \n",
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))\n",
    "\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/shayan/Desktop/aimet/Examples/torch/quantization/'\n",
    "import sys\n",
    "sys.path.append(\"/home/shayan/Desktop/temp/aimet/\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer\n",
    "from Examples.torch.utils.image_net_data_loader import ImageNetDataLoader\n",
    "\n",
    "sys.path.remove(\"/home/shayan/Desktop/temp/aimet/\")\n",
    "\n",
    "class ImageNetDataPipeline:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_val_dataloader() -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Instantiates a validation dataloader for ImageNet dataset and returns it\n",
    "        \"\"\"\n",
    "        data_loader = ImageNetDataLoader(DATASET_DIR,\n",
    "                                         image_size=image_net_config.dataset['image_size'],\n",
    "                                         batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                         is_training=False,\n",
    "                                         num_workers=image_net_config.evaluation['num_workers']).data_loader\n",
    "        return data_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(model: torch.nn.Module, use_cuda: bool) -> float:\n",
    "        \"\"\"\n",
    "        Given a torch model, evaluates its Top-1 accuracy on the dataset\n",
    "        :param model: the model to evaluate\n",
    "        :param iterations: the number of batches to be used to evaluate the model. A value of 'None' means the model will be\n",
    "                           evaluated on the entire dataset once.\n",
    "        :param use_cuda: whether or not the GPU should be used.\n",
    "        \"\"\"\n",
    "        evaluator = ImageNetEvaluator(DATASET_DIR, image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations=None, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    data_loader = ImageNetDataPipeline.get_val_dataloader()\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for path, input_data, target_data in data_loader:\n",
    "            # if \"cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa\" not in path[0]:\n",
    "            #     continue\n",
    "            # if \"cf\" in path[0]:\n",
    "            print(path)\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 08:15:12,496 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-01-07 08:15:12,496 - Quant - INFO - Unsupported op type Mean\n",
      "2025-01-07 08:15:12,498 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "('/home/shayan/Desktop/aimet/Examples/torch/quantization/val/9/cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa.jpeg',)\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88cc70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88c030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88d9b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b8516349fb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88d970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88b030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88db30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88abb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88dcf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88a4b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88dd70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88b1b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88ddf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b85163394b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88df30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b862ab565b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e0b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bfeb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e4b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf9f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e5b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf5b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e830> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e870> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf0f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88ecf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8beff0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88edb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8beab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f170> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be8b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f230> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be370> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f4b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be1b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f4f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bddf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bdcb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88fa70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bd6b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88fcf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bd530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bcfb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c1f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bce70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c2b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bcab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c530> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc7f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c7b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc4b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc370> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39ceb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc170> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be930> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d2f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d1670> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d4b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d2bf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d9f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d26b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39dbb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d3470> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39de30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d02f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39dff0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d30b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e270> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861cc62170> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e430> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c855ab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c856f70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39eb30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8568f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39f070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c855f70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39f230> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88d930> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c4270> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88cc70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c4470> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88d9b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88c030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c3b63b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c42f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='maxpool' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88d9f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c61b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88d970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b8516349fb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88db30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88b030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88db70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c4c30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88dcf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88abb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88dd70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88a4b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88de30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c40f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88ddf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88b1b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88df30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b85163394b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88df70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c4030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e0b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b862ab565b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e4b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bfeb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e3f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c6cf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e5b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf9f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e830> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf5b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c5db0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88e870> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88ecf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bf0f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88ec30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7430> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.downsample.0' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88edb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8beff0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f170> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8beab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88ef70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c4570> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f230> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be8b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f4b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be370> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f5f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c5d30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f4f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be1b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bddf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88f8b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7130> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88fa70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bdcb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88fcf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bd6b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c88fe30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c76b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bd530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c1f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bcfb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c130> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7b30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.downsample.0' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c2b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bce70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c530> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bcab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c470> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7fb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c7b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc7f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39c970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc4b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39cb70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7eb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39ceb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc370> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8bc170> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39cfb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c66b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d2f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8be930> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d4b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d1670> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d6b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c77f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39d9f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d2bf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39dbb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d26b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39daf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7f30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.downsample.0' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39de30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d3470> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39dff0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d02f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39df30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7f70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e270> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c9d30b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e430> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861cc62170> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e630> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c6c30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39e970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c855ab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39eb30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c856f70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39ea70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7ef0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='avgpool' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39ed30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7c70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='fc' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Linear(in_features=512, out_features=1000, bias=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39f070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8568f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39f230> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c39f170> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7e70> \t is_encoding_valid = True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "from copy import deepcopy\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=deepcopy(model),\n",
    "                           quant_scheme=QuantScheme.post_training_tf,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8,\n",
    "                           config_file=\"/home/shayan/Desktop/aimet/my_config.json\")\n",
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 08:15:12,853 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-01-07 08:15:12,854 - Quant - INFO - Unsupported op type Mean\n",
      "2025-01-07 08:15:12,855 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "('/home/shayan/Desktop/aimet/Examples/torch/quantization/val/9/cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa.jpeg',)\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4118b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8744f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4118f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c874cb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b6370> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411a70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b6bb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411c30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b5ef0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411cb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b5bb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411d30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b4970> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411e70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b6db0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4120b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b4930> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b4fb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412470> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c880ab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4126f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8811b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412730> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c882a30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412bb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c882230> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412c70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c881530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412ef0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c882db0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412fb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7170> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413230> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c6af0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413270> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c74b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4136f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7bb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4137f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7c30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413a70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c6b70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413df0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c888e70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413fb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88bb30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c8bd270> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80f430> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c9f6070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80e3b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c8b37b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80e930> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c452370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80d1f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4140b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80d570> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414330> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80eaf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414430> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce6b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4146b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80f6b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414b30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cc570> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414cf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cc4f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414f70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ccbf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415130> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ccd70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4153b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ccdf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415570> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cd030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415ab0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cd630> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415c70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cd830> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4161b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b84fe3165b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c416370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b5db0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c8cdcf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4118b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce630> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4118f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8744f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411830> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce5b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='maxpool' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4113f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce770> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411970> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c874cb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411a70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b6370> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411ab0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce7b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411c30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b6bb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411cb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b5ef0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411d70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce6f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411d30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b5bb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411e70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b4970> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c411eb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce8b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer1.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4120b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b6db0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b4930> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4122b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce9f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412470> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3b4fb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4126f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c880ab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412830> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cea30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412730> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8811b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412bb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c882a30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412af0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ceab0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.0.downsample.0' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412c70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c882230> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412ef0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c881530> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412e30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ceb30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c412fb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c882db0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413230> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7170> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ceb70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer2.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413270> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c6af0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4136f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c74b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413630> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cebb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4137f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7bb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413a70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c7c30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413bb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cebf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413df0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c3c6b70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413fb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c888e70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c413ef0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cec30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.0.downsample.0' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c8bd270> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c88bb30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c9f6070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80f430> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c451330> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cec70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c8b37b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80e3b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c452370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80e930> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c3c5470> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cecb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer3.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4140b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80d1f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414330> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80d570> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4142b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ced30> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414430> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ce6b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4146b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80eaf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4147f0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cedb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414b30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cc570> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414cf0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cc4f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414c30> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ceeb0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.0.downsample.0' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c414f70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ccbf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415130> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ccd70> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415070> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cf070> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.1.conv1' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4153b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8ccdf0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415570> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cd030> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.1.relu' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): ReLU(inplace=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415770> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cf0b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='layer4.1.conv2' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415ab0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cd630> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415c70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cd830> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415bb0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cf0f0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='avgpool' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c415e70> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cf130> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "name='fc' \t layer=StaticGridQuantWrapper(\n",
      "  (_module_to_wrap): Linear(in_features=512, out_features=1000, bias=True)\n",
      ") \t type(layer)=<class 'aimet_torch.v1.qc_quantize_op.StaticGridQuantWrapper'>\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4161b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c80f6b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c416370> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b84fe3165b0> \t is_encoding_valid = True\n",
      "\n",
      "\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "op = <aimet_common.AimetTensorQuantizer.AimetTensorQuantizer object at 0x7b861c4162b0> \t encoding = <aimet_common._libpymo.TfEncoding object at 0x7b861c8cf170> \t is_encoding_valid = True\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.10889194905757904}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "from copy import deepcopy\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "tf_enhanced_sim = QuantizationSimModel(model=deepcopy(model),\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8,\n",
    "                           config_file=\"/home/shayan/Desktop/aimet/my_config.json\")\n",
    "\n",
    "tf_enhanced_sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)\n",
    "\n",
    "histogram = list(dict(tf_enhanced_sim.model.named_modules()).items())[-2][1].output_quantizers[0].get_stats_histogram()[0]\n",
    "\n",
    "steps = []\n",
    "for idx in range(len(histogram)):\n",
    "    if idx == len(histogram) - 1:\n",
    "        break\n",
    "    steps.append(histogram[idx + 1][0] - histogram[idx][0])\n",
    "    \n",
    "set(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.294080898165703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [(key, value) for key, value in histogram if value > 0]\n",
    "sorted(temp, key=lambda x: x[0], reverse=False)\n",
    "temp[0], temp[-1]\n",
    "abs_max = max(abs(temp[0][0]), abs(temp[-1][0]))\n",
    "abs_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.10889194905757904}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = []\n",
    "for idx in range(len(histogram)):\n",
    "    if idx == len(histogram) - 1:\n",
    "        break\n",
    "    steps.append(histogram[idx + 1][0] - histogram[idx][0])\n",
    "    \n",
    "set(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticGrid TensorQuantizer:\n",
      "    quant-scheme:QuantScheme.post_training_tf, round_mode=RoundingMode.ROUND_NEAREST, bitwidth=8, enabled=True\n",
      "    min:-11.122593879699707, max=11.122593879699707, delta=0.08757947936771422, offset=-127.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list(dict(sim.model.named_modules()).items())[-2][1].output_quantizers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Encoding Max 11.12\n",
      "      Encoding range [ 5.56129694  6.7971407   8.03298447  9.26882823 10.504672   11.74051576\n",
      " 12.97635953 14.21220329 15.44804706 16.68389082]\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "def compute_encoding(min, max, bw=8):\n",
    "    delta = (max - min) / (2 ** bw - 1)\n",
    "    offset = min / delta\n",
    "    return {\"min\": min, \"max\": max, \"offset\": int(offset), \"delta\": delta}\n",
    "\n",
    "N = 10\n",
    "def make_ranges(max, n=N, max_diff=0.1):\n",
    "    max_diff = max * 0.5\n",
    "    return np.linspace(max-max_diff if max-max_diff >= 0 else 0, max + max_diff, n)\n",
    "\n",
    "encoding_max = list(dict(sim.model.named_modules()).items())[-2][1].output_quantizers[0].encoding.max\n",
    "encoding_ranges = make_ranges(encoding_max)\n",
    "encodings = [compute_encoding(-encoding_range, encoding_range) for encoding_range in encoding_ranges]\n",
    "\n",
    "print(f\"\"\"\n",
    "      Encoding Max {round(encoding_max, 2)}\n",
    "      Encoding range {encoding_ranges}\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satAndQuantCostGpt import PDF, _quant_and_sat_cost\n",
    "\n",
    "xleft = [x[0] for x in histogram]\n",
    "pdf = [x[1] for x in histogram]\n",
    "\n",
    "pdf = PDF(xLeft=xleft, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 254\n",
    "test_offset = (- num_steps) / 2\n",
    "delta_max = abs_max / (num_steps / 2) \n",
    "test_candidates = []\n",
    "\n",
    "for f in range(1, 101):\n",
    "    testDelta = 0.01 * f * delta_max\n",
    "    test_candidates.append((testDelta, test_offset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.293613985180855, -24.293613985180855)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "pdf.xLeft[0] + i * steps[0], pdf.xLeft[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_ind=226, max_ind=229, delta=0.0009680378659972995, offset=-127.0\n",
      "min_val=-0.12294080898165705, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.22849324345588684\n",
      "\n",
      "min_ind=225, max_ind=230, delta=0.001936075731994599, offset=-127.0\n",
      "min_val=-0.2458816179633141, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.3373851925134659\n",
      "\n",
      "min_ind=224, max_ind=231, delta=0.0029041135979918985, offset=-127.0\n",
      "min_val=-0.3688224269449711, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.4462771415710449\n",
      "\n",
      "min_ind=223, max_ind=232, delta=0.003872151463989198, offset=-127.0\n",
      "min_val=-0.4917632359266282, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.555169090628624\n",
      "\n",
      "min_ind=222, max_ind=233, delta=0.004840189329986498, offset=-127.0\n",
      "min_val=-0.6147040449082852, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.664061039686203\n",
      "\n",
      "min_ind=221, max_ind=234, delta=0.005808227195983797, offset=-127.0\n",
      "min_val=-0.7376448538899422, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.772952988743782\n",
      "\n",
      "min_ind=220, max_ind=236, delta=0.006776265061981097, offset=-127.0\n",
      "min_val=-0.8605856628715993, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.8818449378013611\n",
      "\n",
      "min_ind=219, max_ind=237, delta=0.007744302927978396, offset=-127.0\n",
      "min_val=-0.9835264718532564, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-0.9907368868589401\n",
      "\n",
      "min_ind=217, max_ind=238, delta=0.008712340793975695, offset=-127.0\n",
      "min_val=-1.1064672808349132, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.2085207849740982\n",
      "\n",
      "min_ind=216, max_ind=239, delta=0.009680378659972996, offset=-127.0\n",
      "min_val=-1.2294080898165705, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.3174127340316772\n",
      "\n",
      "min_ind=215, max_ind=240, delta=0.010648416525970294, offset=-127.0\n",
      "min_val=-1.3523488987982273, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.4263046830892563\n",
      "\n",
      "min_ind=214, max_ind=241, delta=0.011616454391967594, offset=-127.0\n",
      "min_val=-1.4752897077798843, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.5351966321468353\n",
      "\n",
      "min_ind=213, max_ind=242, delta=0.012584492257964893, offset=-127.0\n",
      "min_val=-1.5982305167615414, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.6440885812044144\n",
      "\n",
      "min_ind=212, max_ind=244, delta=0.013552530123962195, offset=-127.0\n",
      "min_val=-1.7211713257431986, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.7529805302619934\n",
      "\n",
      "min_ind=211, max_ind=245, delta=0.014520567989959491, offset=-127.0\n",
      "min_val=-1.8441121347248552, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.8618724793195724\n",
      "\n",
      "min_ind=210, max_ind=246, delta=0.015488605855956792, offset=-127.0\n",
      "min_val=-1.9670529437065127, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-1.9707644283771515\n",
      "\n",
      "min_ind=208, max_ind=247, delta=0.016456643721954094, offset=-127.0\n",
      "min_val=-2.08999375268817, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.1885483264923096\n",
      "\n",
      "min_ind=207, max_ind=248, delta=0.01742468158795139, offset=-127.0\n",
      "min_val=-2.2129345616698264, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.2974402755498886\n",
      "\n",
      "min_ind=206, max_ind=249, delta=0.01839271945394869, offset=-127.0\n",
      "min_val=-2.3358753706514834, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.4063322246074677\n",
      "\n",
      "min_ind=205, max_ind=250, delta=0.019360757319945993, offset=-127.0\n",
      "min_val=-2.458816179633141, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.5152241736650467\n",
      "\n",
      "min_ind=204, max_ind=251, delta=0.02032879518594329, offset=-127.0\n",
      "min_val=-2.5817569886147975, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.6241161227226257\n",
      "\n",
      "min_ind=203, max_ind=253, delta=0.02129683305194059, offset=-127.0\n",
      "min_val=-2.7046977975964546, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.7330080717802048\n",
      "\n",
      "min_ind=202, max_ind=254, delta=0.022264870917937888, offset=-127.0\n",
      "min_val=-2.8276386065781116, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.841900020837784\n",
      "\n",
      "min_ind=201, max_ind=255, delta=0.023232908783935188, offset=-127.0\n",
      "min_val=-2.9505794155597687, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-2.950791969895363\n",
      "\n",
      "min_ind=199, max_ind=256, delta=0.024200946649932487, offset=-127.0\n",
      "min_val=-3.0735202245414257, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.168575868010521\n",
      "\n",
      "min_ind=198, max_ind=257, delta=0.025168984515929787, offset=-127.0\n",
      "min_val=-3.1964610335230828, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.2774678170681\n",
      "\n",
      "min_ind=197, max_ind=258, delta=0.026137022381927087, offset=-127.0\n",
      "min_val=-3.31940184250474, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.386359766125679\n",
      "\n",
      "min_ind=196, max_ind=259, delta=0.02710506024792439, offset=-127.0\n",
      "min_val=-3.4423426514863973, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.495251715183258\n",
      "\n",
      "min_ind=195, max_ind=261, delta=0.028073098113921682, offset=-127.0\n",
      "min_val=-3.5652834604680534, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.604143664240837\n",
      "\n",
      "min_ind=194, max_ind=262, delta=0.029041135979918982, offset=-127.0\n",
      "min_val=-3.6882242694497105, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.713035613298416\n",
      "\n",
      "min_ind=193, max_ind=263, delta=0.030009173845916285, offset=-127.0\n",
      "min_val=-3.811165078431368, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-3.821927562355995\n",
      "\n",
      "min_ind=191, max_ind=264, delta=0.030977211711913585, offset=-127.0\n",
      "min_val=-3.9341058874130255, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.039711460471153\n",
      "\n",
      "min_ind=190, max_ind=265, delta=0.03194524957791089, offset=-127.0\n",
      "min_val=-4.057046696394683, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.148603409528732\n",
      "\n",
      "min_ind=189, max_ind=266, delta=0.03291328744390819, offset=-127.0\n",
      "min_val=-4.17998750537634, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.257495358586311\n",
      "\n",
      "min_ind=188, max_ind=267, delta=0.03388132530990549, offset=-127.0\n",
      "min_val=-4.302928314357997, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.36638730764389\n",
      "\n",
      "min_ind=187, max_ind=269, delta=0.03484936317590278, offset=-127.0\n",
      "min_val=-4.425869123339653, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.475279256701469\n",
      "\n",
      "min_ind=186, max_ind=270, delta=0.03581740104190008, offset=-127.0\n",
      "min_val=-4.54880993232131, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.5841712057590485\n",
      "\n",
      "min_ind=185, max_ind=271, delta=0.03678543890789738, offset=-127.0\n",
      "min_val=-4.671750741302967, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.6930631548166275\n",
      "\n",
      "min_ind=184, max_ind=272, delta=0.03775347677389468, offset=-127.0\n",
      "min_val=-4.794691550284624, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-4.8019551038742065\n",
      "\n",
      "min_ind=182, max_ind=273, delta=0.038721514639891985, offset=-127.0\n",
      "min_val=-4.917632359266282, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.019739001989365\n",
      "\n",
      "min_ind=181, max_ind=274, delta=0.039689552505889285, offset=-127.0\n",
      "min_val=-5.040573168247939, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.128630951046944\n",
      "\n",
      "min_ind=180, max_ind=275, delta=0.04065759037188658, offset=-127.0\n",
      "min_val=-5.163513977229595, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.237522900104523\n",
      "\n",
      "min_ind=179, max_ind=277, delta=0.04162562823788388, offset=-127.0\n",
      "min_val=-5.2864547862112525, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.346414849162102\n",
      "\n",
      "min_ind=178, max_ind=278, delta=0.04259366610388118, offset=-127.0\n",
      "min_val=-5.409395595192909, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.455306798219681\n",
      "\n",
      "min_ind=177, max_ind=279, delta=0.043561703969878476, offset=-127.0\n",
      "min_val=-5.532336404174567, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.56419874727726\n",
      "\n",
      "min_ind=176, max_ind=280, delta=0.044529741835875776, offset=-127.0\n",
      "min_val=-5.655277213156223, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.673090696334839\n",
      "\n",
      "min_ind=175, max_ind=281, delta=0.045497779701873076, offset=-127.0\n",
      "min_val=-5.778218022137881, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.781982645392418\n",
      "\n",
      "min_ind=173, max_ind=282, delta=0.046465817567870375, offset=-127.0\n",
      "min_val=-5.901158831119537, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-5.999766543507576\n",
      "\n",
      "min_ind=172, max_ind=283, delta=0.047433855433867675, offset=-127.0\n",
      "min_val=-6.024099640101195, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.108658492565155\n",
      "\n",
      "min_ind=171, max_ind=284, delta=0.048401893299864975, offset=-127.0\n",
      "min_val=-6.147040449082851, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.217550441622734\n",
      "\n",
      "min_ind=170, max_ind=286, delta=0.049369931165862274, offset=-127.0\n",
      "min_val=-6.269981258064509, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.326442390680313\n",
      "\n",
      "min_ind=169, max_ind=287, delta=0.050337969031859574, offset=-127.0\n",
      "min_val=-6.3929220670461655, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.435334339737892\n",
      "\n",
      "min_ind=168, max_ind=288, delta=0.05130600689785687, offset=-127.0\n",
      "min_val=-6.515862876027823, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.544226288795471\n",
      "\n",
      "min_ind=167, max_ind=289, delta=0.05227404476385417, offset=-127.0\n",
      "min_val=-6.63880368500948, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.65311823785305\n",
      "\n",
      "min_ind=166, max_ind=290, delta=0.05324208262985148, offset=-127.0\n",
      "min_val=-6.761744493991138, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.762010186910629\n",
      "\n",
      "min_ind=164, max_ind=291, delta=0.05421012049584878, offset=-127.0\n",
      "min_val=-6.884685302972795, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-6.979794085025787\n",
      "\n",
      "min_ind=163, max_ind=292, delta=0.05517815836184608, offset=-127.0\n",
      "min_val=-7.007626111954452, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.088686034083366\n",
      "\n",
      "min_ind=162, max_ind=294, delta=0.056146196227843365, offset=-127.0\n",
      "min_val=-7.130566920936107, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.197577983140945\n",
      "\n",
      "min_ind=161, max_ind=295, delta=0.057114234093840664, offset=-127.0\n",
      "min_val=-7.253507729917764, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.3064699321985245\n",
      "\n",
      "min_ind=160, max_ind=296, delta=0.058082271959837964, offset=-127.0\n",
      "min_val=-7.376448538899421, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.4153618812561035\n",
      "\n",
      "min_ind=159, max_ind=297, delta=0.05905030982583527, offset=-127.0\n",
      "min_val=-7.499389347881079, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.524253830313683\n",
      "\n",
      "min_ind=158, max_ind=298, delta=0.06001834769183257, offset=-127.0\n",
      "min_val=-7.622330156862736, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.633145779371262\n",
      "\n",
      "min_ind=156, max_ind=299, delta=0.06098638555782987, offset=-127.0\n",
      "min_val=-7.7452709658443935, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.85092967748642\n",
      "\n",
      "min_ind=155, max_ind=300, delta=0.06195442342382717, offset=-127.0\n",
      "min_val=-7.868211774826051, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-7.959821626543999\n",
      "\n",
      "min_ind=154, max_ind=302, delta=0.06292246128982447, offset=-127.0\n",
      "min_val=-7.9911525838077075, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.068713575601578\n",
      "\n",
      "min_ind=153, max_ind=303, delta=0.06389049915582178, offset=-127.0\n",
      "min_val=-8.114093392789366, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.177605524659157\n",
      "\n",
      "min_ind=152, max_ind=304, delta=0.06485853702181907, offset=-127.0\n",
      "min_val=-8.237034201771023, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.286497473716736\n",
      "\n",
      "min_ind=151, max_ind=305, delta=0.06582657488781637, offset=-127.0\n",
      "min_val=-8.35997501075268, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.395389422774315\n",
      "\n",
      "min_ind=150, max_ind=306, delta=0.06679461275381367, offset=-127.0\n",
      "min_val=-8.482915819734336, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.504281371831894\n",
      "\n",
      "min_ind=149, max_ind=307, delta=0.06776265061981097, offset=-127.0\n",
      "min_val=-8.605856628715994, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.613173320889473\n",
      "\n",
      "min_ind=147, max_ind=308, delta=0.06873068848580827, offset=-127.0\n",
      "min_val=-8.72879743769765, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.830957219004631\n",
      "\n",
      "min_ind=146, max_ind=310, delta=0.06969872635180556, offset=-127.0\n",
      "min_val=-8.851738246679306, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-8.93984916806221\n",
      "\n",
      "min_ind=145, max_ind=311, delta=0.07066676421780287, offset=-127.0\n",
      "min_val=-8.974679055660964, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.04874111711979\n",
      "\n",
      "min_ind=144, max_ind=312, delta=0.07163480208380016, offset=-127.0\n",
      "min_val=-9.09761986464262, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.157633066177368\n",
      "\n",
      "min_ind=143, max_ind=313, delta=0.07260283994979747, offset=-127.0\n",
      "min_val=-9.220560673624279, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.266525015234947\n",
      "\n",
      "min_ind=142, max_ind=314, delta=0.07357087781579476, offset=-127.0\n",
      "min_val=-9.343501482605934, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.375416964292526\n",
      "\n",
      "min_ind=141, max_ind=315, delta=0.07453891568179206, offset=-127.0\n",
      "min_val=-9.466442291587592, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.484308913350105\n",
      "\n",
      "min_ind=140, max_ind=316, delta=0.07550695354778936, offset=-127.0\n",
      "min_val=-9.589383100569249, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.593200862407684\n",
      "\n",
      "min_ind=138, max_ind=317, delta=0.07647499141378666, offset=-127.0\n",
      "min_val=-9.712323909550907, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.810984760522842\n",
      "\n",
      "min_ind=137, max_ind=319, delta=0.07744302927978397, offset=-127.0\n",
      "min_val=-9.835264718532564, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-9.919876709580421\n",
      "\n",
      "min_ind=136, max_ind=320, delta=0.07841106714578126, offset=-127.0\n",
      "min_val=-9.95820552751422, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.028768658638\n",
      "\n",
      "min_ind=135, max_ind=321, delta=0.07937910501177857, offset=-127.0\n",
      "min_val=-10.081146336495879, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.13766060769558\n",
      "\n",
      "min_ind=134, max_ind=322, delta=0.08034714287777586, offset=-127.0\n",
      "min_val=-10.204087145477535, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.246552556753159\n",
      "\n",
      "min_ind=133, max_ind=323, delta=0.08131518074377315, offset=-127.0\n",
      "min_val=-10.32702795445919, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.355444505810738\n",
      "\n",
      "min_ind=132, max_ind=324, delta=0.08228321860977045, offset=-127.0\n",
      "min_val=-10.449968763440847, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.464336454868317\n",
      "\n",
      "min_ind=131, max_ind=325, delta=0.08325125647576775, offset=-127.0\n",
      "min_val=-10.572909572422505, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.573228403925896\n",
      "\n",
      "min_ind=129, max_ind=327, delta=0.08421929434176506, offset=-127.0\n",
      "min_val=-10.695850381404163, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.791012302041054\n",
      "\n",
      "min_ind=128, max_ind=328, delta=0.08518733220776235, offset=-127.0\n",
      "min_val=-10.818791190385818, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-10.899904251098633\n",
      "\n",
      "min_ind=127, max_ind=329, delta=0.08615537007375966, offset=-127.0\n",
      "min_val=-10.941731999367477, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.008796200156212\n",
      "\n",
      "min_ind=126, max_ind=330, delta=0.08712340793975695, offset=-127.0\n",
      "min_val=-11.064672808349133, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.117688149213791\n",
      "\n",
      "min_ind=125, max_ind=331, delta=0.08809144580575426, offset=-127.0\n",
      "min_val=-11.187613617330792, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.22658009827137\n",
      "\n",
      "min_ind=124, max_ind=332, delta=0.08905948367175155, offset=-127.0\n",
      "min_val=-11.310554426312446, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.335472047328949\n",
      "\n",
      "min_ind=123, max_ind=333, delta=0.09002752153774886, offset=-127.0\n",
      "min_val=-11.433495235294105, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.444363996386528\n",
      "\n",
      "min_ind=121, max_ind=335, delta=0.09099555940374615, offset=-127.0\n",
      "min_val=-11.556436044275761, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.662147894501686\n",
      "\n",
      "min_ind=120, max_ind=336, delta=0.09196359726974346, offset=-127.0\n",
      "min_val=-11.67937685325742, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.771039843559265\n",
      "\n",
      "min_ind=119, max_ind=337, delta=0.09293163513574075, offset=-127.0\n",
      "min_val=-11.802317662239075, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.879931792616844\n",
      "\n",
      "min_ind=118, max_ind=338, delta=0.09389967300173804, offset=-127.0\n",
      "min_val=-11.925258471220731, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-11.988823741674423\n",
      "\n",
      "min_ind=117, max_ind=339, delta=0.09486771086773535, offset=-127.0\n",
      "min_val=-12.04819928020239, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-12.097715690732002\n",
      "\n",
      "min_ind=116, max_ind=340, delta=0.09583574873373264, offset=-127.0\n",
      "min_val=-12.171140089184046, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-12.206607639789581\n",
      "\n",
      "min_ind=115, max_ind=341, delta=0.09680378659972995, offset=-127.0\n",
      "min_val=-12.294080898165703, pdf_start=-24.83807373046875 pdf_step=0.10889194905757904\n",
      "pdf.xLeft[min_ind]=-12.31549958884716\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'delta': 0.09583574873373264, 'offset': -127.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = 1e9\n",
    "# for idx, encoding in enumerate(encodings):\n",
    "#     loss = _quant_and_sat_cost(pdf, 8, encoding['delta'], encoding['offset'])\n",
    "#     if loss < best_loss:\n",
    "#         best_loss = loss\n",
    "#         best_encoding = encoding\n",
    "        \n",
    "for delta, offset in test_candidates:\n",
    "    loss = _quant_and_sat_cost(pdf, 8, delta, offset)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_encoding = {\"delta\": delta, \"offset\": offset}\n",
    "        \n",
    "best_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.390884684765433"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "bw = 8\n",
    "min_val = delta * offset\n",
    "step_size = math.pow(2, bw) - 1\n",
    "max_val = delta * (offset + step_size)\n",
    "\n",
    "max_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
