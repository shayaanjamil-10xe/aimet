{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [l.strip() for l in lines]\n",
    "\n",
    "def read_annot_file(file_path):\n",
    "    annots = read_text_file(file_path)\n",
    "    annots = {l.split(\" \")[0]: int(l.split(\" \")[1]) for l in annots}\n",
    "    return annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "ANNOT_FILE = \"/datasets/imagenet1k/tags.txt\"\n",
    "DATASET_DIR = '/datasets/imagenet1k/new_images'\n",
    "image_size = 224\n",
    "images_mean = [0.485, 0.456, 0.406]\n",
    "images_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalize = transforms.Normalize(mean=images_mean,\n",
    "                                std=images_std)\n",
    "transforms = transforms.Compose([\n",
    "            transforms.Resize(image_size + 24),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "annots = read_annot_file(ANNOT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class ImagenetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, annots_dict, transform=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.image_paths = glob(os.path.join(dataset_dir, '*.JPEG'))\n",
    "        self.transform = transform\n",
    "        self.annots_dict = annots_dict\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        # print(f\"{image_path=} \\t {idx=}\")\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = self.transform(image) if self.transform else image\n",
    "        label = self.annots_dict[os.path.basename(image_path)]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImagenetDataset(DATASET_DIR, annots, transform=transforms)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.utils\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res\n",
    "\n",
    "def evaluate(model: nn.Module, data_loader: torch.utils.data.DataLoader, iterations: int = None, use_cuda: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the specified model using the specified number of samples batches from the\n",
    "    validation set.\n",
    "    :param model: The model to be evaluated.\n",
    "    :param iterations: The number of batches to use from the validation set.\n",
    "    :param use_cuda: If True then use a GPU for inference.\n",
    "    :return: The accuracy for the sample with the maximum accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    if use_cuda:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            print('use_cuda is selected but no cuda device found.')\n",
    "            raise RuntimeError(\"Found no CUDA Device while use_cuda is selected\")\n",
    "\n",
    "    if iterations is None:\n",
    "        print('No value of iteration is provided, running evaluation on complete dataset.')\n",
    "        iterations = len(data_loader)\n",
    "    if iterations <= 0:\n",
    "        print('Cannot evaluate on %d iterations', iterations)\n",
    "\n",
    "    acc_top1 = 0\n",
    "    acc_top5 = 0\n",
    "\n",
    "    print(\"Evaluating nn.Module for %d iterations with batch_size %d\",\n",
    "                iterations, data_loader.batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_data, target_data) in tqdm(enumerate(data_loader), total=iterations):\n",
    "            if i == iterations:\n",
    "                break\n",
    "            inputs_batch = input_data.to(device)\n",
    "            target_batch = target_data.to(device)\n",
    "\n",
    "            predicted_batch = model(inputs_batch)\n",
    "\n",
    "            batch_avg_top_1_5 = accuracy(output=predicted_batch, target=target_batch,\n",
    "                                            topk=(1, 5))\n",
    "\n",
    "            acc_top1 += batch_avg_top_1_5[0].item()\n",
    "            acc_top5 += batch_avg_top_1_5[1].item()\n",
    "\n",
    "    acc_top1 /= iterations\n",
    "    acc_top5 /= iterations\n",
    "\n",
    "    print(f\"Avg accuracy Top 1: {acc_top1}%\\nAvg accuracy Top 5: {acc_top5}%\\non validation Dataset\")\n",
    "\n",
    "    return acc_top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model, dataloader, iterations=5000, use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load the model and evaluate to get a baseline FP32 accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Load a pretrained resnet18 model from torchvision.** \n",
    "\n",
    "You can load any pretrained PyTorch model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIMET quantization simulation requires the model definition to follow certain guidelines. For example, functionals defined in the forward pass should be changed to the equivalent **torch.nn.Module**.\n",
    "The [AIMET user guide](https://quic.github.io/aimet-pages/releases/latest/user_guide/index.html) lists all these guidelines.\n",
    "\n",
    "**2.2 Use the following ModelPreparer API call to automate the model definition changes required to comply with the AIMET guidelines.** \n",
    "\n",
    "The call uses the graph transformation feature available in PyTorch 1.9+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-02 09:20:44,273 - root - INFO - AIMET\n",
      "2025-01-02 09:20:44,323 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer1.0.module_add} \n",
      "2025-01-02 09:20:44,323 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.0.module_relu_1} \n",
      "2025-01-02 09:20:44,324 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer1.1.module_add_1} \n",
      "2025-01-02 09:20:44,324 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.1.module_relu_1} \n",
      "2025-01-02 09:20:44,325 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer2.0.module_add_2} \n",
      "2025-01-02 09:20:44,325 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.0.module_relu_1} \n",
      "2025-01-02 09:20:44,325 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer2.1.module_add_3} \n",
      "2025-01-02 09:20:44,326 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.1.module_relu_1} \n",
      "2025-01-02 09:20:44,326 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.0.module_add_4} \n",
      "2025-01-02 09:20:44,326 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.0.module_relu_1} \n",
      "2025-01-02 09:20:44,327 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.1.module_add_5} \n",
      "2025-01-02 09:20:44,327 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.1.module_relu_1} \n",
      "2025-01-02 09:20:44,327 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer4.0.module_add_6} \n",
      "2025-01-02 09:20:44,328 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.0.module_relu_1} \n",
      "2025-01-02 09:20:44,328 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer4.1.module_add_7} \n",
      "2025-01-02 09:20:44,328 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.1.module_relu_1} \n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.model_preparer import prepare_model\n",
    "\n",
    "model = prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.3 Decide whether to place the model on a CPU or CUDA device.** \n",
    "\n",
    "This example uses CUDA if it is available. You can change this logic and force a device placement if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.4 Compute the floating point 32-bit (FP32) accuracy of this model using the evaluate() routine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating nn.Module for %d iterations with batch_size %d 5000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:08<00:00, 72.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy Top 1: 69.56%\n",
      "Avg accuracy Top 5: 88.82%\n",
      "on validation Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dataloader, iterations=5000, use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Create a quantization simulation model and determine quantized accuracy\n",
    "\n",
    "### Fold Batch Norm layers\n",
    "\n",
    "Before calculating the simulated quantized accuracy using QuantizationSimModel, fold the BatchNorm (BN) layers into adjacent Convolutional layers. The BN layers that cannot be folded are left as they are.\n",
    "\n",
    "BN folding improves inference performance on quantized runtimes but can degrade accuracy on these platforms. This step simulates this on-target drop in accuracy. \n",
    "\n",
    "**3.1 Use the following code to call AIMET to fold the BN layers in-place on the given model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-02 09:21:53,470] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-02 09:21:53,470] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "\n",
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Quantization Sim Model\n",
    "\n",
    "**3.2 Use AIMET to create a QuantizationSimModel.**\n",
    "\n",
    " In this step, AIMET inserts fake quantization ops in the model graph and configures them.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- Setting **default_output_bw** to 8 performs all activation quantizations in the model using integer 8-bit precision\n",
    "- Setting **default_param_bw** to 8 performs all parameter quantizations in the model using integer 8-bit precision\n",
    "- **num_batches** is the number of batches to use to compute encodings. Only five batches are used here for the sake of speed\n",
    "\n",
    "See [QuantizationSimModel in the AIMET API documentation](https://quic.github.io/aimet-pages/AimetDocs/api_docs/torch_quantsim.html#aimet_torch.quantsim.QuantizationSimModel.compute_encodings) for a full explanation of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-02 09:21:54,515 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-01-02 09:21:54,516 - Quant - INFO - Unsupported op type Mean\n",
      "2025-01-02 09:21:54,517 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v1.quantsim import QuantizationSimModel\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8,\n",
    "                           config_file=\"/home/shayan/Desktop/aimet/my_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=1, shuffle=False)\n",
    "    batch_size = dataloader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in tqdm(dataloader):\n",
    "            # if \"cf135f199d8c7a9d0dce9aa35acfb4c70c14e0aa\" not in path[0]:\n",
    "            #     continue\n",
    "            # if \"cf\" in path[0]:\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "            batch_cntr += 1\n",
    "            if batch_cntr * batch_size >= samples:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 999/50000 [01:12<59:36, 13.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Int 8 quantization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating nn.Module for %d iterations with batch_size %d 5000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:55<00:00, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy Top 1: 69.22%\n",
      "Avg accuracy Top 5: 88.5%\n",
      "on validation Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(sim.model, dataloader, iterations=5000, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 BIT Quantization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating nn.Module for %d iterations with batch_size %d 5000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [07:02<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy Top 1: 1.38%\n",
      "Avg accuracy Top 5: 6.62%\n",
      "on validation Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(sim.model, dataloader, iterations=5000, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
