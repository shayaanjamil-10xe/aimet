{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Layer Equalization (CLE) and Bias Correction (BC)\n",
    "\n",
    "This notebook showcases a working code example of how to use AIMET to apply Cross-Layer Equalization (CLE) and Bias Correction (BC). CLE and BC are post-training quantization techniques that aim to improve quantized accuracy of a given model. CLE does not need any data samples. BC may optionally need unlabelled data samples. These techniques help recover quantized accuracy when the model quantization is sensitive to parameter quantization as opposed to activation quantization.\n",
    "\n",
    "To learn more about this techniques, please refer to the \"Data-Free Quantization Through Weight Equalization and Bias Correction\" paper from ICCV 2019 - https://arxiv.org/abs/1906.04721\n",
    "\n",
    "**Cross-Layer Equalization**\n",
    "AIMET performs the following steps when running CLE:\n",
    "1. Batch Norm Folding: Folds BN layers into Conv layers immediate before or after the Conv layers.\n",
    "2. Cross-Layer Scaling: Given a set of consecutive Conv layers, equalizes the range of tensor values per-channel by scaling up/down per-channel weight tensor values of a layer and corresponding scaling down/up per-channel weight tensor values of the subsequent layer.\n",
    "3. High Bias Folding: Cross-layer scaling may result in high bias parameter values for some layers. This technique folds some of the bias of a layer into the subsequent layer's parameters.\n",
    "\n",
    "**Bias Correction**  \n",
    "Quantization sometimes leads to a shift in layer outputs. This techniques helps correct this shift by adjusting the bias parameters of that layer. Note that this technique is generally applied after CLE, but it is a optional step.\n",
    "\n",
    "\n",
    "#### Overall flow\n",
    "This notebook covers the following\n",
    "1. Instantiate the example evaluation and training pipeline\n",
    "2. Load the FP32 model and evaluate the model to find the baseline FP32 accuracy\n",
    "3. Create a quantization simulation model (with fake quantization ops inserted) and evaluate this simuation model to get a quantized accuracy score\n",
    "4. Apply CLE, BC and and evaluate the simulation model to get a post-finetuned quantized accuracy score\n",
    "\n",
    "\n",
    "#### What this notebook is not\n",
    "* This notebook is not designed to show state-of-the-art results. For example, it uses a relatively quantization-friendly model like Resnet18. Also, some optimization parameters are deliberately chosen to have the notebook execute more quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmcv.transforms import Compose\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip() for x in data]\n",
    "    return data\n",
    "\n",
    "def preprocess(test_pipeline, image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Calling this method across libraries will result\n",
    "        # in module unregistered error if not prefixed with mmdet.\n",
    "        test_pipeline[0].type = 'mmdet.LoadImageFromNDArray'\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    return test_pipeline(dict(img=image))\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, annotations_json_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_json = read_json(annotations_json_path)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_json['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_dict = self.annotations_json['images'][idx]\n",
    "        image_path = os.path.join(self.images_dir, image_dict['file_name'])\n",
    "        image_id = image_dict['id']\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            transformed_images = self.transform(image)\n",
    "        else:\n",
    "            transformed_images = image\n",
    "\n",
    "        return image_id, image_path, transformed_images\n",
    "\n",
    "\n",
    "# calibrationDataloader = DataLoader(calibrationDataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-10 05:44:45,038] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "Loads checkpoint by local backend from path: /teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n",
      "09/10 05:44:47 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([640, 640]),  # Resize\n",
    "])\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIG_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco.py'\n",
    "WEIGHTS_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "EVAL_DATASET_SIZE = 5000\n",
    "CALIBRATION_DATASET_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ROOT_DATASET_DIR = '/teamspace/studios/this_studio/COCO'\n",
    "IMAGES_DIR = os.path.join(ROOT_DATASET_DIR, 'images')\n",
    "ANNOTATIONS_JSON_PATH = os.path.join(ROOT_DATASET_DIR, 'annotations/instances_val2017.json')\n",
    "# ANNOTATIONS_JSON_PATH = \"/home/shayaan/Desktop/aimet/my_mmdet/temp.json\"\n",
    "\n",
    "model = DetInferencer(model=CONFIG_PATH, weights=WEIGHTS_PATH, device=DEVICE)\n",
    "evalDataset = CustomImageDataset(images_dir=IMAGES_DIR, annotations_json_path=ANNOTATIONS_JSON_PATH, transform=transform)\n",
    "eval_data_loader = DataLoader(evalDataset, batch_size=BATCH_SIZE)\n",
    "calibration_images = read_txt('/teamspace/studios/this_studio/aimet/Examples/torch/quantization/calibration_image_ids.txt')\n",
    "calibration_data_loader = DataLoader(calibration_images, batch_size=BATCH_SIZE)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-10 05:44:49,094 - root - INFO - AIMET\n",
      "2024-09-10 05:44:54,998 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.module_add} \n",
      "2024-09-10 05:44:54,999 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.module_cat} \n",
      "2024-09-10 05:44:55,000 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.attention.module_mul} \n",
      "2024-09-10 05:44:55,000 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.module_add_1} \n",
      "2024-09-10 05:44:55,001 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.module_cat_1} \n",
      "2024-09-10 05:44:55,001 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.attention.module_mul_1} \n",
      "2024-09-10 05:44:55,002 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.module_add_2} \n",
      "2024-09-10 05:44:55,003 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.module_cat_2} \n",
      "2024-09-10 05:44:55,004 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.attention.module_mul_2} \n",
      "2024-09-10 05:44:55,004 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.module_cat_3} \n",
      "2024-09-10 05:44:55,005 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.module_cat_4} \n",
      "2024-09-10 05:44:55,006 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.attention.module_mul_3} \n",
      "2024-09-10 05:44:55,006 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_5} \n",
      "2024-09-10 05:44:55,007 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.module_cat_6} \n",
      "2024-09-10 05:44:55,008 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {neck.module_upsample_1} \n",
      "2024-09-10 05:44:55,009 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_7} \n",
      "2024-09-10 05:44:55,009 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.module_cat_8} \n",
      "2024-09-10 05:44:55,010 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_9} \n",
      "2024-09-10 05:44:55,011 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.module_cat_10} \n",
      "2024-09-10 05:44:55,012 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_11} \n",
      "2024-09-10 05:44:55,012 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.module_cat_12} \n",
      "2024-09-10 05:44:55,013 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_4} \n",
      "2024-09-10 05:44:55,015 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.0.module_conv_1} \n",
      "2024-09-10 05:44:55,016 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.1.module_conv_1} \n",
      "2024-09-10 05:44:55,017 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.0.module_conv_1} \n",
      "2024-09-10 05:44:55,018 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.1.module_conv_1} \n",
      "2024-09-10 05:44:55,019 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_5} \n",
      "2024-09-10 05:44:55,020 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.0.module_conv_2} \n",
      "2024-09-10 05:44:55,021 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.1.module_conv_2} \n",
      "2024-09-10 05:44:55,022 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.0.module_conv_2} \n",
      "2024-09-10 05:44:55,023 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.1.module_conv_2} \n",
      "2024-09-10 05:44:55,024 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_6} \n",
      "2024-09-10 05:45:02,703 - BatchNormFolding - INFO - 0 BatchNorms' weights got converted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "m = deepcopy(model.model)\n",
    "\n",
    "def is_leaf(module): \n",
    "    return len(module._modules) == 0\n",
    "\n",
    "def replace_bn(m):\n",
    "\n",
    "    if is_leaf(m):\n",
    "        return \n",
    "\n",
    "    for _, child in m.named_children(): \n",
    "        \n",
    "        if \"bn\" in child._modules.keys():\n",
    "            bn = child._modules.get(\"bn\")\n",
    "            bn_params = deepcopy(bn._parameters)\n",
    "            bn_buffers = deepcopy(bn._buffers)\n",
    "            new_bn = torch.nn.BatchNorm2d(bn.num_features, eps=bn.eps, momentum=bn.momentum, affine=bn.affine, track_running_stats=bn.track_running_stats)\n",
    "            new_bn._parameters[\"weight\"].data = bn_params[\"weight\"].data\n",
    "            new_bn._parameters[\"bias\"].data = bn_params[\"bias\"].data\n",
    "            new_bn._buffers[\"running_mean\"].data = bn_buffers[\"running_mean\"].data\n",
    "            new_bn._buffers[\"running_var\"].data = bn_buffers[\"running_var\"].data\n",
    "            new_bn._buffers[\"num_batches_tracked\"].data = bn_buffers[\"num_batches_tracked\"].data\n",
    "            child._modules[\"bn\"] = new_bn\n",
    "            \n",
    "        replace_bn(child)\n",
    "\n",
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "from aimet_torch.model_preparer import prepare_model\n",
    "replace_bn(m)\n",
    "aimet_m = prepare_model(deepcopy(m))\n",
    "folded_pairs = fold_all_batch_norms(aimet_m, input_shapes=(1, 3, 640, 640))\n",
    "len(folded_pairs)\n",
    "# print(m)\n",
    "# m(torch.rand(1, 3, 640, 640).to(DEVICE))[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "from mmdet.models.utils import samplelist_boxtype2tensor\n",
    "from mmengine.registry import MODELS\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "test_evaluator = model.cfg.test_evaluator\n",
    "test_evaluator.type = 'mmdet.evaluation.CocoMetric' \n",
    "test_evaluator.dataset_meta = model.model.dataset_meta\n",
    "test_evaluator.ann_file = ANNOTATIONS_JSON_PATH\n",
    "test_evaluator = Compose(test_evaluator)\n",
    "\n",
    "collate_preprocessor = model.preprocess\n",
    "predict_by_feat = model.model.bbox_head.predict_by_feat\n",
    "rescale = True\n",
    "\n",
    "preprocessor = MODELS.build(model.cfg.model.data_preprocessor)\n",
    "def add_pred_to_datasample(data_samples, results_list):\n",
    "    for data_sample, pred_instances in zip(data_samples, results_list):\n",
    "        data_sample.pred_instances = pred_instances\n",
    "    samplelist_boxtype2tensor(data_samples)\n",
    "    return data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(model: torch.nn.Module, samples: int):\n",
    "    data_loader = eval_data_loader\n",
    "    batch_size = data_loader.batch_size\n",
    "    model.eval()\n",
    "    batch_ctr = 0\n",
    "    with torch.no_grad():\n",
    "        for image_path in tqdm(calibration_data_loader):\n",
    "            image_path = [os.path.join(IMAGES_DIR, x) for x in image_path]\n",
    "            pre_processed = collate_preprocessor(inputs=image_path, batch_size=batch_size)\n",
    "            _, data = list(pre_processed)[0]\n",
    "            data = preprocessor(data, False)\n",
    "            \n",
    "            preds = model(data['inputs'].to(DEVICE))  \n",
    "\n",
    "            # batch_ctr += 1\n",
    "            # if (batch_ctr * batch_size) > samples:\n",
    "            #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_nodes=[Conv_0]\n",
      "Visiting node: GraphModule.backbone.stem.0.conv\n",
      "Visiting node: GraphModule.CG_Split_0\n",
      "Visiting node: GraphModule.backbone.stem.0.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stem.0.activate.mul\n",
      "Visiting node: GraphModule.backbone.stem.1.conv\n",
      "Visiting node: GraphModule.CG_Split_1\n",
      "Visiting node: GraphModule.backbone.stem.1.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stem.1.activate.mul\n",
      "Visiting node: GraphModule.backbone.stem.2.conv\n",
      "Visiting node: GraphModule.CG_Split_2\n",
      "Visiting node: GraphModule.backbone.stem.2.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stem.2.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage1.0.conv\n",
      "Visiting node: GraphModule.CG_Split_3\n",
      "Visiting node: GraphModule.backbone.stage1.0.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.0.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_4\n",
      "Visiting node: GraphModule.backbone.stage1.1.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_5\n",
      "Visiting node: GraphModule.backbone.stage1.1.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.1.short_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage1.1.module_cat\n",
      "Visiting node: GraphModule.CG_Split_11\n",
      "Visiting node: GraphModule.backbone.stage1.1.attention.global_avgpool\n",
      "Visiting node: GraphModule.backbone.stage1.1.attention.fc\n",
      "Visiting node: GraphModule.backbone.stage1.1.attention.act\n",
      "Visiting node: GraphModule.backbone.stage1.1.attention.module_mul\n",
      "Visiting node: GraphModule.backbone.stage1.1.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_12\n",
      "Visiting node: GraphModule.backbone.stage1.1.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.1.final_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage2.0.conv\n",
      "Visiting node: GraphModule.CG_Split_13\n",
      "Visiting node: GraphModule.backbone.stage2.0.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.0.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_14\n",
      "Visiting node: GraphModule.backbone.stage2.1.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_15\n",
      "Visiting node: GraphModule.backbone.stage2.1.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.1.short_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage2.1.module_cat_1\n",
      "Visiting node: GraphModule.CG_Split_21\n",
      "Visiting node: GraphModule.backbone.stage2.1.attention.global_avgpool\n",
      "Visiting node: GraphModule.backbone.stage2.1.attention.fc\n",
      "Visiting node: GraphModule.backbone.stage2.1.attention.act\n",
      "Visiting node: GraphModule.backbone.stage2.1.attention.module_mul_1\n",
      "Visiting node: GraphModule.backbone.stage2.1.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_22\n",
      "Visiting node: GraphModule.backbone.stage2.1.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.1.final_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_23\n",
      "Visiting node: GraphModule.backbone.stage3.0.conv\n",
      "Visiting node: GraphModule.CG_Split_24\n",
      "Visiting node: GraphModule.backbone.stage3.0.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.0.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_25\n",
      "Visiting node: GraphModule.backbone.stage3.1.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_26\n",
      "Visiting node: GraphModule.backbone.stage3.1.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.1.short_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage3.1.module_cat_2\n",
      "Visiting node: GraphModule.CG_Split_32\n",
      "Visiting node: GraphModule.backbone.stage3.1.attention.global_avgpool\n",
      "Visiting node: GraphModule.backbone.stage3.1.attention.fc\n",
      "Visiting node: GraphModule.backbone.stage3.1.attention.act\n",
      "Visiting node: GraphModule.backbone.stage3.1.attention.module_mul_2\n",
      "Visiting node: GraphModule.backbone.stage3.1.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_33\n",
      "Visiting node: GraphModule.backbone.stage3.1.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.1.final_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_34\n",
      "Visiting node: GraphModule.backbone.stage4.0.conv\n",
      "Visiting node: GraphModule.CG_Split_35\n",
      "Visiting node: GraphModule.backbone.stage4.0.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.0.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.1.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_36\n",
      "Visiting node: GraphModule.backbone.stage4.1.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.1.conv1.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_37\n",
      "Visiting node: GraphModule.backbone.stage4.1.poolings.0\n",
      "Visiting node: GraphModule.backbone.stage4.1.module_cat_3\n",
      "Visiting node: GraphModule.backbone.stage4.1.conv2.conv\n",
      "Visiting node: GraphModule.CG_Split_38\n",
      "Visiting node: GraphModule.backbone.stage4.1.conv2.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.1.conv2.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_39\n",
      "Visiting node: GraphModule.backbone.stage4.2.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_40\n",
      "Visiting node: GraphModule.backbone.stage4.2.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.2.short_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.2.module_cat_4\n",
      "Visiting node: GraphModule.CG_Split_45\n",
      "Visiting node: GraphModule.backbone.stage4.2.attention.global_avgpool\n",
      "Visiting node: GraphModule.backbone.stage4.2.attention.fc\n",
      "Visiting node: GraphModule.backbone.stage4.2.attention.act\n",
      "Visiting node: GraphModule.backbone.stage4.2.attention.module_mul_3\n",
      "Visiting node: GraphModule.backbone.stage4.2.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_46\n",
      "Visiting node: GraphModule.backbone.stage4.2.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.2.final_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.reduce_layers.0.conv\n",
      "Visiting node: GraphModule.CG_Split_47\n",
      "Visiting node: GraphModule.neck.reduce_layers.0.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.reduce_layers.0.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_48\n",
      "Visiting node: GraphModule.neck.upsample\n",
      "Visiting node: GraphModule.neck.module_cat_5\n",
      "Visiting node: GraphModule.CG_Split_49\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_50\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.short_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.module_cat_6\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_55\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.final_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.reduce_layers.1.conv\n",
      "Visiting node: GraphModule.CG_Split_56\n",
      "Visiting node: GraphModule.neck.reduce_layers.1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.reduce_layers.1.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_57\n",
      "Visiting node: GraphModule.neck.module_upsample_1\n",
      "Visiting node: GraphModule.neck.module_cat_7\n",
      "Visiting node: GraphModule.CG_Split_58\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_59\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.short_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.module_cat_8\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_64\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.final_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_65\n",
      "Visiting node: GraphModule.neck.downsamples.0.conv\n",
      "Visiting node: GraphModule.CG_Split_66\n",
      "Visiting node: GraphModule.neck.downsamples.0.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.downsamples.0.activate.mul\n",
      "Visiting node: GraphModule.neck.module_cat_9\n",
      "Visiting node: GraphModule.CG_Split_67\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_68\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.short_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.module_cat_10\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_73\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.final_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_74\n",
      "Visiting node: GraphModule.neck.downsamples.1.conv\n",
      "Visiting node: GraphModule.CG_Split_75\n",
      "Visiting node: GraphModule.neck.downsamples.1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.downsamples.1.activate.mul\n",
      "Visiting node: GraphModule.neck.module_cat_11\n",
      "Visiting node: GraphModule.CG_Split_76\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.short_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_77\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.short_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.short_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.module_cat_12\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.final_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_82\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.final_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.final_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.out_convs.2.conv\n",
      "Visiting node: GraphModule.CG_Split_87\n",
      "Visiting node: GraphModule.neck.out_convs.2.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.out_convs.2.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_88\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.0.module_conv_2\n",
      "Visiting node: GraphModule.CG_Split_97\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.2.0.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.2.0.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.1.module_conv_2\n",
      "Visiting node: GraphModule.CG_Split_98\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.2.1.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.2.1.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.rtm_cls.2\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.0.module_conv_2\n",
      "Visiting node: GraphModule.CG_Split_99\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.2.0.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.2.0.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.1.module_conv_2\n",
      "Visiting node: GraphModule.CG_Split_100\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.2.1.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.2.1.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.rtm_reg.2\n",
      "Visiting node: GraphModule.bbox_head.module_mul_6\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_78\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.main_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_79\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_80\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_81\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.1.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.out_convs.1.conv\n",
      "Visiting node: GraphModule.CG_Split_85\n",
      "Visiting node: GraphModule.neck.out_convs.1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.out_convs.1.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_86\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.0.module_conv_1\n",
      "Visiting node: GraphModule.CG_Split_93\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.1.0.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.1.0.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.1.module_conv_1\n",
      "Visiting node: GraphModule.CG_Split_94\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.1.1.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.1.1.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.rtm_cls.1\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.0.module_conv_1\n",
      "Visiting node: GraphModule.CG_Split_95\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.1.0.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.1.0.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.1.module_conv_1\n",
      "Visiting node: GraphModule.CG_Split_96\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.1.1.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.1.1.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.rtm_reg.1\n",
      "Visiting node: GraphModule.bbox_head.module_mul_5\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_69\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.main_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_70\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_71\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_72\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.bottom_up_blocks.0.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.out_convs.0.conv\n",
      "Visiting node: GraphModule.CG_Split_83\n",
      "Visiting node: GraphModule.neck.out_convs.0.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.out_convs.0.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_84\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.0.conv\n",
      "Visiting node: GraphModule.CG_Split_89\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.0.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.0.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.1.conv\n",
      "Visiting node: GraphModule.CG_Split_90\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.1.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.cls_convs.0.1.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.rtm_cls.0\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.0.conv\n",
      "Visiting node: GraphModule.CG_Split_91\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.0.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.0.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.1.conv\n",
      "Visiting node: GraphModule.CG_Split_92\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.1.activate.sigmoid\n",
      "Visiting node: GraphModule.bbox_head.reg_convs.0.1.activate.mul\n",
      "Visiting node: GraphModule.bbox_head.rtm_reg.0\n",
      "Visiting node: GraphModule.bbox_head.module_mul_4\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_60\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.main_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_61\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_62\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_63\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.1.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_51\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.main_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_52\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_53\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_54\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.neck.top_down_blocks.0.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.2.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_41\n",
      "Visiting node: GraphModule.backbone.stage4.2.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.2.main_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_42\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_43\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_44\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage4.2.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage4.1.poolings.1\n",
      "Visiting node: GraphModule.backbone.stage4.1.poolings.2\n",
      "Visiting node: GraphModule.backbone.stage3.1.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_27\n",
      "Visiting node: GraphModule.backbone.stage3.1.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.1.main_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_28\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_29\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_30\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_31\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage3.1.blocks.0.module_add_2\n",
      "Visiting node: GraphModule.backbone.stage2.1.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_16\n",
      "Visiting node: GraphModule.backbone.stage2.1.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.1.main_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_17\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_18\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_19\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_20\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage2.1.blocks.0.module_add_1\n",
      "Visiting node: GraphModule.backbone.stage1.1.main_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_6\n",
      "Visiting node: GraphModule.backbone.stage1.1.main_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.1.main_conv.activate.mul\n",
      "Visiting node: GraphModule.CG_Split_7\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv1.conv\n",
      "Visiting node: GraphModule.CG_Split_8\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv1.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv1.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_9\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv\n",
      "Visiting node: GraphModule.CG_Split_10\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.sigmoid\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.mul\n",
      "Visiting node: GraphModule.backbone.stage1.1.blocks.0.module_add\n",
      "layer_groups=[]\n",
      "ordered_layer_groups=[]\n",
      "layer_groups=[]\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model, equalize_bn_folded_model\n",
    "\n",
    "equalize_bn_folded_model(aimet_m, input_shapes=(1, 3, 640, 640), folded_pairs=folded_pairs, dummy_input=torch.rand(1, 3, 640, 640))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIMET quantization simulation requires the user's model definition to follow certain guidelines. For example, functionals defined in forward pass should be changed to equivalent torch.nn.Module.\n",
    "AIMET user guide lists all these guidelines.\n",
    "The following **ModelPreparer** API uses new graph transformation feature available in PyTorch 1.9+ version and automates model definition changes required to comply with the above guidelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We should decide whether to place the model on a CPU or CUDA device. This example code will use CUDA if available in your current execution environment. You can change this logic and force a device placement if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))\n",
    "    \n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's determine the FP32 (floating point 32-bit) accuracy of this model using the evaluate() routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:05:35,560 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:05:35,572 - Eval - INFO - No value of iteration is provided, running evaluation on complete dataset.\n",
      "2024-09-09 08:05:35,579 - Eval - INFO - Evaluating nn.Module for 313 iterations with batch_size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 313) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "  0% (2 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:38\n",
      "  0% (3 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:01:00\n",
      "  1% (4 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:01:08\n",
      "  1% (5 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:12\n",
      "  1% (6 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:15\n",
      "  2% (7 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:17\n",
      "  2% (8 of 313) |                        | Elapsed Time: 0:00:02 ETA:   0:01:18\n",
      "  2% (9 of 313) |                        | Elapsed Time: 0:00:02 ETA:   0:01:20\n",
      "  3% (10 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:01:21\n",
      "  3% (11 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:01:21\n",
      "  3% (12 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:23\n",
      "  4% (13 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:24\n",
      "  4% (14 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:01:25\n",
      "  4% (15 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:25\n",
      "  5% (16 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:25\n",
      "  5% (17 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:25\n",
      "  5% (18 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:25\n",
      "  6% (19 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:25\n",
      "  6% (20 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:24\n",
      "  6% (21 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:24\n",
      "  7% (22 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:24\n",
      "  7% (23 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:24\n",
      "  7% (24 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:24\n",
      "  7% (25 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:23\n",
      "  8% (26 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:23\n",
      "  8% (27 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:23\n",
      "  8% (28 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:23\n",
      "  9% (29 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:23\n",
      "  9% (30 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:22\n",
      "  9% (31 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:22\n",
      " 10% (32 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:22\n",
      " 10% (33 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:21\n",
      " 10% (34 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:21\n",
      " 11% (35 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:21\n",
      " 11% (36 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:20\n",
      " 11% (37 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:20\n",
      " 12% (38 of 313) |##                     | Elapsed Time: 0:00:11 ETA:   0:01:21\n",
      " 12% (39 of 313) |##                     | Elapsed Time: 0:00:11 ETA:   0:01:23\n",
      " 12% (40 of 313) |##                     | Elapsed Time: 0:00:12 ETA:   0:01:22\n",
      " 13% (41 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:22\n",
      " 13% (42 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:22\n",
      " 13% (43 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:22\n",
      " 14% (44 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:22\n",
      " 14% (45 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:21\n",
      " 14% (46 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:21\n",
      " 15% (47 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:20\n",
      " 15% (48 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:20\n",
      " 15% (49 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:19\n",
      " 15% (50 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:19\n",
      " 16% (51 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:18\n",
      " 16% (52 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:18\n",
      " 16% (53 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:18\n",
      " 17% (54 of 313) |###                    | Elapsed Time: 0:00:16 ETA:   0:01:18\n",
      " 17% (55 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:17\n",
      " 17% (56 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:17\n",
      " 18% (57 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:17\n",
      " 18% (58 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:16\n",
      " 18% (59 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:16\n",
      " 19% (60 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:15\n",
      " 19% (61 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:15\n",
      " 19% (62 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:14\n",
      " 20% (63 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:13\n",
      " 20% (64 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:13\n",
      " 20% (65 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:12\n",
      " 21% (66 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:12\n",
      " 21% (67 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:11\n",
      " 21% (68 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:11\n",
      " 22% (69 of 313) |#####                  | Elapsed Time: 0:00:19 ETA:   0:01:10\n",
      " 22% (70 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:09\n",
      " 22% (71 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:09\n",
      " 23% (72 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:08\n",
      " 23% (73 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:08\n",
      " 23% (74 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:07\n",
      " 23% (75 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:07\n",
      " 24% (76 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:06\n",
      " 24% (77 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:06\n",
      " 24% (78 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:05\n",
      " 25% (79 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:05\n",
      " 25% (80 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:04\n",
      " 25% (81 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:04\n",
      " 26% (82 of 313) |######                 | Elapsed Time: 0:00:22 ETA:   0:01:03\n",
      " 26% (83 of 313) |######                 | Elapsed Time: 0:00:22 ETA:   0:01:03\n",
      " 26% (84 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:03\n",
      " 27% (85 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:02\n",
      " 27% (86 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:02\n",
      " 27% (87 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:02\n",
      " 28% (88 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:01\n",
      " 28% (89 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:01\n",
      " 28% (90 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:01\n",
      " 29% (91 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:00\n",
      " 29% (92 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:01:00\n",
      " 29% (93 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:00:59\n",
      " 30% (94 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:00:59\n",
      " 30% (95 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:00:59\n",
      " 30% (96 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:59\n",
      " 30% (97 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:58\n",
      " 31% (98 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:58\n",
      " 31% (99 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:57\n",
      " 31% (100 of 313) |#######               | Elapsed Time: 0:00:26 ETA:   0:00:57\n",
      " 32% (101 of 313) |#######               | Elapsed Time: 0:00:27 ETA:   0:00:57\n",
      " 32% (102 of 313) |#######               | Elapsed Time: 0:00:27 ETA:   0:00:56\n",
      " 32% (103 of 313) |#######               | Elapsed Time: 0:00:27 ETA:   0:00:56\n",
      " 33% (104 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:56\n",
      " 33% (105 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:56\n",
      " 33% (106 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:55\n",
      " 34% (107 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:55\n",
      " 34% (108 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:54\n",
      " 34% (109 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:54\n",
      " 35% (110 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:54\n",
      " 35% (111 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:54\n",
      " 35% (112 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:53\n",
      " 36% (113 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:53\n",
      " 36% (114 of 313) |########              | Elapsed Time: 0:00:30 ETA:   0:00:53\n",
      " 36% (115 of 313) |########              | Elapsed Time: 0:00:30 ETA:   0:00:52\n",
      " 37% (116 of 313) |########              | Elapsed Time: 0:00:30 ETA:   0:00:52\n",
      " 37% (117 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:52\n",
      " 37% (118 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:51\n",
      " 38% (119 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:51\n",
      " 38% (120 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:51\n",
      " 38% (121 of 313) |########              | Elapsed Time: 0:00:32 ETA:   0:00:51\n",
      " 38% (122 of 313) |########              | Elapsed Time: 0:00:32 ETA:   0:00:50\n",
      " 39% (123 of 313) |########              | Elapsed Time: 0:00:32 ETA:   0:00:50\n",
      " 39% (124 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:50\n",
      " 39% (125 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:49\n",
      " 40% (126 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:49\n",
      " 40% (127 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:49\n",
      " 40% (128 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:48\n",
      " 41% (129 of 313) |#########             | Elapsed Time: 0:00:34 ETA:   0:00:48\n",
      " 41% (130 of 313) |#########             | Elapsed Time: 0:00:34 ETA:   0:00:48\n",
      " 41% (131 of 313) |#########             | Elapsed Time: 0:00:34 ETA:   0:00:47\n",
      " 42% (132 of 313) |#########             | Elapsed Time: 0:00:34 ETA:   0:00:47\n",
      " 42% (133 of 313) |#########             | Elapsed Time: 0:00:35 ETA:   0:00:47\n",
      " 42% (134 of 313) |#########             | Elapsed Time: 0:00:35 ETA:   0:00:47\n",
      " 43% (135 of 313) |#########             | Elapsed Time: 0:00:35 ETA:   0:00:46\n",
      " 43% (136 of 313) |#########             | Elapsed Time: 0:00:35 ETA:   0:00:46\n",
      " 43% (137 of 313) |#########             | Elapsed Time: 0:00:35 ETA:   0:00:46\n",
      " 44% (138 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:45\n",
      " 44% (139 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:45\n",
      " 44% (140 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:45\n",
      " 45% (141 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:44\n",
      " 45% (142 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:44\n",
      " 45% (143 of 313) |##########            | Elapsed Time: 0:00:37 ETA:   0:00:44\n",
      " 46% (144 of 313) |##########            | Elapsed Time: 0:00:37 ETA:   0:00:43\n",
      " 46% (145 of 313) |##########            | Elapsed Time: 0:00:37 ETA:   0:00:43\n",
      " 46% (146 of 313) |##########            | Elapsed Time: 0:00:37 ETA:   0:00:43\n",
      " 46% (147 of 313) |##########            | Elapsed Time: 0:00:38 ETA:   0:00:43\n",
      " 47% (148 of 313) |##########            | Elapsed Time: 0:00:38 ETA:   0:00:42\n",
      " 47% (149 of 313) |##########            | Elapsed Time: 0:00:38 ETA:   0:00:42\n",
      " 47% (150 of 313) |##########            | Elapsed Time: 0:00:38 ETA:   0:00:42\n",
      " 48% (151 of 313) |##########            | Elapsed Time: 0:00:39 ETA:   0:00:41\n",
      " 48% (152 of 313) |##########            | Elapsed Time: 0:00:39 ETA:   0:00:41\n",
      " 48% (153 of 313) |##########            | Elapsed Time: 0:00:39 ETA:   0:00:41\n",
      " 49% (154 of 313) |##########            | Elapsed Time: 0:00:39 ETA:   0:00:40\n",
      " 49% (155 of 313) |##########            | Elapsed Time: 0:00:39 ETA:   0:00:40\n",
      " 49% (156 of 313) |##########            | Elapsed Time: 0:00:40 ETA:   0:00:40\n",
      " 50% (157 of 313) |###########           | Elapsed Time: 0:00:40 ETA:   0:00:40\n",
      " 50% (158 of 313) |###########           | Elapsed Time: 0:00:40 ETA:   0:00:39\n",
      " 50% (159 of 313) |###########           | Elapsed Time: 0:00:40 ETA:   0:00:39\n",
      " 51% (160 of 313) |###########           | Elapsed Time: 0:00:40 ETA:   0:00:39\n",
      " 51% (161 of 313) |###########           | Elapsed Time: 0:00:41 ETA:   0:00:38\n",
      " 51% (162 of 313) |###########           | Elapsed Time: 0:00:41 ETA:   0:00:38\n",
      " 52% (163 of 313) |###########           | Elapsed Time: 0:00:41 ETA:   0:00:38\n",
      " 52% (164 of 313) |###########           | Elapsed Time: 0:00:41 ETA:   0:00:37\n",
      " 52% (165 of 313) |###########           | Elapsed Time: 0:00:42 ETA:   0:00:37\n",
      " 53% (166 of 313) |###########           | Elapsed Time: 0:00:42 ETA:   0:00:37\n",
      " 53% (167 of 313) |###########           | Elapsed Time: 0:00:42 ETA:   0:00:37\n",
      " 53% (168 of 313) |###########           | Elapsed Time: 0:00:42 ETA:   0:00:36\n",
      " 53% (169 of 313) |###########           | Elapsed Time: 0:00:42 ETA:   0:00:36\n",
      " 54% (170 of 313) |###########           | Elapsed Time: 0:00:43 ETA:   0:00:36\n",
      " 54% (171 of 313) |############          | Elapsed Time: 0:00:43 ETA:   0:00:36\n",
      " 54% (172 of 313) |############          | Elapsed Time: 0:00:43 ETA:   0:00:35\n",
      " 55% (173 of 313) |############          | Elapsed Time: 0:00:43 ETA:   0:00:35\n",
      " 55% (174 of 313) |############          | Elapsed Time: 0:00:43 ETA:   0:00:35\n",
      " 55% (175 of 313) |############          | Elapsed Time: 0:00:44 ETA:   0:00:34\n",
      " 56% (176 of 313) |############          | Elapsed Time: 0:00:44 ETA:   0:00:34\n",
      " 56% (177 of 313) |############          | Elapsed Time: 0:00:44 ETA:   0:00:34\n",
      " 56% (178 of 313) |############          | Elapsed Time: 0:00:44 ETA:   0:00:34\n",
      " 57% (179 of 313) |############          | Elapsed Time: 0:00:45 ETA:   0:00:33\n",
      " 57% (180 of 313) |############          | Elapsed Time: 0:00:45 ETA:   0:00:33\n",
      " 57% (181 of 313) |############          | Elapsed Time: 0:00:45 ETA:   0:00:33\n",
      " 58% (182 of 313) |############          | Elapsed Time: 0:00:45 ETA:   0:00:33\n",
      " 58% (183 of 313) |############          | Elapsed Time: 0:00:46 ETA:   0:00:32\n",
      " 58% (184 of 313) |############          | Elapsed Time: 0:00:46 ETA:   0:00:32\n",
      " 59% (185 of 313) |#############         | Elapsed Time: 0:00:46 ETA:   0:00:32\n",
      " 59% (186 of 313) |#############         | Elapsed Time: 0:00:46 ETA:   0:00:31\n",
      " 59% (187 of 313) |#############         | Elapsed Time: 0:00:47 ETA:   0:00:31\n",
      " 60% (188 of 313) |#############         | Elapsed Time: 0:00:47 ETA:   0:00:31\n",
      " 60% (189 of 313) |#############         | Elapsed Time: 0:00:47 ETA:   0:00:31\n",
      " 60% (190 of 313) |#############         | Elapsed Time: 0:00:47 ETA:   0:00:31\n",
      " 61% (191 of 313) |#############         | Elapsed Time: 0:00:48 ETA:   0:00:30\n",
      " 61% (192 of 313) |#############         | Elapsed Time: 0:00:48 ETA:   0:00:30\n",
      " 61% (193 of 313) |#############         | Elapsed Time: 0:00:48 ETA:   0:00:30\n",
      " 61% (194 of 313) |#############         | Elapsed Time: 0:00:48 ETA:   0:00:29\n",
      " 62% (195 of 313) |#############         | Elapsed Time: 0:00:48 ETA:   0:00:29\n",
      " 62% (196 of 313) |#############         | Elapsed Time: 0:00:49 ETA:   0:00:29\n",
      " 62% (197 of 313) |#############         | Elapsed Time: 0:00:49 ETA:   0:00:29\n",
      " 63% (198 of 313) |#############         | Elapsed Time: 0:00:49 ETA:   0:00:28\n",
      " 63% (199 of 313) |#############         | Elapsed Time: 0:00:49 ETA:   0:00:28\n",
      " 63% (200 of 313) |##############        | Elapsed Time: 0:00:49 ETA:   0:00:28\n",
      " 64% (201 of 313) |##############        | Elapsed Time: 0:00:50 ETA:   0:00:27\n",
      " 64% (202 of 313) |##############        | Elapsed Time: 0:00:50 ETA:   0:00:27\n",
      " 64% (203 of 313) |##############        | Elapsed Time: 0:00:50 ETA:   0:00:27\n",
      " 65% (204 of 313) |##############        | Elapsed Time: 0:00:50 ETA:   0:00:27\n",
      " 65% (205 of 313) |##############        | Elapsed Time: 0:00:51 ETA:   0:00:26\n",
      " 65% (206 of 313) |##############        | Elapsed Time: 0:00:51 ETA:   0:00:26\n",
      " 66% (207 of 313) |##############        | Elapsed Time: 0:00:51 ETA:   0:00:26\n",
      " 66% (208 of 313) |##############        | Elapsed Time: 0:00:51 ETA:   0:00:26\n",
      " 66% (209 of 313) |##############        | Elapsed Time: 0:00:52 ETA:   0:00:25\n",
      " 67% (210 of 313) |##############        | Elapsed Time: 0:00:52 ETA:   0:00:25\n",
      " 67% (211 of 313) |##############        | Elapsed Time: 0:00:52 ETA:   0:00:25\n",
      " 67% (212 of 313) |##############        | Elapsed Time: 0:00:52 ETA:   0:00:25\n",
      " 68% (213 of 313) |##############        | Elapsed Time: 0:00:52 ETA:   0:00:24\n",
      " 68% (214 of 313) |###############       | Elapsed Time: 0:00:53 ETA:   0:00:24\n",
      " 68% (215 of 313) |###############       | Elapsed Time: 0:00:53 ETA:   0:00:24\n",
      " 69% (216 of 313) |###############       | Elapsed Time: 0:00:53 ETA:   0:00:24\n",
      " 69% (217 of 313) |###############       | Elapsed Time: 0:00:53 ETA:   0:00:23\n",
      " 69% (218 of 313) |###############       | Elapsed Time: 0:00:54 ETA:   0:00:23\n",
      " 69% (219 of 313) |###############       | Elapsed Time: 0:00:54 ETA:   0:00:23\n",
      " 70% (220 of 313) |###############       | Elapsed Time: 0:00:54 ETA:   0:00:23\n",
      " 70% (221 of 313) |###############       | Elapsed Time: 0:00:54 ETA:   0:00:22\n",
      " 70% (222 of 313) |###############       | Elapsed Time: 0:00:54 ETA:   0:00:22\n",
      " 71% (223 of 313) |###############       | Elapsed Time: 0:00:55 ETA:   0:00:22\n",
      " 71% (224 of 313) |###############       | Elapsed Time: 0:00:55 ETA:   0:00:21\n",
      " 71% (225 of 313) |###############       | Elapsed Time: 0:00:55 ETA:   0:00:21\n",
      " 72% (226 of 313) |###############       | Elapsed Time: 0:00:55 ETA:   0:00:21\n",
      " 72% (227 of 313) |###############       | Elapsed Time: 0:00:56 ETA:   0:00:21\n",
      " 72% (228 of 313) |################      | Elapsed Time: 0:00:56 ETA:   0:00:20\n",
      " 73% (229 of 313) |################      | Elapsed Time: 0:00:56 ETA:   0:00:20\n",
      " 73% (230 of 313) |################      | Elapsed Time: 0:00:56 ETA:   0:00:20\n",
      " 73% (231 of 313) |################      | Elapsed Time: 0:00:56 ETA:   0:00:20\n",
      " 74% (232 of 313) |################      | Elapsed Time: 0:00:57 ETA:   0:00:19\n",
      " 74% (233 of 313) |################      | Elapsed Time: 0:00:57 ETA:   0:00:19\n",
      " 74% (234 of 313) |################      | Elapsed Time: 0:00:57 ETA:   0:00:19\n",
      " 75% (235 of 313) |################      | Elapsed Time: 0:00:57 ETA:   0:00:19\n",
      " 75% (236 of 313) |################      | Elapsed Time: 0:00:57 ETA:   0:00:18\n",
      " 75% (237 of 313) |################      | Elapsed Time: 0:00:58 ETA:   0:00:18\n",
      " 76% (238 of 313) |################      | Elapsed Time: 0:00:58 ETA:   0:00:18\n",
      " 76% (239 of 313) |################      | Elapsed Time: 0:00:58 ETA:   0:00:18\n",
      " 76% (240 of 313) |################      | Elapsed Time: 0:00:58 ETA:   0:00:17\n",
      " 76% (241 of 313) |################      | Elapsed Time: 0:00:58 ETA:   0:00:17\n",
      " 77% (242 of 313) |#################     | Elapsed Time: 0:00:59 ETA:   0:00:17\n",
      " 77% (243 of 313) |#################     | Elapsed Time: 0:00:59 ETA:   0:00:17\n",
      " 77% (244 of 313) |#################     | Elapsed Time: 0:00:59 ETA:   0:00:16\n",
      " 78% (245 of 313) |#################     | Elapsed Time: 0:00:59 ETA:   0:00:16\n",
      " 78% (246 of 313) |#################     | Elapsed Time: 0:01:00 ETA:   0:00:16\n",
      " 78% (247 of 313) |#################     | Elapsed Time: 0:01:00 ETA:   0:00:16\n",
      " 79% (248 of 313) |#################     | Elapsed Time: 0:01:00 ETA:   0:00:15\n",
      " 79% (249 of 313) |#################     | Elapsed Time: 0:01:00 ETA:   0:00:15\n",
      " 79% (250 of 313) |#################     | Elapsed Time: 0:01:00 ETA:   0:00:15\n",
      " 80% (251 of 313) |#################     | Elapsed Time: 0:01:01 ETA:   0:00:15\n",
      " 80% (252 of 313) |#################     | Elapsed Time: 0:01:01 ETA:   0:00:14\n",
      " 80% (253 of 313) |#################     | Elapsed Time: 0:01:01 ETA:   0:00:14\n",
      " 81% (254 of 313) |#################     | Elapsed Time: 0:01:01 ETA:   0:00:14\n",
      " 81% (255 of 313) |#################     | Elapsed Time: 0:01:01 ETA:   0:00:14\n",
      " 81% (256 of 313) |#################     | Elapsed Time: 0:01:02 ETA:   0:00:13\n",
      " 82% (257 of 313) |##################    | Elapsed Time: 0:01:02 ETA:   0:00:13\n",
      " 82% (258 of 313) |##################    | Elapsed Time: 0:01:02 ETA:   0:00:13\n",
      " 82% (259 of 313) |##################    | Elapsed Time: 0:01:02 ETA:   0:00:13\n",
      " 83% (260 of 313) |##################    | Elapsed Time: 0:01:02 ETA:   0:00:12\n",
      " 83% (261 of 313) |##################    | Elapsed Time: 0:01:03 ETA:   0:00:12\n",
      " 83% (262 of 313) |##################    | Elapsed Time: 0:01:03 ETA:   0:00:12\n",
      " 84% (263 of 313) |##################    | Elapsed Time: 0:01:03 ETA:   0:00:12\n",
      " 84% (264 of 313) |##################    | Elapsed Time: 0:01:03 ETA:   0:00:11\n",
      " 84% (265 of 313) |##################    | Elapsed Time: 0:01:04 ETA:   0:00:11\n",
      " 84% (266 of 313) |##################    | Elapsed Time: 0:01:04 ETA:   0:00:11\n",
      " 85% (267 of 313) |##################    | Elapsed Time: 0:01:04 ETA:   0:00:11\n",
      " 85% (268 of 313) |##################    | Elapsed Time: 0:01:04 ETA:   0:00:10\n",
      " 85% (269 of 313) |##################    | Elapsed Time: 0:01:05 ETA:   0:00:10\n",
      " 86% (270 of 313) |##################    | Elapsed Time: 0:01:05 ETA:   0:00:10\n",
      " 86% (271 of 313) |###################   | Elapsed Time: 0:01:05 ETA:   0:00:10\n",
      " 86% (272 of 313) |###################   | Elapsed Time: 0:01:05 ETA:   0:00:09\n",
      " 87% (273 of 313) |###################   | Elapsed Time: 0:01:05 ETA:   0:00:09\n",
      " 87% (274 of 313) |###################   | Elapsed Time: 0:01:06 ETA:   0:00:09\n",
      " 87% (275 of 313) |###################   | Elapsed Time: 0:01:06 ETA:   0:00:09\n",
      " 88% (276 of 313) |###################   | Elapsed Time: 0:01:06 ETA:   0:00:08\n",
      " 88% (277 of 313) |###################   | Elapsed Time: 0:01:06 ETA:   0:00:08\n",
      " 88% (278 of 313) |###################   | Elapsed Time: 0:01:07 ETA:   0:00:08\n",
      " 89% (279 of 313) |###################   | Elapsed Time: 0:01:07 ETA:   0:00:08\n",
      " 89% (280 of 313) |###################   | Elapsed Time: 0:01:07 ETA:   0:00:07\n",
      " 89% (281 of 313) |###################   | Elapsed Time: 0:01:07 ETA:   0:00:07\n",
      " 90% (282 of 313) |###################   | Elapsed Time: 0:01:07 ETA:   0:00:07\n",
      " 90% (283 of 313) |###################   | Elapsed Time: 0:01:08 ETA:   0:00:07\n",
      " 90% (284 of 313) |###################   | Elapsed Time: 0:01:08 ETA:   0:00:06\n",
      " 91% (285 of 313) |####################  | Elapsed Time: 0:01:08 ETA:   0:00:06\n",
      " 91% (286 of 313) |####################  | Elapsed Time: 0:01:08 ETA:   0:00:06\n",
      " 91% (287 of 313) |####################  | Elapsed Time: 0:01:08 ETA:   0:00:06\n",
      " 92% (288 of 313) |####################  | Elapsed Time: 0:01:09 ETA:   0:00:06\n",
      " 92% (289 of 313) |####################  | Elapsed Time: 0:01:09 ETA:   0:00:05\n",
      " 92% (290 of 313) |####################  | Elapsed Time: 0:01:09 ETA:   0:00:05\n",
      " 92% (291 of 313) |####################  | Elapsed Time: 0:01:09 ETA:   0:00:05\n",
      " 93% (292 of 313) |####################  | Elapsed Time: 0:01:10 ETA:   0:00:05\n",
      " 93% (293 of 313) |####################  | Elapsed Time: 0:01:10 ETA:   0:00:04\n",
      " 93% (294 of 313) |####################  | Elapsed Time: 0:01:10 ETA:   0:00:04\n",
      " 94% (295 of 313) |####################  | Elapsed Time: 0:01:10 ETA:   0:00:04\n",
      " 94% (296 of 313) |####################  | Elapsed Time: 0:01:11 ETA:   0:00:04\n",
      " 94% (297 of 313) |####################  | Elapsed Time: 0:01:11 ETA:   0:00:03\n",
      " 95% (298 of 313) |####################  | Elapsed Time: 0:01:11 ETA:   0:00:03\n",
      " 95% (299 of 313) |##################### | Elapsed Time: 0:01:11 ETA:   0:00:03\n",
      " 95% (300 of 313) |##################### | Elapsed Time: 0:01:11 ETA:   0:00:03\n",
      " 96% (301 of 313) |##################### | Elapsed Time: 0:01:12 ETA:   0:00:02\n",
      " 96% (302 of 313) |##################### | Elapsed Time: 0:01:12 ETA:   0:00:02\n",
      " 96% (303 of 313) |##################### | Elapsed Time: 0:01:12 ETA:   0:00:02\n",
      " 97% (304 of 313) |##################### | Elapsed Time: 0:01:12 ETA:   0:00:02\n",
      " 97% (305 of 313) |##################### | Elapsed Time: 0:01:12 ETA:   0:00:01\n",
      " 97% (306 of 313) |##################### | Elapsed Time: 0:01:13 ETA:   0:00:01\n",
      " 98% (307 of 313) |##################### | Elapsed Time: 0:01:13 ETA:   0:00:01\n",
      " 98% (308 of 313) |##################### | Elapsed Time: 0:01:13 ETA:   0:00:01\n",
      " 98% (309 of 313) |##################### | Elapsed Time: 0:01:13 ETA:   0:00:00\n",
      " 99% (310 of 313) |##################### | Elapsed Time: 0:01:14 ETA:   0:00:00\n",
      " 99% (311 of 313) |##################### | Elapsed Time: 0:01:14 ETA:   0:00:00\n",
      " 99% (312 of 313) |##################### | Elapsed Time: 0:01:14 ETA:   0:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:14 ETA:  00:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:14 Time:  0:01:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:06:51,073 - Eval - INFO - Avg accuracy Top 1: 71.285942 Avg accuracy Top 5: 90.335463 on validation Dataset\n",
      "71.28594249201278\n"
     ]
    }
   ],
   "source": [
    "accuracy = ImageNetDataPipeline.evaluate(model, use_cuda)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Create a quantization simulation model and determine quantized accuracy\n",
    "\n",
    "## Fold Batch Normalization layers\n",
    "Before we determine the simulated quantized accuracy using QuantizationSimModel, we will fold the BatchNormalization (BN) layers in the model. These layers get folded into adjacent Convolutional layers. The BN layers that cannot be folded are left as they are.\n",
    "\n",
    "**Why do we need to this?**\n",
    "On quantized runtimes (like TFLite, SnapDragon Neural Processing SDK, etc.), it is a common practice to fold the BN layers. Doing so, results in an inferences/sec speedup since unnecessary computation is avoided. Now from a floating point compute perspective, a BN-folded model is mathematically equivalent to a model with BN layers from an inference perspective, and produces the same accuracy. However, folding the BN layers can increase the range of the tensor values for the weight parameters of the adjacent layers. And this can have a negative impact on the quantized accuracy of the model (especially when using INT8 or lower precision). So, we want to simulate that on-target behavior by doing BN folding here.\n",
    "\n",
    "The following code calls AIMET to fold the BN layers in-place on the given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:07:10,439 - BatchNormFolding - INFO - 0 BatchNorms' weights got converted\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "\n",
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Quantization Sim Model\n",
    "Now we use AIMET to create a QuantizationSimModel. This basically means that AIMET will insert fake quantization ops in the model graph and will configure them.\n",
    "A few of the parameters are explained here\n",
    "- **quant_scheme**: We set this to \"QuantScheme.post_training_tf_enhanced\"\n",
    "    - Supported options are 'tf_enhanced' or 'tf' or using Quant Scheme Enum QuantScheme.post_training_tf or QuantScheme.post_training_tf_enhanced\n",
    "- **default_output_bw**: Setting this to 8, essentially means that we are asking AIMET to perform all activation quantizations in the model using integer 8-bit precision\n",
    "- **default_param_bw**: Setting this to 8, essentially means that we are asking AIMET to perform all parameter quantizations in the model using integer 8-bit precision\n",
    "- **num_batches**: The number of batches used to evaluate the model while calculating the quantization encodings.Number of batches to use for computing encodings. Only 5 batches are used here to speed up the process. In addition, the number of images in these 5 batches should be sufficient for compute encodings\n",
    "- **rounding_mode**: The rounding mode used for quantization. There are two possible choices here - 'nearest' or 'stochastic' We will use \"nearest.\"\n",
    "\n",
    "There are other parameters that are set to default values in this example. Please check the AIMET API documentation of QuantizationSimModel to see reference documentation for all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:07:11,769 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-09-09 08:07:11,820 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-09-09 08:07:11,821 - Quant - INFO - Unsupported op type Mean\n",
      "2024-09-09 08:07:11,829 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "\n",
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can check the modifications AIMET has made to the model graph. One way is to print the model, and we can see that AIMET has added quantization wrapper layers. Note: use sim.model to access the modified PyTorch model. By default, AIMET creates a copy of the original model prior to modifying it. There is a parameter to override this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (features): Module(\n",
      "    (0): Module(\n",
      "      (0): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (4): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_1): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (6): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_2): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (7): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_3): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (9): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_4): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (10): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_5): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (11): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_6): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (13): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_7): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (14): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_8): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (16): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (module_add_9): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Add()\n",
      "      )\n",
      "    )\n",
      "    (17): Module(\n",
      "      (conv): Module(\n",
      "        (0): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Module(\n",
      "          (0): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): StaticGridQuantWrapper(\n",
      "            (_module_to_wrap): ReLU6(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): StaticGridQuantWrapper(\n",
      "          (_module_to_wrap): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "    )\n",
      "    (18): Module(\n",
      "      (0): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): StaticGridQuantWrapper(\n",
      "        (_module_to_wrap): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Module(\n",
      "    (0): StaticGridQuantWrapper(\n",
      "      (_module_to_wrap): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): StaticGridQuantWrapper(\n",
      "      (_module_to_wrap): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (module_adaptive_avg_pool2d): StaticGridQuantWrapper(\n",
      "    (_module_to_wrap): AdaptiveAvgPool2d()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    features_0_0 = getattr(getattr(self.features, \"0\"), \"0\")(x);  x = None\n",
      "    features_0_1 = getattr(getattr(self.features, \"0\"), \"1\")(features_0_0);  features_0_0 = None\n",
      "    features_0_2 = getattr(getattr(self.features, \"0\"), \"2\")(features_0_1);  features_0_1 = None\n",
      "    features_1_conv_0_0 = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"0\")(features_0_2);  features_0_2 = None\n",
      "    features_1_conv_0_1 = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"1\")(features_1_conv_0_0);  features_1_conv_0_0 = None\n",
      "    features_1_conv_0_2 = getattr(getattr(getattr(self.features, \"1\").conv, \"0\"), \"2\")(features_1_conv_0_1);  features_1_conv_0_1 = None\n",
      "    features_1_conv_1 = getattr(getattr(self.features, \"1\").conv, \"1\")(features_1_conv_0_2);  features_1_conv_0_2 = None\n",
      "    features_1_conv_2 = getattr(getattr(self.features, \"1\").conv, \"2\")(features_1_conv_1);  features_1_conv_1 = None\n",
      "    features_2_conv_0_0 = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"0\")(features_1_conv_2);  features_1_conv_2 = None\n",
      "    features_2_conv_0_1 = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"1\")(features_2_conv_0_0);  features_2_conv_0_0 = None\n",
      "    features_2_conv_0_2 = getattr(getattr(getattr(self.features, \"2\").conv, \"0\"), \"2\")(features_2_conv_0_1);  features_2_conv_0_1 = None\n",
      "    features_2_conv_1_0 = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"0\")(features_2_conv_0_2);  features_2_conv_0_2 = None\n",
      "    features_2_conv_1_1 = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"1\")(features_2_conv_1_0);  features_2_conv_1_0 = None\n",
      "    features_2_conv_1_2 = getattr(getattr(getattr(self.features, \"2\").conv, \"1\"), \"2\")(features_2_conv_1_1);  features_2_conv_1_1 = None\n",
      "    features_2_conv_2 = getattr(getattr(self.features, \"2\").conv, \"2\")(features_2_conv_1_2);  features_2_conv_1_2 = None\n",
      "    features_2_conv_3 = getattr(getattr(self.features, \"2\").conv, \"3\")(features_2_conv_2);  features_2_conv_2 = None\n",
      "    features_3_conv_0_0 = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"0\")(features_2_conv_3)\n",
      "    features_3_conv_0_1 = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"1\")(features_3_conv_0_0);  features_3_conv_0_0 = None\n",
      "    features_3_conv_0_2 = getattr(getattr(getattr(self.features, \"3\").conv, \"0\"), \"2\")(features_3_conv_0_1);  features_3_conv_0_1 = None\n",
      "    features_3_conv_1_0 = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"0\")(features_3_conv_0_2);  features_3_conv_0_2 = None\n",
      "    features_3_conv_1_1 = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"1\")(features_3_conv_1_0);  features_3_conv_1_0 = None\n",
      "    features_3_conv_1_2 = getattr(getattr(getattr(self.features, \"3\").conv, \"1\"), \"2\")(features_3_conv_1_1);  features_3_conv_1_1 = None\n",
      "    features_3_conv_2 = getattr(getattr(self.features, \"3\").conv, \"2\")(features_3_conv_1_2);  features_3_conv_1_2 = None\n",
      "    features_3_conv_3 = getattr(getattr(self.features, \"3\").conv, \"3\")(features_3_conv_2);  features_3_conv_2 = None\n",
      "    features_3_module_add = getattr(self.features, \"3\").module_add(features_2_conv_3, features_3_conv_3);  features_2_conv_3 = features_3_conv_3 = None\n",
      "    features_4_conv_0_0 = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"0\")(features_3_module_add);  features_3_module_add = None\n",
      "    features_4_conv_0_1 = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"1\")(features_4_conv_0_0);  features_4_conv_0_0 = None\n",
      "    features_4_conv_0_2 = getattr(getattr(getattr(self.features, \"4\").conv, \"0\"), \"2\")(features_4_conv_0_1);  features_4_conv_0_1 = None\n",
      "    features_4_conv_1_0 = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"0\")(features_4_conv_0_2);  features_4_conv_0_2 = None\n",
      "    features_4_conv_1_1 = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"1\")(features_4_conv_1_0);  features_4_conv_1_0 = None\n",
      "    features_4_conv_1_2 = getattr(getattr(getattr(self.features, \"4\").conv, \"1\"), \"2\")(features_4_conv_1_1);  features_4_conv_1_1 = None\n",
      "    features_4_conv_2 = getattr(getattr(self.features, \"4\").conv, \"2\")(features_4_conv_1_2);  features_4_conv_1_2 = None\n",
      "    features_4_conv_3 = getattr(getattr(self.features, \"4\").conv, \"3\")(features_4_conv_2);  features_4_conv_2 = None\n",
      "    features_5_conv_0_0 = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"0\")(features_4_conv_3)\n",
      "    features_5_conv_0_1 = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"1\")(features_5_conv_0_0);  features_5_conv_0_0 = None\n",
      "    features_5_conv_0_2 = getattr(getattr(getattr(self.features, \"5\").conv, \"0\"), \"2\")(features_5_conv_0_1);  features_5_conv_0_1 = None\n",
      "    features_5_conv_1_0 = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"0\")(features_5_conv_0_2);  features_5_conv_0_2 = None\n",
      "    features_5_conv_1_1 = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"1\")(features_5_conv_1_0);  features_5_conv_1_0 = None\n",
      "    features_5_conv_1_2 = getattr(getattr(getattr(self.features, \"5\").conv, \"1\"), \"2\")(features_5_conv_1_1);  features_5_conv_1_1 = None\n",
      "    features_5_conv_2 = getattr(getattr(self.features, \"5\").conv, \"2\")(features_5_conv_1_2);  features_5_conv_1_2 = None\n",
      "    features_5_conv_3 = getattr(getattr(self.features, \"5\").conv, \"3\")(features_5_conv_2);  features_5_conv_2 = None\n",
      "    features_5_module_add_1 = getattr(self.features, \"5\").module_add_1(features_4_conv_3, features_5_conv_3);  features_4_conv_3 = features_5_conv_3 = None\n",
      "    features_6_conv_0_0 = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"0\")(features_5_module_add_1)\n",
      "    features_6_conv_0_1 = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"1\")(features_6_conv_0_0);  features_6_conv_0_0 = None\n",
      "    features_6_conv_0_2 = getattr(getattr(getattr(self.features, \"6\").conv, \"0\"), \"2\")(features_6_conv_0_1);  features_6_conv_0_1 = None\n",
      "    features_6_conv_1_0 = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"0\")(features_6_conv_0_2);  features_6_conv_0_2 = None\n",
      "    features_6_conv_1_1 = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"1\")(features_6_conv_1_0);  features_6_conv_1_0 = None\n",
      "    features_6_conv_1_2 = getattr(getattr(getattr(self.features, \"6\").conv, \"1\"), \"2\")(features_6_conv_1_1);  features_6_conv_1_1 = None\n",
      "    features_6_conv_2 = getattr(getattr(self.features, \"6\").conv, \"2\")(features_6_conv_1_2);  features_6_conv_1_2 = None\n",
      "    features_6_conv_3 = getattr(getattr(self.features, \"6\").conv, \"3\")(features_6_conv_2);  features_6_conv_2 = None\n",
      "    features_6_module_add_2 = getattr(self.features, \"6\").module_add_2(features_5_module_add_1, features_6_conv_3);  features_5_module_add_1 = features_6_conv_3 = None\n",
      "    features_7_conv_0_0 = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"0\")(features_6_module_add_2);  features_6_module_add_2 = None\n",
      "    features_7_conv_0_1 = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"1\")(features_7_conv_0_0);  features_7_conv_0_0 = None\n",
      "    features_7_conv_0_2 = getattr(getattr(getattr(self.features, \"7\").conv, \"0\"), \"2\")(features_7_conv_0_1);  features_7_conv_0_1 = None\n",
      "    features_7_conv_1_0 = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"0\")(features_7_conv_0_2);  features_7_conv_0_2 = None\n",
      "    features_7_conv_1_1 = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"1\")(features_7_conv_1_0);  features_7_conv_1_0 = None\n",
      "    features_7_conv_1_2 = getattr(getattr(getattr(self.features, \"7\").conv, \"1\"), \"2\")(features_7_conv_1_1);  features_7_conv_1_1 = None\n",
      "    features_7_conv_2 = getattr(getattr(self.features, \"7\").conv, \"2\")(features_7_conv_1_2);  features_7_conv_1_2 = None\n",
      "    features_7_conv_3 = getattr(getattr(self.features, \"7\").conv, \"3\")(features_7_conv_2);  features_7_conv_2 = None\n",
      "    features_8_conv_0_0 = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"0\")(features_7_conv_3)\n",
      "    features_8_conv_0_1 = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"1\")(features_8_conv_0_0);  features_8_conv_0_0 = None\n",
      "    features_8_conv_0_2 = getattr(getattr(getattr(self.features, \"8\").conv, \"0\"), \"2\")(features_8_conv_0_1);  features_8_conv_0_1 = None\n",
      "    features_8_conv_1_0 = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"0\")(features_8_conv_0_2);  features_8_conv_0_2 = None\n",
      "    features_8_conv_1_1 = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"1\")(features_8_conv_1_0);  features_8_conv_1_0 = None\n",
      "    features_8_conv_1_2 = getattr(getattr(getattr(self.features, \"8\").conv, \"1\"), \"2\")(features_8_conv_1_1);  features_8_conv_1_1 = None\n",
      "    features_8_conv_2 = getattr(getattr(self.features, \"8\").conv, \"2\")(features_8_conv_1_2);  features_8_conv_1_2 = None\n",
      "    features_8_conv_3 = getattr(getattr(self.features, \"8\").conv, \"3\")(features_8_conv_2);  features_8_conv_2 = None\n",
      "    features_8_module_add_3 = getattr(self.features, \"8\").module_add_3(features_7_conv_3, features_8_conv_3);  features_7_conv_3 = features_8_conv_3 = None\n",
      "    features_9_conv_0_0 = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"0\")(features_8_module_add_3)\n",
      "    features_9_conv_0_1 = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"1\")(features_9_conv_0_0);  features_9_conv_0_0 = None\n",
      "    features_9_conv_0_2 = getattr(getattr(getattr(self.features, \"9\").conv, \"0\"), \"2\")(features_9_conv_0_1);  features_9_conv_0_1 = None\n",
      "    features_9_conv_1_0 = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"0\")(features_9_conv_0_2);  features_9_conv_0_2 = None\n",
      "    features_9_conv_1_1 = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"1\")(features_9_conv_1_0);  features_9_conv_1_0 = None\n",
      "    features_9_conv_1_2 = getattr(getattr(getattr(self.features, \"9\").conv, \"1\"), \"2\")(features_9_conv_1_1);  features_9_conv_1_1 = None\n",
      "    features_9_conv_2 = getattr(getattr(self.features, \"9\").conv, \"2\")(features_9_conv_1_2);  features_9_conv_1_2 = None\n",
      "    features_9_conv_3 = getattr(getattr(self.features, \"9\").conv, \"3\")(features_9_conv_2);  features_9_conv_2 = None\n",
      "    features_9_module_add_4 = getattr(self.features, \"9\").module_add_4(features_8_module_add_3, features_9_conv_3);  features_8_module_add_3 = features_9_conv_3 = None\n",
      "    features_10_conv_0_0 = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"0\")(features_9_module_add_4)\n",
      "    features_10_conv_0_1 = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"1\")(features_10_conv_0_0);  features_10_conv_0_0 = None\n",
      "    features_10_conv_0_2 = getattr(getattr(getattr(self.features, \"10\").conv, \"0\"), \"2\")(features_10_conv_0_1);  features_10_conv_0_1 = None\n",
      "    features_10_conv_1_0 = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"0\")(features_10_conv_0_2);  features_10_conv_0_2 = None\n",
      "    features_10_conv_1_1 = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"1\")(features_10_conv_1_0);  features_10_conv_1_0 = None\n",
      "    features_10_conv_1_2 = getattr(getattr(getattr(self.features, \"10\").conv, \"1\"), \"2\")(features_10_conv_1_1);  features_10_conv_1_1 = None\n",
      "    features_10_conv_2 = getattr(getattr(self.features, \"10\").conv, \"2\")(features_10_conv_1_2);  features_10_conv_1_2 = None\n",
      "    features_10_conv_3 = getattr(getattr(self.features, \"10\").conv, \"3\")(features_10_conv_2);  features_10_conv_2 = None\n",
      "    features_10_module_add_5 = getattr(self.features, \"10\").module_add_5(features_9_module_add_4, features_10_conv_3);  features_9_module_add_4 = features_10_conv_3 = None\n",
      "    features_11_conv_0_0 = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"0\")(features_10_module_add_5);  features_10_module_add_5 = None\n",
      "    features_11_conv_0_1 = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"1\")(features_11_conv_0_0);  features_11_conv_0_0 = None\n",
      "    features_11_conv_0_2 = getattr(getattr(getattr(self.features, \"11\").conv, \"0\"), \"2\")(features_11_conv_0_1);  features_11_conv_0_1 = None\n",
      "    features_11_conv_1_0 = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"0\")(features_11_conv_0_2);  features_11_conv_0_2 = None\n",
      "    features_11_conv_1_1 = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"1\")(features_11_conv_1_0);  features_11_conv_1_0 = None\n",
      "    features_11_conv_1_2 = getattr(getattr(getattr(self.features, \"11\").conv, \"1\"), \"2\")(features_11_conv_1_1);  features_11_conv_1_1 = None\n",
      "    features_11_conv_2 = getattr(getattr(self.features, \"11\").conv, \"2\")(features_11_conv_1_2);  features_11_conv_1_2 = None\n",
      "    features_11_conv_3 = getattr(getattr(self.features, \"11\").conv, \"3\")(features_11_conv_2);  features_11_conv_2 = None\n",
      "    features_12_conv_0_0 = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"0\")(features_11_conv_3)\n",
      "    features_12_conv_0_1 = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"1\")(features_12_conv_0_0);  features_12_conv_0_0 = None\n",
      "    features_12_conv_0_2 = getattr(getattr(getattr(self.features, \"12\").conv, \"0\"), \"2\")(features_12_conv_0_1);  features_12_conv_0_1 = None\n",
      "    features_12_conv_1_0 = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"0\")(features_12_conv_0_2);  features_12_conv_0_2 = None\n",
      "    features_12_conv_1_1 = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"1\")(features_12_conv_1_0);  features_12_conv_1_0 = None\n",
      "    features_12_conv_1_2 = getattr(getattr(getattr(self.features, \"12\").conv, \"1\"), \"2\")(features_12_conv_1_1);  features_12_conv_1_1 = None\n",
      "    features_12_conv_2 = getattr(getattr(self.features, \"12\").conv, \"2\")(features_12_conv_1_2);  features_12_conv_1_2 = None\n",
      "    features_12_conv_3 = getattr(getattr(self.features, \"12\").conv, \"3\")(features_12_conv_2);  features_12_conv_2 = None\n",
      "    features_12_module_add_6 = getattr(self.features, \"12\").module_add_6(features_11_conv_3, features_12_conv_3);  features_11_conv_3 = features_12_conv_3 = None\n",
      "    features_13_conv_0_0 = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"0\")(features_12_module_add_6)\n",
      "    features_13_conv_0_1 = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"1\")(features_13_conv_0_0);  features_13_conv_0_0 = None\n",
      "    features_13_conv_0_2 = getattr(getattr(getattr(self.features, \"13\").conv, \"0\"), \"2\")(features_13_conv_0_1);  features_13_conv_0_1 = None\n",
      "    features_13_conv_1_0 = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"0\")(features_13_conv_0_2);  features_13_conv_0_2 = None\n",
      "    features_13_conv_1_1 = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"1\")(features_13_conv_1_0);  features_13_conv_1_0 = None\n",
      "    features_13_conv_1_2 = getattr(getattr(getattr(self.features, \"13\").conv, \"1\"), \"2\")(features_13_conv_1_1);  features_13_conv_1_1 = None\n",
      "    features_13_conv_2 = getattr(getattr(self.features, \"13\").conv, \"2\")(features_13_conv_1_2);  features_13_conv_1_2 = None\n",
      "    features_13_conv_3 = getattr(getattr(self.features, \"13\").conv, \"3\")(features_13_conv_2);  features_13_conv_2 = None\n",
      "    features_13_module_add_7 = getattr(self.features, \"13\").module_add_7(features_12_module_add_6, features_13_conv_3);  features_12_module_add_6 = features_13_conv_3 = None\n",
      "    features_14_conv_0_0 = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"0\")(features_13_module_add_7);  features_13_module_add_7 = None\n",
      "    features_14_conv_0_1 = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"1\")(features_14_conv_0_0);  features_14_conv_0_0 = None\n",
      "    features_14_conv_0_2 = getattr(getattr(getattr(self.features, \"14\").conv, \"0\"), \"2\")(features_14_conv_0_1);  features_14_conv_0_1 = None\n",
      "    features_14_conv_1_0 = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"0\")(features_14_conv_0_2);  features_14_conv_0_2 = None\n",
      "    features_14_conv_1_1 = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"1\")(features_14_conv_1_0);  features_14_conv_1_0 = None\n",
      "    features_14_conv_1_2 = getattr(getattr(getattr(self.features, \"14\").conv, \"1\"), \"2\")(features_14_conv_1_1);  features_14_conv_1_1 = None\n",
      "    features_14_conv_2 = getattr(getattr(self.features, \"14\").conv, \"2\")(features_14_conv_1_2);  features_14_conv_1_2 = None\n",
      "    features_14_conv_3 = getattr(getattr(self.features, \"14\").conv, \"3\")(features_14_conv_2);  features_14_conv_2 = None\n",
      "    features_15_conv_0_0 = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"0\")(features_14_conv_3)\n",
      "    features_15_conv_0_1 = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"1\")(features_15_conv_0_0);  features_15_conv_0_0 = None\n",
      "    features_15_conv_0_2 = getattr(getattr(getattr(self.features, \"15\").conv, \"0\"), \"2\")(features_15_conv_0_1);  features_15_conv_0_1 = None\n",
      "    features_15_conv_1_0 = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"0\")(features_15_conv_0_2);  features_15_conv_0_2 = None\n",
      "    features_15_conv_1_1 = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"1\")(features_15_conv_1_0);  features_15_conv_1_0 = None\n",
      "    features_15_conv_1_2 = getattr(getattr(getattr(self.features, \"15\").conv, \"1\"), \"2\")(features_15_conv_1_1);  features_15_conv_1_1 = None\n",
      "    features_15_conv_2 = getattr(getattr(self.features, \"15\").conv, \"2\")(features_15_conv_1_2);  features_15_conv_1_2 = None\n",
      "    features_15_conv_3 = getattr(getattr(self.features, \"15\").conv, \"3\")(features_15_conv_2);  features_15_conv_2 = None\n",
      "    features_15_module_add_8 = getattr(self.features, \"15\").module_add_8(features_14_conv_3, features_15_conv_3);  features_14_conv_3 = features_15_conv_3 = None\n",
      "    features_16_conv_0_0 = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"0\")(features_15_module_add_8)\n",
      "    features_16_conv_0_1 = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"1\")(features_16_conv_0_0);  features_16_conv_0_0 = None\n",
      "    features_16_conv_0_2 = getattr(getattr(getattr(self.features, \"16\").conv, \"0\"), \"2\")(features_16_conv_0_1);  features_16_conv_0_1 = None\n",
      "    features_16_conv_1_0 = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"0\")(features_16_conv_0_2);  features_16_conv_0_2 = None\n",
      "    features_16_conv_1_1 = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"1\")(features_16_conv_1_0);  features_16_conv_1_0 = None\n",
      "    features_16_conv_1_2 = getattr(getattr(getattr(self.features, \"16\").conv, \"1\"), \"2\")(features_16_conv_1_1);  features_16_conv_1_1 = None\n",
      "    features_16_conv_2 = getattr(getattr(self.features, \"16\").conv, \"2\")(features_16_conv_1_2);  features_16_conv_1_2 = None\n",
      "    features_16_conv_3 = getattr(getattr(self.features, \"16\").conv, \"3\")(features_16_conv_2);  features_16_conv_2 = None\n",
      "    features_16_module_add_9 = getattr(self.features, \"16\").module_add_9(features_15_module_add_8, features_16_conv_3);  features_15_module_add_8 = features_16_conv_3 = None\n",
      "    features_17_conv_0_0 = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"0\")(features_16_module_add_9);  features_16_module_add_9 = None\n",
      "    features_17_conv_0_1 = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"1\")(features_17_conv_0_0);  features_17_conv_0_0 = None\n",
      "    features_17_conv_0_2 = getattr(getattr(getattr(self.features, \"17\").conv, \"0\"), \"2\")(features_17_conv_0_1);  features_17_conv_0_1 = None\n",
      "    features_17_conv_1_0 = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"0\")(features_17_conv_0_2);  features_17_conv_0_2 = None\n",
      "    features_17_conv_1_1 = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"1\")(features_17_conv_1_0);  features_17_conv_1_0 = None\n",
      "    features_17_conv_1_2 = getattr(getattr(getattr(self.features, \"17\").conv, \"1\"), \"2\")(features_17_conv_1_1);  features_17_conv_1_1 = None\n",
      "    features_17_conv_2 = getattr(getattr(self.features, \"17\").conv, \"2\")(features_17_conv_1_2);  features_17_conv_1_2 = None\n",
      "    features_17_conv_3 = getattr(getattr(self.features, \"17\").conv, \"3\")(features_17_conv_2);  features_17_conv_2 = None\n",
      "    features_18_0 = getattr(getattr(self.features, \"18\"), \"0\")(features_17_conv_3);  features_17_conv_3 = None\n",
      "    features_18_1 = getattr(getattr(self.features, \"18\"), \"1\")(features_18_0);  features_18_0 = None\n",
      "    features_18_2 = getattr(getattr(self.features, \"18\"), \"2\")(features_18_1);  features_18_1 = None\n",
      "    module_adaptive_avg_pool2d = self.module_adaptive_avg_pool2d(features_18_2, (1, 1));  features_18_2 = None\n",
      "    flatten = torch.flatten(module_adaptive_avg_pool2d, 1);  module_adaptive_avg_pool2d = None\n",
      "    classifier_0 = getattr(self.classifier, \"0\")(flatten);  flatten = None\n",
      "    classifier_1 = getattr(self.classifier, \"1\")(classifier_0);  classifier_0 = None\n",
      "    return classifier_1\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can also check how AIMET has configured the added fake quantization nodes, which AIMET refers to as 'quantizers'. You can see this by printing the sim object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.0\n",
      "  Input[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.1.conv.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.2.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.3.module_add\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.4.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.5.module_add_1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.6.module_add_2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.7.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.8.module_add_3\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.9.module_add_4\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.10.module_add_5\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.11.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.12.module_add_6\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.13.module_add_7\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.14.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.15.module_add_8\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.16.module_add_9\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.0.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.1.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.17.conv.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: features.18.2\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: classifier.0\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Output[0]: Not quantized\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: classifier.1\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Param[weight]: bw=8, encoding-present=False\n",
      "  -------\n",
      "  Param[bias]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "----------------------------------------------------------\n",
      "Layer: module_adaptive_avg_pool2d\n",
      "  Input[0]: Not quantized\n",
      "  -------\n",
      "  Input[1]: Not quantized\n",
      "  -------\n",
      "  Output[0]: bw=8, encoding-present=False\n",
      "  -------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Even though AIMET has added 'quantizer' nodes to the model graph but the model is not ready to be used yet. Before we can use the sim model for inference or training, we need to find appropriate scale/offset quantization parameters for each 'quantizer' node. For activation quantization nodes, we need to pass unlabeled data samples through the model to collect range statistics which will then let AIMET calculate appropriate scale/offset quantization parameters. This process is sometimes referred to as calibration. AIMET simply refers to it as 'computing encodings'.\n",
    "\n",
    "So we create a routine to pass unlabeled data samples through the model. This should be fairly simple - use the existing train or validation data loader to extract some samples and pass them to the model. We don't need to compute any loss metric etc. So we can just ignore the model output for this purpose. A few pointers regarding the data samples\n",
    "\n",
    "In practice, we need a very small percentage of the overall data samples for computing encodings. For example, the training dataset for ImageNet has 1M samples. For computing encodings we only need 500 or 1000 samples.\n",
    "It may be beneficial if the samples used for computing encoding are well distributed. It's not necessary that all classes need to be covered etc. since we are only looking at the range of values at every layer activation. However, we definitely want to avoid an extreme scenario like all 'dark' or 'light' samples are used - e.g. only using pictures captured at night might not give ideal results.\n",
    "The following shows an example of a routine that passes unlabeled samples through the model for computing encodings. This routine can be written in many different ways, this is just an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we call AIMET to use the above routine to pass data through the model and then subsequently compute the quantization encodings. Encodings here refer to scale/offset quantization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:07:12,195 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now the QuantizationSim model is ready to be used for inference or training. First we can pass this model to the same evaluation routine we used before. The evaluation routine will now give us a simulated quantized accuracy score for INT8 quantization instead of the FP32 accuracy score we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:07:33,552 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:07:33,555 - Eval - INFO - No value of iteration is provided, running evaluation on complete dataset.\n",
      "2024-09-09 08:07:33,556 - Eval - INFO - Evaluating nn.Module for 313 iterations with batch_size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 313) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "  0% (2 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:48\n",
      "  0% (3 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:01:05\n",
      "  1% (4 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:01:08\n",
      "  1% (5 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:12\n",
      "  1% (6 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:16\n",
      "  2% (7 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:17\n",
      "  2% (8 of 313) |                        | Elapsed Time: 0:00:02 ETA:   0:01:20\n",
      "  2% (9 of 313) |                        | Elapsed Time: 0:00:02 ETA:   0:01:21\n",
      "  3% (10 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:01:22\n",
      "  3% (11 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:23\n",
      "  3% (12 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:26\n",
      "  4% (13 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:26\n",
      "  4% (14 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:26\n",
      "  4% (15 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:26\n",
      "  5% (16 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:26\n",
      "  5% (17 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:27\n",
      "  5% (18 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:26\n",
      "  6% (19 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:26\n",
      "  6% (20 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:25\n",
      "  6% (21 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:25\n",
      "  7% (22 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:25\n",
      "  7% (23 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:24\n",
      "  7% (24 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:24\n",
      "  7% (25 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:24\n",
      "  8% (26 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:24\n",
      "  8% (27 of 313) |#                      | Elapsed Time: 0:00:08 ETA:   0:01:25\n",
      "  8% (28 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:25\n",
      "  9% (29 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:24\n",
      "  9% (30 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:24\n",
      "  9% (31 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:24\n",
      " 10% (32 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:24\n",
      " 10% (33 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:24\n",
      " 10% (34 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:23\n",
      " 11% (35 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:23\n",
      " 11% (36 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:23\n",
      " 11% (37 of 313) |##                     | Elapsed Time: 0:00:11 ETA:   0:01:23\n",
      " 12% (38 of 313) |##                     | Elapsed Time: 0:00:11 ETA:   0:01:23\n",
      " 12% (39 of 313) |##                     | Elapsed Time: 0:00:12 ETA:   0:01:25\n",
      " 12% (40 of 313) |##                     | Elapsed Time: 0:00:12 ETA:   0:01:25\n",
      " 13% (41 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:25\n",
      " 13% (42 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:24\n",
      " 13% (43 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:24\n",
      " 14% (44 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:23\n",
      " 14% (45 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:23\n",
      " 14% (46 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:23\n",
      " 15% (47 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:22\n",
      " 15% (48 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:22\n",
      " 15% (49 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:21\n",
      " 15% (50 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:21\n",
      " 16% (51 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:20\n",
      " 16% (52 of 313) |###                    | Elapsed Time: 0:00:16 ETA:   0:01:20\n",
      " 16% (53 of 313) |###                    | Elapsed Time: 0:00:16 ETA:   0:01:20\n",
      " 17% (54 of 313) |###                    | Elapsed Time: 0:00:16 ETA:   0:01:19\n",
      " 17% (55 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:19\n",
      " 17% (56 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:18\n",
      " 18% (57 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:18\n",
      " 18% (58 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:18\n",
      " 18% (59 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:17\n",
      " 19% (60 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:17\n",
      " 19% (61 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:17\n",
      " 19% (62 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:16\n",
      " 20% (63 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:16\n",
      " 20% (64 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:15\n",
      " 20% (65 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:15\n",
      " 21% (66 of 313) |####                   | Elapsed Time: 0:00:20 ETA:   0:01:15\n",
      " 21% (67 of 313) |####                   | Elapsed Time: 0:00:20 ETA:   0:01:15\n",
      " 21% (68 of 313) |####                   | Elapsed Time: 0:00:20 ETA:   0:01:15\n",
      " 22% (69 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:14\n",
      " 22% (70 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:14\n",
      " 22% (71 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:14\n",
      " 23% (72 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:14\n",
      " 23% (73 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:13\n",
      " 23% (74 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:13\n",
      " 23% (75 of 313) |#####                  | Elapsed Time: 0:00:23 ETA:   0:01:13\n",
      " 24% (76 of 313) |#####                  | Elapsed Time: 0:00:23 ETA:   0:01:12\n",
      " 24% (77 of 313) |#####                  | Elapsed Time: 0:00:23 ETA:   0:01:12\n",
      " 24% (78 of 313) |#####                  | Elapsed Time: 0:00:23 ETA:   0:01:12\n",
      " 25% (79 of 313) |#####                  | Elapsed Time: 0:00:24 ETA:   0:01:11\n",
      " 25% (80 of 313) |#####                  | Elapsed Time: 0:00:24 ETA:   0:01:11\n",
      " 25% (81 of 313) |#####                  | Elapsed Time: 0:00:24 ETA:   0:01:10\n",
      " 26% (82 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:10\n",
      " 26% (83 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:01:10\n",
      " 26% (84 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:01:10\n",
      " 27% (85 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:09\n",
      " 27% (86 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:09\n",
      " 27% (87 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:09\n",
      " 28% (88 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:08\n",
      " 28% (89 of 313) |######                 | Elapsed Time: 0:00:27 ETA:   0:01:08\n",
      " 28% (90 of 313) |######                 | Elapsed Time: 0:00:27 ETA:   0:01:08\n",
      " 29% (91 of 313) |######                 | Elapsed Time: 0:00:27 ETA:   0:01:07\n",
      " 29% (92 of 313) |######                 | Elapsed Time: 0:00:28 ETA:   0:01:07\n",
      " 29% (93 of 313) |######                 | Elapsed Time: 0:00:28 ETA:   0:01:07\n",
      " 30% (94 of 313) |######                 | Elapsed Time: 0:00:28 ETA:   0:01:07\n",
      " 30% (95 of 313) |######                 | Elapsed Time: 0:00:29 ETA:   0:01:07\n",
      " 30% (96 of 313) |#######                | Elapsed Time: 0:00:29 ETA:   0:01:07\n",
      " 30% (97 of 313) |#######                | Elapsed Time: 0:00:29 ETA:   0:01:06\n",
      " 31% (98 of 313) |#######                | Elapsed Time: 0:00:30 ETA:   0:01:06\n",
      " 31% (99 of 313) |#######                | Elapsed Time: 0:00:30 ETA:   0:01:05\n",
      " 31% (100 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:01:05\n",
      " 32% (101 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:01:05\n",
      " 32% (102 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:01:04\n",
      " 32% (103 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:01:04\n",
      " 33% (104 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:01:04\n",
      " 33% (105 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:01:03\n",
      " 33% (106 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:01:03\n",
      " 34% (107 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:01:03\n",
      " 34% (108 of 313) |#######               | Elapsed Time: 0:00:33 ETA:   0:01:02\n",
      " 34% (109 of 313) |#######               | Elapsed Time: 0:00:33 ETA:   0:01:02\n",
      " 35% (110 of 313) |#######               | Elapsed Time: 0:00:33 ETA:   0:01:02\n",
      " 35% (111 of 313) |#######               | Elapsed Time: 0:00:34 ETA:   0:01:01\n",
      " 35% (112 of 313) |#######               | Elapsed Time: 0:00:34 ETA:   0:01:01\n",
      " 36% (113 of 313) |#######               | Elapsed Time: 0:00:34 ETA:   0:01:01\n",
      " 36% (114 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:01:01\n",
      " 36% (115 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:01:00\n",
      " 37% (116 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:01:00\n",
      " 37% (117 of 313) |########              | Elapsed Time: 0:00:36 ETA:   0:01:00\n",
      " 37% (118 of 313) |########              | Elapsed Time: 0:00:36 ETA:   0:00:59\n",
      " 38% (119 of 313) |########              | Elapsed Time: 0:00:36 ETA:   0:00:59\n",
      " 38% (120 of 313) |########              | Elapsed Time: 0:00:37 ETA:   0:00:59\n",
      " 38% (121 of 313) |########              | Elapsed Time: 0:00:37 ETA:   0:00:59\n",
      " 38% (122 of 313) |########              | Elapsed Time: 0:00:37 ETA:   0:00:59\n",
      " 39% (123 of 313) |########              | Elapsed Time: 0:00:38 ETA:   0:00:58\n",
      " 39% (124 of 313) |########              | Elapsed Time: 0:00:38 ETA:   0:00:58\n",
      " 39% (125 of 313) |########              | Elapsed Time: 0:00:38 ETA:   0:00:58\n",
      " 40% (126 of 313) |########              | Elapsed Time: 0:00:38 ETA:   0:00:57\n",
      " 40% (127 of 313) |########              | Elapsed Time: 0:00:39 ETA:   0:00:57\n",
      " 40% (128 of 313) |########              | Elapsed Time: 0:00:39 ETA:   0:00:57\n",
      " 41% (129 of 313) |#########             | Elapsed Time: 0:00:39 ETA:   0:00:56\n",
      " 41% (130 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:56\n",
      " 41% (131 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:55\n",
      " 42% (132 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:55\n",
      " 42% (133 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:55\n",
      " 42% (134 of 313) |#########             | Elapsed Time: 0:00:41 ETA:   0:00:55\n",
      " 43% (135 of 313) |#########             | Elapsed Time: 0:00:41 ETA:   0:00:54\n",
      " 43% (136 of 313) |#########             | Elapsed Time: 0:00:41 ETA:   0:00:54\n",
      " 43% (137 of 313) |#########             | Elapsed Time: 0:00:42 ETA:   0:00:53\n",
      " 44% (138 of 313) |#########             | Elapsed Time: 0:00:42 ETA:   0:00:53\n",
      " 44% (139 of 313) |#########             | Elapsed Time: 0:00:42 ETA:   0:00:53\n",
      " 44% (140 of 313) |#########             | Elapsed Time: 0:00:42 ETA:   0:00:52\n",
      " 45% (141 of 313) |#########             | Elapsed Time: 0:00:43 ETA:   0:00:52\n",
      " 45% (142 of 313) |#########             | Elapsed Time: 0:00:43 ETA:   0:00:52\n",
      " 45% (143 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:51\n",
      " 46% (144 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:51\n",
      " 46% (145 of 313) |##########            | Elapsed Time: 0:00:44 ETA:   0:00:51\n",
      " 46% (146 of 313) |##########            | Elapsed Time: 0:00:44 ETA:   0:00:51\n",
      " 46% (147 of 313) |##########            | Elapsed Time: 0:00:45 ETA:   0:00:51\n",
      " 47% (148 of 313) |##########            | Elapsed Time: 0:00:45 ETA:   0:00:50\n",
      " 47% (149 of 313) |##########            | Elapsed Time: 0:00:45 ETA:   0:00:50\n",
      " 47% (150 of 313) |##########            | Elapsed Time: 0:00:46 ETA:   0:00:50\n",
      " 48% (151 of 313) |##########            | Elapsed Time: 0:00:46 ETA:   0:00:49\n",
      " 48% (152 of 313) |##########            | Elapsed Time: 0:00:46 ETA:   0:00:49\n",
      " 48% (153 of 313) |##########            | Elapsed Time: 0:00:46 ETA:   0:00:49\n",
      " 49% (154 of 313) |##########            | Elapsed Time: 0:00:47 ETA:   0:00:48\n",
      " 49% (155 of 313) |##########            | Elapsed Time: 0:00:47 ETA:   0:00:48\n",
      " 49% (156 of 313) |##########            | Elapsed Time: 0:00:47 ETA:   0:00:48\n",
      " 50% (157 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:47\n",
      " 50% (158 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:47\n",
      " 50% (159 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:47\n",
      " 51% (160 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:46\n",
      " 51% (161 of 313) |###########           | Elapsed Time: 0:00:49 ETA:   0:00:46\n",
      " 51% (162 of 313) |###########           | Elapsed Time: 0:00:49 ETA:   0:00:46\n",
      " 52% (163 of 313) |###########           | Elapsed Time: 0:00:49 ETA:   0:00:45\n",
      " 52% (164 of 313) |###########           | Elapsed Time: 0:00:50 ETA:   0:00:45\n",
      " 52% (165 of 313) |###########           | Elapsed Time: 0:00:50 ETA:   0:00:45\n",
      " 53% (166 of 313) |###########           | Elapsed Time: 0:00:50 ETA:   0:00:44\n",
      " 53% (167 of 313) |###########           | Elapsed Time: 0:00:50 ETA:   0:00:44\n",
      " 53% (168 of 313) |###########           | Elapsed Time: 0:00:51 ETA:   0:00:44\n",
      " 53% (169 of 313) |###########           | Elapsed Time: 0:00:51 ETA:   0:00:43\n",
      " 54% (170 of 313) |###########           | Elapsed Time: 0:00:51 ETA:   0:00:43\n",
      " 54% (171 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:43\n",
      " 54% (172 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:42\n",
      " 55% (173 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:42\n",
      " 55% (174 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:42\n",
      " 55% (175 of 313) |############          | Elapsed Time: 0:00:53 ETA:   0:00:42\n",
      " 56% (176 of 313) |############          | Elapsed Time: 0:00:53 ETA:   0:00:41\n",
      " 56% (177 of 313) |############          | Elapsed Time: 0:00:54 ETA:   0:00:41\n",
      " 56% (178 of 313) |############          | Elapsed Time: 0:00:54 ETA:   0:00:41\n",
      " 57% (179 of 313) |############          | Elapsed Time: 0:00:54 ETA:   0:00:41\n",
      " 57% (180 of 313) |############          | Elapsed Time: 0:00:55 ETA:   0:00:40\n",
      " 57% (181 of 313) |############          | Elapsed Time: 0:00:55 ETA:   0:00:40\n",
      " 58% (182 of 313) |############          | Elapsed Time: 0:00:55 ETA:   0:00:40\n",
      " 58% (183 of 313) |############          | Elapsed Time: 0:00:56 ETA:   0:00:39\n",
      " 58% (184 of 313) |############          | Elapsed Time: 0:00:56 ETA:   0:00:39\n",
      " 59% (185 of 313) |#############         | Elapsed Time: 0:00:56 ETA:   0:00:39\n",
      " 59% (186 of 313) |#############         | Elapsed Time: 0:00:57 ETA:   0:00:39\n",
      " 59% (187 of 313) |#############         | Elapsed Time: 0:00:57 ETA:   0:00:38\n",
      " 60% (188 of 313) |#############         | Elapsed Time: 0:00:57 ETA:   0:00:38\n",
      " 60% (189 of 313) |#############         | Elapsed Time: 0:00:58 ETA:   0:00:38\n",
      " 60% (190 of 313) |#############         | Elapsed Time: 0:00:58 ETA:   0:00:37\n",
      " 61% (191 of 313) |#############         | Elapsed Time: 0:00:58 ETA:   0:00:37\n",
      " 61% (192 of 313) |#############         | Elapsed Time: 0:00:58 ETA:   0:00:37\n",
      " 61% (193 of 313) |#############         | Elapsed Time: 0:00:59 ETA:   0:00:36\n",
      " 61% (194 of 313) |#############         | Elapsed Time: 0:00:59 ETA:   0:00:36\n",
      " 62% (195 of 313) |#############         | Elapsed Time: 0:00:59 ETA:   0:00:36\n",
      " 62% (196 of 313) |#############         | Elapsed Time: 0:01:00 ETA:   0:00:35\n",
      " 62% (197 of 313) |#############         | Elapsed Time: 0:01:00 ETA:   0:00:35\n",
      " 63% (198 of 313) |#############         | Elapsed Time: 0:01:00 ETA:   0:00:35\n",
      " 63% (199 of 313) |#############         | Elapsed Time: 0:01:00 ETA:   0:00:34\n",
      " 63% (200 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:34\n",
      " 64% (201 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:34\n",
      " 64% (202 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:34\n",
      " 64% (203 of 313) |##############        | Elapsed Time: 0:01:02 ETA:   0:00:33\n",
      " 65% (204 of 313) |##############        | Elapsed Time: 0:01:02 ETA:   0:00:33\n",
      " 65% (205 of 313) |##############        | Elapsed Time: 0:01:02 ETA:   0:00:33\n",
      " 65% (206 of 313) |##############        | Elapsed Time: 0:01:03 ETA:   0:00:32\n",
      " 66% (207 of 313) |##############        | Elapsed Time: 0:01:03 ETA:   0:00:32\n",
      " 66% (208 of 313) |##############        | Elapsed Time: 0:01:03 ETA:   0:00:32\n",
      " 66% (209 of 313) |##############        | Elapsed Time: 0:01:04 ETA:   0:00:31\n",
      " 67% (210 of 313) |##############        | Elapsed Time: 0:01:04 ETA:   0:00:31\n",
      " 67% (211 of 313) |##############        | Elapsed Time: 0:01:04 ETA:   0:00:31\n",
      " 67% (212 of 313) |##############        | Elapsed Time: 0:01:05 ETA:   0:00:30\n",
      " 68% (213 of 313) |##############        | Elapsed Time: 0:01:05 ETA:   0:00:30\n",
      " 68% (214 of 313) |###############       | Elapsed Time: 0:01:05 ETA:   0:00:30\n",
      " 68% (215 of 313) |###############       | Elapsed Time: 0:01:05 ETA:   0:00:30\n",
      " 69% (216 of 313) |###############       | Elapsed Time: 0:01:06 ETA:   0:00:29\n",
      " 69% (217 of 313) |###############       | Elapsed Time: 0:01:06 ETA:   0:00:29\n",
      " 69% (218 of 313) |###############       | Elapsed Time: 0:01:06 ETA:   0:00:29\n",
      " 69% (219 of 313) |###############       | Elapsed Time: 0:01:07 ETA:   0:00:28\n",
      " 70% (220 of 313) |###############       | Elapsed Time: 0:01:07 ETA:   0:00:28\n",
      " 70% (221 of 313) |###############       | Elapsed Time: 0:01:07 ETA:   0:00:28\n",
      " 70% (222 of 313) |###############       | Elapsed Time: 0:01:08 ETA:   0:00:27\n",
      " 71% (223 of 313) |###############       | Elapsed Time: 0:01:08 ETA:   0:00:27\n",
      " 71% (224 of 313) |###############       | Elapsed Time: 0:01:08 ETA:   0:00:27\n",
      " 71% (225 of 313) |###############       | Elapsed Time: 0:01:08 ETA:   0:00:26\n",
      " 72% (226 of 313) |###############       | Elapsed Time: 0:01:09 ETA:   0:00:26\n",
      " 72% (227 of 313) |###############       | Elapsed Time: 0:01:09 ETA:   0:00:26\n",
      " 72% (228 of 313) |################      | Elapsed Time: 0:01:09 ETA:   0:00:26\n",
      " 73% (229 of 313) |################      | Elapsed Time: 0:01:10 ETA:   0:00:25\n",
      " 73% (230 of 313) |################      | Elapsed Time: 0:01:10 ETA:   0:00:25\n",
      " 73% (231 of 313) |################      | Elapsed Time: 0:01:10 ETA:   0:00:25\n",
      " 74% (232 of 313) |################      | Elapsed Time: 0:01:11 ETA:   0:00:24\n",
      " 74% (233 of 313) |################      | Elapsed Time: 0:01:11 ETA:   0:00:24\n",
      " 74% (234 of 313) |################      | Elapsed Time: 0:01:11 ETA:   0:00:24\n",
      " 75% (235 of 313) |################      | Elapsed Time: 0:01:11 ETA:   0:00:23\n",
      " 75% (236 of 313) |################      | Elapsed Time: 0:01:12 ETA:   0:00:23\n",
      " 75% (237 of 313) |################      | Elapsed Time: 0:01:12 ETA:   0:00:23\n",
      " 76% (238 of 313) |################      | Elapsed Time: 0:01:12 ETA:   0:00:22\n",
      " 76% (239 of 313) |################      | Elapsed Time: 0:01:12 ETA:   0:00:22\n",
      " 76% (240 of 313) |################      | Elapsed Time: 0:01:13 ETA:   0:00:22\n",
      " 76% (241 of 313) |################      | Elapsed Time: 0:01:13 ETA:   0:00:21\n",
      " 77% (242 of 313) |#################     | Elapsed Time: 0:01:13 ETA:   0:00:21\n",
      " 77% (243 of 313) |#################     | Elapsed Time: 0:01:14 ETA:   0:00:21\n",
      " 77% (244 of 313) |#################     | Elapsed Time: 0:01:14 ETA:   0:00:21\n",
      " 78% (245 of 313) |#################     | Elapsed Time: 0:01:14 ETA:   0:00:20\n",
      " 78% (246 of 313) |#################     | Elapsed Time: 0:01:14 ETA:   0:00:20\n",
      " 78% (247 of 313) |#################     | Elapsed Time: 0:01:15 ETA:   0:00:20\n",
      " 79% (248 of 313) |#################     | Elapsed Time: 0:01:15 ETA:   0:00:19\n",
      " 79% (249 of 313) |#################     | Elapsed Time: 0:01:15 ETA:   0:00:19\n",
      " 79% (250 of 313) |#################     | Elapsed Time: 0:01:15 ETA:   0:00:19\n",
      " 80% (251 of 313) |#################     | Elapsed Time: 0:01:16 ETA:   0:00:18\n",
      " 80% (252 of 313) |#################     | Elapsed Time: 0:01:16 ETA:   0:00:18\n",
      " 80% (253 of 313) |#################     | Elapsed Time: 0:01:16 ETA:   0:00:18\n",
      " 81% (254 of 313) |#################     | Elapsed Time: 0:01:17 ETA:   0:00:17\n",
      " 81% (255 of 313) |#################     | Elapsed Time: 0:01:17 ETA:   0:00:17\n",
      " 81% (256 of 313) |#################     | Elapsed Time: 0:01:17 ETA:   0:00:17\n",
      " 82% (257 of 313) |##################    | Elapsed Time: 0:01:18 ETA:   0:00:17\n",
      " 82% (258 of 313) |##################    | Elapsed Time: 0:01:18 ETA:   0:00:16\n",
      " 82% (259 of 313) |##################    | Elapsed Time: 0:01:18 ETA:   0:00:16\n",
      " 83% (260 of 313) |##################    | Elapsed Time: 0:01:19 ETA:   0:00:16\n",
      " 83% (261 of 313) |##################    | Elapsed Time: 0:01:19 ETA:   0:00:15\n",
      " 83% (262 of 313) |##################    | Elapsed Time: 0:01:19 ETA:   0:00:15\n",
      " 84% (263 of 313) |##################    | Elapsed Time: 0:01:20 ETA:   0:00:15\n",
      " 84% (264 of 313) |##################    | Elapsed Time: 0:01:20 ETA:   0:00:14\n",
      " 84% (265 of 313) |##################    | Elapsed Time: 0:01:20 ETA:   0:00:14\n",
      " 84% (266 of 313) |##################    | Elapsed Time: 0:01:20 ETA:   0:00:14\n",
      " 85% (267 of 313) |##################    | Elapsed Time: 0:01:21 ETA:   0:00:13\n",
      " 85% (268 of 313) |##################    | Elapsed Time: 0:01:21 ETA:   0:00:13\n",
      " 85% (269 of 313) |##################    | Elapsed Time: 0:01:21 ETA:   0:00:13\n",
      " 86% (270 of 313) |##################    | Elapsed Time: 0:01:21 ETA:   0:00:13\n",
      " 86% (271 of 313) |###################   | Elapsed Time: 0:01:22 ETA:   0:00:12\n",
      " 86% (272 of 313) |###################   | Elapsed Time: 0:01:22 ETA:   0:00:12\n",
      " 87% (273 of 313) |###################   | Elapsed Time: 0:01:22 ETA:   0:00:12\n",
      " 87% (274 of 313) |###################   | Elapsed Time: 0:01:23 ETA:   0:00:11\n",
      " 87% (275 of 313) |###################   | Elapsed Time: 0:01:23 ETA:   0:00:11\n",
      " 88% (276 of 313) |###################   | Elapsed Time: 0:01:23 ETA:   0:00:11\n",
      " 88% (277 of 313) |###################   | Elapsed Time: 0:01:23 ETA:   0:00:10\n",
      " 88% (278 of 313) |###################   | Elapsed Time: 0:01:24 ETA:   0:00:10\n",
      " 89% (279 of 313) |###################   | Elapsed Time: 0:01:24 ETA:   0:00:10\n",
      " 89% (280 of 313) |###################   | Elapsed Time: 0:01:24 ETA:   0:00:10\n",
      " 89% (281 of 313) |###################   | Elapsed Time: 0:01:25 ETA:   0:00:09\n",
      " 90% (282 of 313) |###################   | Elapsed Time: 0:01:25 ETA:   0:00:09\n",
      " 90% (283 of 313) |###################   | Elapsed Time: 0:01:25 ETA:   0:00:09\n",
      " 90% (284 of 313) |###################   | Elapsed Time: 0:01:26 ETA:   0:00:08\n",
      " 91% (285 of 313) |####################  | Elapsed Time: 0:01:26 ETA:   0:00:08\n",
      " 91% (286 of 313) |####################  | Elapsed Time: 0:01:26 ETA:   0:00:08\n",
      " 91% (287 of 313) |####################  | Elapsed Time: 0:01:27 ETA:   0:00:07\n",
      " 92% (288 of 313) |####################  | Elapsed Time: 0:01:27 ETA:   0:00:07\n",
      " 92% (289 of 313) |####################  | Elapsed Time: 0:01:27 ETA:   0:00:07\n",
      " 92% (290 of 313) |####################  | Elapsed Time: 0:01:28 ETA:   0:00:06\n",
      " 92% (291 of 313) |####################  | Elapsed Time: 0:01:28 ETA:   0:00:06\n",
      " 93% (292 of 313) |####################  | Elapsed Time: 0:01:28 ETA:   0:00:06\n",
      " 93% (293 of 313) |####################  | Elapsed Time: 0:01:28 ETA:   0:00:06\n",
      " 93% (294 of 313) |####################  | Elapsed Time: 0:01:29 ETA:   0:00:05\n",
      " 94% (295 of 313) |####################  | Elapsed Time: 0:01:29 ETA:   0:00:05\n",
      " 94% (296 of 313) |####################  | Elapsed Time: 0:01:29 ETA:   0:00:05\n",
      " 94% (297 of 313) |####################  | Elapsed Time: 0:01:30 ETA:   0:00:04\n",
      " 95% (298 of 313) |####################  | Elapsed Time: 0:01:30 ETA:   0:00:04\n",
      " 95% (299 of 313) |##################### | Elapsed Time: 0:01:30 ETA:   0:00:04\n",
      " 95% (300 of 313) |##################### | Elapsed Time: 0:01:31 ETA:   0:00:03\n",
      " 96% (301 of 313) |##################### | Elapsed Time: 0:01:31 ETA:   0:00:03\n",
      " 96% (302 of 313) |##################### | Elapsed Time: 0:01:31 ETA:   0:00:03\n",
      " 96% (303 of 313) |##################### | Elapsed Time: 0:01:31 ETA:   0:00:03\n",
      " 97% (304 of 313) |##################### | Elapsed Time: 0:01:32 ETA:   0:00:02\n",
      " 97% (305 of 313) |##################### | Elapsed Time: 0:01:32 ETA:   0:00:02\n",
      " 97% (306 of 313) |##################### | Elapsed Time: 0:01:32 ETA:   0:00:02\n",
      " 98% (307 of 313) |##################### | Elapsed Time: 0:01:32 ETA:   0:00:01\n",
      " 98% (308 of 313) |##################### | Elapsed Time: 0:01:33 ETA:   0:00:01\n",
      " 98% (309 of 313) |##################### | Elapsed Time: 0:01:33 ETA:   0:00:01\n",
      " 99% (310 of 313) |##################### | Elapsed Time: 0:01:33 ETA:   0:00:00\n",
      " 99% (311 of 313) |##################### | Elapsed Time: 0:01:34 ETA:   0:00:00\n",
      " 99% (312 of 313) |##################### | Elapsed Time: 0:01:34 ETA:   0:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:34 ETA:  00:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:34 Time:  0:01:34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:09:08,906 - Eval - INFO - Avg accuracy Top 1: 67.711661 Avg accuracy Top 5: 87.649760 on validation Dataset\n",
      "67.71166134185303\n"
     ]
    }
   ],
   "source": [
    "accuracy = ImageNetDataPipeline.evaluate(sim.model, use_cuda)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 1 Cross Layer Equalization\n",
    "\n",
    "The next cell performs cross-layer equalization on the model. As noted before, the function folds batch norms, applies cross-layer scaling, and then folds high biases.\n",
    "\n",
    "**Note:** Interestingly, CLE needs BN statistics for its procedure. If a BN folded model is provided, CLE will run the CLS (cross-layer scaling) optimization step but will skip the HBA (high-bias absorption) step. To avoid this, we simply load the original model again before running CLE.\n",
    "\n",
    "**Note:** CLE equalizes the model in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:09:09,152 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.3.module_add} \n",
      "2024-09-09 08:09:09,154 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.5.module_add_1} \n",
      "2024-09-09 08:09:09,158 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.6.module_add_2} \n",
      "2024-09-09 08:09:09,160 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.8.module_add_3} \n",
      "2024-09-09 08:09:09,161 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.9.module_add_4} \n",
      "2024-09-09 08:09:09,162 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.10.module_add_5} \n",
      "2024-09-09 08:09:09,163 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.12.module_add_6} \n",
      "2024-09-09 08:09:09,164 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.13.module_add_7} \n",
      "2024-09-09 08:09:09,165 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.15.module_add_8} \n",
      "2024-09-09 08:09:09,166 - ModelPreparer - INFO - Functional         : Adding new module for node: {features.16.module_add_9} \n",
      "2024-09-09 08:09:09,167 - ModelPreparer - INFO - Functional         : Adding new module for node: {module_adaptive_avg_pool2d} \n"
     ]
    }
   ],
   "source": [
    "model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "model = prepare_model(model)\n",
    "\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:09:13,506 - BatchNormFolding - INFO - 0 BatchNorms' weights got converted\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "\n",
    "equalize_model(model, input_shapes=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, we can determine the simulated quantized accuracy of the equalized model. We again create a simulation model like before and evaluate to determine simulated quantized accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:09:20,802 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-09-09 08:09:20,849 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-09-09 08:09:20,850 - Quant - INFO - Unsupported op type Mean\n",
      "2024-09-09 08:09:20,857 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "2024-09-09 08:09:22,086 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:09:33,505 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:09:33,508 - Eval - INFO - No value of iteration is provided, running evaluation on complete dataset.\n",
      "2024-09-09 08:09:33,509 - Eval - INFO - Evaluating nn.Module for 313 iterations with batch_size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 313) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "  0% (2 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:46\n",
      "  0% (3 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:01:14\n",
      "  1% (4 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:01:16\n",
      "  1% (5 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:14\n",
      "  1% (6 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:17\n",
      "  2% (7 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:01:18\n",
      "  2% (8 of 313) |                        | Elapsed Time: 0:00:02 ETA:   0:01:19\n",
      "  2% (9 of 313) |                        | Elapsed Time: 0:00:02 ETA:   0:01:19\n",
      "  3% (10 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:01:20\n",
      "  3% (11 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:01:20\n",
      "  3% (12 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:20\n",
      "  4% (13 of 313) |                       | Elapsed Time: 0:00:03 ETA:   0:01:20\n",
      "  4% (14 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:01:21\n",
      "  4% (15 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:20\n",
      "  5% (16 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:20\n",
      "  5% (17 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:20\n",
      "  5% (18 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:01:19\n",
      "  6% (19 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:19\n",
      "  6% (20 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:20\n",
      "  6% (21 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:19\n",
      "  7% (22 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:19\n",
      "  7% (23 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:19\n",
      "  7% (24 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:19\n",
      "  7% (25 of 313) |#                      | Elapsed Time: 0:00:06 ETA:   0:01:19\n",
      "  8% (26 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:19\n",
      "  8% (27 of 313) |#                      | Elapsed Time: 0:00:07 ETA:   0:01:19\n",
      "  8% (28 of 313) |##                     | Elapsed Time: 0:00:07 ETA:   0:01:18\n",
      "  9% (29 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:18\n",
      "  9% (30 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:19\n",
      "  9% (31 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:19\n",
      " 10% (32 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:19\n",
      " 10% (33 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:18\n",
      " 10% (34 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:18\n",
      " 11% (35 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:17\n",
      " 11% (36 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:17\n",
      " 11% (37 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:17\n",
      " 12% (38 of 313) |##                     | Elapsed Time: 0:00:10 ETA:   0:01:17\n",
      " 12% (39 of 313) |##                     | Elapsed Time: 0:00:11 ETA:   0:01:18\n",
      " 12% (40 of 313) |##                     | Elapsed Time: 0:00:11 ETA:   0:01:18\n",
      " 13% (41 of 313) |###                    | Elapsed Time: 0:00:11 ETA:   0:01:18\n",
      " 13% (42 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:17\n",
      " 13% (43 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:17\n",
      " 14% (44 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:17\n",
      " 14% (45 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:16\n",
      " 14% (46 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:16\n",
      " 15% (47 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:16\n",
      " 15% (48 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:15\n",
      " 15% (49 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:15\n",
      " 15% (50 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:14\n",
      " 16% (51 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:14\n",
      " 16% (52 of 313) |###                    | Elapsed Time: 0:00:14 ETA:   0:01:14\n",
      " 16% (53 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:13\n",
      " 17% (54 of 313) |###                    | Elapsed Time: 0:00:15 ETA:   0:01:13\n",
      " 17% (55 of 313) |####                   | Elapsed Time: 0:00:15 ETA:   0:01:13\n",
      " 17% (56 of 313) |####                   | Elapsed Time: 0:00:15 ETA:   0:01:13\n",
      " 18% (57 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:13\n",
      " 18% (58 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:12\n",
      " 18% (59 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:12\n",
      " 19% (60 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:12\n",
      " 19% (61 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:11\n",
      " 19% (62 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:11\n",
      " 20% (63 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:11\n",
      " 20% (64 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:10\n",
      " 20% (65 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:10\n",
      " 21% (66 of 313) |####                   | Elapsed Time: 0:00:18 ETA:   0:01:10\n",
      " 21% (67 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:10\n",
      " 21% (68 of 313) |####                   | Elapsed Time: 0:00:19 ETA:   0:01:09\n",
      " 22% (69 of 313) |#####                  | Elapsed Time: 0:00:19 ETA:   0:01:09\n",
      " 22% (70 of 313) |#####                  | Elapsed Time: 0:00:19 ETA:   0:01:09\n",
      " 22% (71 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:08\n",
      " 23% (72 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:08\n",
      " 23% (73 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:08\n",
      " 23% (74 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:07\n",
      " 23% (75 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:07\n",
      " 24% (76 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:07\n",
      " 24% (77 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:06\n",
      " 24% (78 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:06\n",
      " 25% (79 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:06\n",
      " 25% (80 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:05\n",
      " 25% (81 of 313) |#####                  | Elapsed Time: 0:00:22 ETA:   0:01:05\n",
      " 26% (82 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:05\n",
      " 26% (83 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:05\n",
      " 26% (84 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:05\n",
      " 27% (85 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:05\n",
      " 27% (86 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:04\n",
      " 27% (87 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:04\n",
      " 28% (88 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:01:04\n",
      " 28% (89 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:01:04\n",
      " 28% (90 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:01:04\n",
      " 29% (91 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:03\n",
      " 29% (92 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:03\n",
      " 29% (93 of 313) |######                 | Elapsed Time: 0:00:26 ETA:   0:01:02\n",
      " 30% (94 of 313) |######                 | Elapsed Time: 0:00:27 ETA:   0:01:03\n",
      " 30% (95 of 313) |######                 | Elapsed Time: 0:00:27 ETA:   0:01:02\n",
      " 30% (96 of 313) |#######                | Elapsed Time: 0:00:27 ETA:   0:01:02\n",
      " 30% (97 of 313) |#######                | Elapsed Time: 0:00:27 ETA:   0:01:02\n",
      " 31% (98 of 313) |#######                | Elapsed Time: 0:00:28 ETA:   0:01:01\n",
      " 31% (99 of 313) |#######                | Elapsed Time: 0:00:28 ETA:   0:01:01\n",
      " 31% (100 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:01:01\n",
      " 32% (101 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:01:01\n",
      " 32% (102 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:01:00\n",
      " 32% (103 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:01:00\n",
      " 33% (104 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:01:00\n",
      " 33% (105 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:59\n",
      " 33% (106 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:59\n",
      " 34% (107 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:59\n",
      " 34% (108 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:00:59\n",
      " 34% (109 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:00:58\n",
      " 35% (110 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:00:58\n",
      " 35% (111 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:00:58\n",
      " 35% (112 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:00:58\n",
      " 36% (113 of 313) |#######               | Elapsed Time: 0:00:32 ETA:   0:00:57\n",
      " 36% (114 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:57\n",
      " 36% (115 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:57\n",
      " 37% (116 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:56\n",
      " 37% (117 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:56\n",
      " 37% (118 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:00:56\n",
      " 38% (119 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:00:56\n",
      " 38% (120 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:56\n",
      " 38% (121 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:56\n",
      " 38% (122 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:55\n",
      " 39% (123 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:55\n",
      " 39% (124 of 313) |########              | Elapsed Time: 0:00:36 ETA:   0:00:55\n",
      " 39% (125 of 313) |########              | Elapsed Time: 0:00:36 ETA:   0:00:54\n",
      " 40% (126 of 313) |########              | Elapsed Time: 0:00:36 ETA:   0:00:54\n",
      " 40% (127 of 313) |########              | Elapsed Time: 0:00:37 ETA:   0:00:54\n",
      " 40% (128 of 313) |########              | Elapsed Time: 0:00:37 ETA:   0:00:53\n",
      " 41% (129 of 313) |#########             | Elapsed Time: 0:00:37 ETA:   0:00:53\n",
      " 41% (130 of 313) |#########             | Elapsed Time: 0:00:37 ETA:   0:00:53\n",
      " 41% (131 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:53\n",
      " 42% (132 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:52\n",
      " 42% (133 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:52\n",
      " 42% (134 of 313) |#########             | Elapsed Time: 0:00:39 ETA:   0:00:52\n",
      " 43% (135 of 313) |#########             | Elapsed Time: 0:00:39 ETA:   0:00:52\n",
      " 43% (136 of 313) |#########             | Elapsed Time: 0:00:39 ETA:   0:00:51\n",
      " 43% (137 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:51\n",
      " 44% (138 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:51\n",
      " 44% (139 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:50\n",
      " 44% (140 of 313) |#########             | Elapsed Time: 0:00:40 ETA:   0:00:50\n",
      " 45% (141 of 313) |#########             | Elapsed Time: 0:00:41 ETA:   0:00:50\n",
      " 45% (142 of 313) |#########             | Elapsed Time: 0:00:41 ETA:   0:00:49\n",
      " 45% (143 of 313) |##########            | Elapsed Time: 0:00:41 ETA:   0:00:49\n",
      " 46% (144 of 313) |##########            | Elapsed Time: 0:00:41 ETA:   0:00:49\n",
      " 46% (145 of 313) |##########            | Elapsed Time: 0:00:42 ETA:   0:00:48\n",
      " 46% (146 of 313) |##########            | Elapsed Time: 0:00:42 ETA:   0:00:48\n",
      " 46% (147 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:48\n",
      " 47% (148 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:48\n",
      " 47% (149 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:48\n",
      " 47% (150 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:47\n",
      " 48% (151 of 313) |##########            | Elapsed Time: 0:00:44 ETA:   0:00:47\n",
      " 48% (152 of 313) |##########            | Elapsed Time: 0:00:44 ETA:   0:00:47\n",
      " 48% (153 of 313) |##########            | Elapsed Time: 0:00:44 ETA:   0:00:46\n",
      " 49% (154 of 313) |##########            | Elapsed Time: 0:00:44 ETA:   0:00:46\n",
      " 49% (155 of 313) |##########            | Elapsed Time: 0:00:45 ETA:   0:00:46\n",
      " 49% (156 of 313) |##########            | Elapsed Time: 0:00:45 ETA:   0:00:45\n",
      " 50% (157 of 313) |###########           | Elapsed Time: 0:00:45 ETA:   0:00:45\n",
      " 50% (158 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:45\n",
      " 50% (159 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:44\n",
      " 51% (160 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:44\n",
      " 51% (161 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:44\n",
      " 51% (162 of 313) |###########           | Elapsed Time: 0:00:47 ETA:   0:00:44\n",
      " 52% (163 of 313) |###########           | Elapsed Time: 0:00:47 ETA:   0:00:43\n",
      " 52% (164 of 313) |###########           | Elapsed Time: 0:00:47 ETA:   0:00:43\n",
      " 52% (165 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:43\n",
      " 53% (166 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:42\n",
      " 53% (167 of 313) |###########           | Elapsed Time: 0:00:48 ETA:   0:00:42\n",
      " 53% (168 of 313) |###########           | Elapsed Time: 0:00:49 ETA:   0:00:42\n",
      " 53% (169 of 313) |###########           | Elapsed Time: 0:00:49 ETA:   0:00:41\n",
      " 54% (170 of 313) |###########           | Elapsed Time: 0:00:49 ETA:   0:00:41\n",
      " 54% (171 of 313) |############          | Elapsed Time: 0:00:49 ETA:   0:00:41\n",
      " 54% (172 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:41\n",
      " 55% (173 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:40\n",
      " 55% (174 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:40\n",
      " 55% (175 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:40\n",
      " 56% (176 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:40\n",
      " 56% (177 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:39\n",
      " 56% (178 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:39\n",
      " 57% (179 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:39\n",
      " 57% (180 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:38\n",
      " 57% (181 of 313) |############          | Elapsed Time: 0:00:52 ETA:   0:00:38\n",
      " 58% (182 of 313) |############          | Elapsed Time: 0:00:53 ETA:   0:00:38\n",
      " 58% (183 of 313) |############          | Elapsed Time: 0:00:53 ETA:   0:00:37\n",
      " 58% (184 of 313) |############          | Elapsed Time: 0:00:53 ETA:   0:00:37\n",
      " 59% (185 of 313) |#############         | Elapsed Time: 0:00:54 ETA:   0:00:37\n",
      " 59% (186 of 313) |#############         | Elapsed Time: 0:00:54 ETA:   0:00:37\n",
      " 59% (187 of 313) |#############         | Elapsed Time: 0:00:54 ETA:   0:00:36\n",
      " 60% (188 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:36\n",
      " 60% (189 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:36\n",
      " 60% (190 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:35\n",
      " 61% (191 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:35\n",
      " 61% (192 of 313) |#############         | Elapsed Time: 0:00:56 ETA:   0:00:35\n",
      " 61% (193 of 313) |#############         | Elapsed Time: 0:00:56 ETA:   0:00:35\n",
      " 61% (194 of 313) |#############         | Elapsed Time: 0:00:56 ETA:   0:00:34\n",
      " 62% (195 of 313) |#############         | Elapsed Time: 0:00:56 ETA:   0:00:34\n",
      " 62% (196 of 313) |#############         | Elapsed Time: 0:00:57 ETA:   0:00:34\n",
      " 62% (197 of 313) |#############         | Elapsed Time: 0:00:57 ETA:   0:00:33\n",
      " 63% (198 of 313) |#############         | Elapsed Time: 0:00:57 ETA:   0:00:33\n",
      " 63% (199 of 313) |#############         | Elapsed Time: 0:00:58 ETA:   0:00:33\n",
      " 63% (200 of 313) |##############        | Elapsed Time: 0:00:58 ETA:   0:00:32\n",
      " 64% (201 of 313) |##############        | Elapsed Time: 0:00:58 ETA:   0:00:32\n",
      " 64% (202 of 313) |##############        | Elapsed Time: 0:00:58 ETA:   0:00:32\n",
      " 64% (203 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:32\n",
      " 65% (204 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:31\n",
      " 65% (205 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:31\n",
      " 65% (206 of 313) |##############        | Elapsed Time: 0:01:00 ETA:   0:00:31\n",
      " 66% (207 of 313) |##############        | Elapsed Time: 0:01:00 ETA:   0:00:30\n",
      " 66% (208 of 313) |##############        | Elapsed Time: 0:01:00 ETA:   0:00:30\n",
      " 66% (209 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:30\n",
      " 67% (210 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:30\n",
      " 67% (211 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:29\n",
      " 67% (212 of 313) |##############        | Elapsed Time: 0:01:01 ETA:   0:00:29\n",
      " 68% (213 of 313) |##############        | Elapsed Time: 0:01:02 ETA:   0:00:29\n",
      " 68% (214 of 313) |###############       | Elapsed Time: 0:01:02 ETA:   0:00:28\n",
      " 68% (215 of 313) |###############       | Elapsed Time: 0:01:02 ETA:   0:00:28\n",
      " 69% (216 of 313) |###############       | Elapsed Time: 0:01:03 ETA:   0:00:28\n",
      " 69% (217 of 313) |###############       | Elapsed Time: 0:01:03 ETA:   0:00:28\n",
      " 69% (218 of 313) |###############       | Elapsed Time: 0:01:03 ETA:   0:00:27\n",
      " 69% (219 of 313) |###############       | Elapsed Time: 0:01:04 ETA:   0:00:27\n",
      " 70% (220 of 313) |###############       | Elapsed Time: 0:01:04 ETA:   0:00:27\n",
      " 70% (221 of 313) |###############       | Elapsed Time: 0:01:04 ETA:   0:00:26\n",
      " 70% (222 of 313) |###############       | Elapsed Time: 0:01:05 ETA:   0:00:26\n",
      " 71% (223 of 313) |###############       | Elapsed Time: 0:01:05 ETA:   0:00:26\n",
      " 71% (224 of 313) |###############       | Elapsed Time: 0:01:05 ETA:   0:00:26\n",
      " 71% (225 of 313) |###############       | Elapsed Time: 0:01:05 ETA:   0:00:25\n",
      " 72% (226 of 313) |###############       | Elapsed Time: 0:01:06 ETA:   0:00:25\n",
      " 72% (227 of 313) |###############       | Elapsed Time: 0:01:06 ETA:   0:00:25\n",
      " 72% (228 of 313) |################      | Elapsed Time: 0:01:06 ETA:   0:00:24\n",
      " 73% (229 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:24\n",
      " 73% (230 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:24\n",
      " 73% (231 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:23\n",
      " 74% (232 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:23\n",
      " 74% (233 of 313) |################      | Elapsed Time: 0:01:08 ETA:   0:00:23\n",
      " 74% (234 of 313) |################      | Elapsed Time: 0:01:08 ETA:   0:00:23\n",
      " 75% (235 of 313) |################      | Elapsed Time: 0:01:08 ETA:   0:00:22\n",
      " 75% (236 of 313) |################      | Elapsed Time: 0:01:08 ETA:   0:00:22\n",
      " 75% (237 of 313) |################      | Elapsed Time: 0:01:09 ETA:   0:00:22\n",
      " 76% (238 of 313) |################      | Elapsed Time: 0:01:09 ETA:   0:00:21\n",
      " 76% (239 of 313) |################      | Elapsed Time: 0:01:09 ETA:   0:00:21\n",
      " 76% (240 of 313) |################      | Elapsed Time: 0:01:09 ETA:   0:00:21\n",
      " 76% (241 of 313) |################      | Elapsed Time: 0:01:10 ETA:   0:00:20\n",
      " 77% (242 of 313) |#################     | Elapsed Time: 0:01:10 ETA:   0:00:20\n",
      " 77% (243 of 313) |#################     | Elapsed Time: 0:01:10 ETA:   0:00:20\n",
      " 77% (244 of 313) |#################     | Elapsed Time: 0:01:11 ETA:   0:00:20\n",
      " 78% (245 of 313) |#################     | Elapsed Time: 0:01:11 ETA:   0:00:19\n",
      " 78% (246 of 313) |#################     | Elapsed Time: 0:01:11 ETA:   0:00:19\n",
      " 78% (247 of 313) |#################     | Elapsed Time: 0:01:12 ETA:   0:00:19\n",
      " 79% (248 of 313) |#################     | Elapsed Time: 0:01:12 ETA:   0:00:18\n",
      " 79% (249 of 313) |#################     | Elapsed Time: 0:01:12 ETA:   0:00:18\n",
      " 79% (250 of 313) |#################     | Elapsed Time: 0:01:12 ETA:   0:00:18\n",
      " 80% (251 of 313) |#################     | Elapsed Time: 0:01:13 ETA:   0:00:18\n",
      " 80% (252 of 313) |#################     | Elapsed Time: 0:01:13 ETA:   0:00:17\n",
      " 80% (253 of 313) |#################     | Elapsed Time: 0:01:13 ETA:   0:00:17\n",
      " 81% (254 of 313) |#################     | Elapsed Time: 0:01:13 ETA:   0:00:17\n",
      " 81% (255 of 313) |#################     | Elapsed Time: 0:01:14 ETA:   0:00:16\n",
      " 81% (256 of 313) |#################     | Elapsed Time: 0:01:14 ETA:   0:00:16\n",
      " 82% (257 of 313) |##################    | Elapsed Time: 0:01:14 ETA:   0:00:16\n",
      " 82% (258 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:16\n",
      " 82% (259 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:15\n",
      " 83% (260 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:15\n",
      " 83% (261 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:15\n",
      " 83% (262 of 313) |##################    | Elapsed Time: 0:01:16 ETA:   0:00:14\n",
      " 84% (263 of 313) |##################    | Elapsed Time: 0:01:16 ETA:   0:00:14\n",
      " 84% (264 of 313) |##################    | Elapsed Time: 0:01:16 ETA:   0:00:14\n",
      " 84% (265 of 313) |##################    | Elapsed Time: 0:01:17 ETA:   0:00:13\n",
      " 84% (266 of 313) |##################    | Elapsed Time: 0:01:17 ETA:   0:00:13\n",
      " 85% (267 of 313) |##################    | Elapsed Time: 0:01:17 ETA:   0:00:13\n",
      " 85% (268 of 313) |##################    | Elapsed Time: 0:01:17 ETA:   0:00:13\n",
      " 85% (269 of 313) |##################    | Elapsed Time: 0:01:18 ETA:   0:00:12\n",
      " 86% (270 of 313) |##################    | Elapsed Time: 0:01:18 ETA:   0:00:12\n",
      " 86% (271 of 313) |###################   | Elapsed Time: 0:01:18 ETA:   0:00:12\n",
      " 86% (272 of 313) |###################   | Elapsed Time: 0:01:18 ETA:   0:00:11\n",
      " 87% (273 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:11\n",
      " 87% (274 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:11\n",
      " 87% (275 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:11\n",
      " 88% (276 of 313) |###################   | Elapsed Time: 0:01:20 ETA:   0:00:10\n",
      " 88% (277 of 313) |###################   | Elapsed Time: 0:01:20 ETA:   0:00:10\n",
      " 88% (278 of 313) |###################   | Elapsed Time: 0:01:20 ETA:   0:00:10\n",
      " 89% (279 of 313) |###################   | Elapsed Time: 0:01:20 ETA:   0:00:09\n",
      " 89% (280 of 313) |###################   | Elapsed Time: 0:01:21 ETA:   0:00:09\n",
      " 89% (281 of 313) |###################   | Elapsed Time: 0:01:21 ETA:   0:00:09\n",
      " 90% (282 of 313) |###################   | Elapsed Time: 0:01:21 ETA:   0:00:09\n",
      " 90% (283 of 313) |###################   | Elapsed Time: 0:01:22 ETA:   0:00:08\n",
      " 90% (284 of 313) |###################   | Elapsed Time: 0:01:22 ETA:   0:00:08\n",
      " 91% (285 of 313) |####################  | Elapsed Time: 0:01:22 ETA:   0:00:08\n",
      " 91% (286 of 313) |####################  | Elapsed Time: 0:01:22 ETA:   0:00:07\n",
      " 91% (287 of 313) |####################  | Elapsed Time: 0:01:23 ETA:   0:00:07\n",
      " 92% (288 of 313) |####################  | Elapsed Time: 0:01:23 ETA:   0:00:07\n",
      " 92% (289 of 313) |####################  | Elapsed Time: 0:01:23 ETA:   0:00:06\n",
      " 92% (290 of 313) |####################  | Elapsed Time: 0:01:24 ETA:   0:00:06\n",
      " 92% (291 of 313) |####################  | Elapsed Time: 0:01:24 ETA:   0:00:06\n",
      " 93% (292 of 313) |####################  | Elapsed Time: 0:01:24 ETA:   0:00:06\n",
      " 93% (293 of 313) |####################  | Elapsed Time: 0:01:24 ETA:   0:00:05\n",
      " 93% (294 of 313) |####################  | Elapsed Time: 0:01:25 ETA:   0:00:05\n",
      " 94% (295 of 313) |####################  | Elapsed Time: 0:01:25 ETA:   0:00:05\n",
      " 94% (296 of 313) |####################  | Elapsed Time: 0:01:25 ETA:   0:00:04\n",
      " 94% (297 of 313) |####################  | Elapsed Time: 0:01:26 ETA:   0:00:04\n",
      " 95% (298 of 313) |####################  | Elapsed Time: 0:01:26 ETA:   0:00:04\n",
      " 95% (299 of 313) |##################### | Elapsed Time: 0:01:26 ETA:   0:00:04\n",
      " 95% (300 of 313) |##################### | Elapsed Time: 0:01:27 ETA:   0:00:03\n",
      " 96% (301 of 313) |##################### | Elapsed Time: 0:01:27 ETA:   0:00:03\n",
      " 96% (302 of 313) |##################### | Elapsed Time: 0:01:27 ETA:   0:00:03\n",
      " 96% (303 of 313) |##################### | Elapsed Time: 0:01:28 ETA:   0:00:02\n",
      " 97% (304 of 313) |##################### | Elapsed Time: 0:01:28 ETA:   0:00:02\n",
      " 97% (305 of 313) |##################### | Elapsed Time: 0:01:28 ETA:   0:00:02\n",
      " 97% (306 of 313) |##################### | Elapsed Time: 0:01:28 ETA:   0:00:02\n",
      " 98% (307 of 313) |##################### | Elapsed Time: 0:01:29 ETA:   0:00:01\n",
      " 98% (308 of 313) |##################### | Elapsed Time: 0:01:29 ETA:   0:00:01\n",
      " 98% (309 of 313) |##################### | Elapsed Time: 0:01:29 ETA:   0:00:01\n",
      " 99% (310 of 313) |##################### | Elapsed Time: 0:01:29 ETA:   0:00:00\n",
      " 99% (311 of 313) |##################### | Elapsed Time: 0:01:30 ETA:   0:00:00\n",
      " 99% (312 of 313) |##################### | Elapsed Time: 0:01:30 ETA:   0:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:30 ETA:  00:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:30 Time:  0:01:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:11:05,031 - Eval - INFO - Avg accuracy Top 1: 70.207668 Avg accuracy Top 5: 89.656550 on validation Dataset\n",
      "70.2076677316294\n"
     ]
    }
   ],
   "source": [
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8)\n",
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)\n",
    "\n",
    "accuracy = ImageNetDataPipeline.evaluate(sim.model, use_cuda)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 2 Bias Correction\n",
    "\n",
    "This section shows how we can apply AIMET Bias Correction on top of the already equalized model from the previous step. Bias correction under the hood uses a reference FP32 model and a QuantizationSimModel to perform its procedure. More details are explained in the AIMET User Guide documentation.\n",
    "\n",
    "For the correct_bias API, we pass the following parameters\n",
    "\n",
    "- **num_quant_samples**: Number of samples used for computing encodings. We are setting this to a low number here to speed up execution. A typical number would be 500-1000.\n",
    "- **num_bias_correct_samples**: Number of samples used for bias correction. We are setting this to a low number here to speed up execution. A typical number would be 1000-2000.\n",
    "- **data_loader**: BC uses unlabeled data samples from this data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:11:06,551 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:11:10,917 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-09-09 08:11:10,960 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-09-09 08:11:10,962 - Quant - INFO - Unsupported op type Mean\n",
      "2024-09-09 08:11:10,966 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "2024-09-09 08:11:12,289 - Quant - INFO - Correcting layer features.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:13,449 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:13,452 - Quant - INFO - Correcting layer features.1.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:14,646 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:14,650 - Quant - INFO - Correcting layer features.1.conv.1 using Empirical Bias Correction\n",
      "2024-09-09 08:11:15,889 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:15,893 - Quant - INFO - Correcting layer features.2.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:18,192 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:18,204 - Quant - INFO - Correcting layer features.2.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:19,400 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:19,405 - Quant - INFO - Correcting layer features.2.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:20,581 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:20,585 - Quant - INFO - Correcting layer features.3.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:21,755 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:21,758 - Quant - INFO - Correcting layer features.3.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:23,049 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:23,057 - Quant - INFO - Correcting layer features.3.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:24,255 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:24,259 - Quant - INFO - Correcting layer features.4.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:25,464 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:25,468 - Quant - INFO - Correcting layer features.4.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:26,598 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:26,602 - Quant - INFO - Correcting layer features.4.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:27,759 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:27,763 - Quant - INFO - Correcting layer features.5.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:28,899 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:28,904 - Quant - INFO - Correcting layer features.5.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:30,176 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:30,184 - Quant - INFO - Correcting layer features.5.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:31,455 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:31,460 - Quant - INFO - Correcting layer features.6.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:32,598 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:32,603 - Quant - INFO - Correcting layer features.6.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:33,762 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:33,767 - Quant - INFO - Correcting layer features.6.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:34,910 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:34,914 - Quant - INFO - Correcting layer features.7.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:36,094 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:36,103 - Quant - INFO - Correcting layer features.7.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:37,384 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:37,389 - Quant - INFO - Correcting layer features.7.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:38,566 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:38,570 - Quant - INFO - Correcting layer features.8.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:39,708 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:39,712 - Quant - INFO - Correcting layer features.8.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:40,867 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:40,870 - Quant - INFO - Correcting layer features.8.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:42,018 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:42,022 - Quant - INFO - Correcting layer features.9.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:43,186 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:43,190 - Quant - INFO - Correcting layer features.9.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:44,439 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:44,446 - Quant - INFO - Correcting layer features.9.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:45,631 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:45,636 - Quant - INFO - Correcting layer features.10.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:46,829 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:46,832 - Quant - INFO - Correcting layer features.10.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:47,987 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:47,991 - Quant - INFO - Correcting layer features.10.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:49,197 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:49,201 - Quant - INFO - Correcting layer features.11.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:50,385 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:50,389 - Quant - INFO - Correcting layer features.11.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:51,699 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:51,703 - Quant - INFO - Correcting layer features.11.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:52,856 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:52,859 - Quant - INFO - Correcting layer features.12.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:54,014 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:54,018 - Quant - INFO - Correcting layer features.12.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:55,171 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:55,176 - Quant - INFO - Correcting layer features.12.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:56,328 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:56,331 - Quant - INFO - Correcting layer features.13.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:57,494 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:57,498 - Quant - INFO - Correcting layer features.13.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:11:58,773 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:58,777 - Quant - INFO - Correcting layer features.13.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:11:59,930 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:11:59,933 - Quant - INFO - Correcting layer features.14.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:01,129 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:01,136 - Quant - INFO - Correcting layer features.14.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:02,338 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:02,342 - Quant - INFO - Correcting layer features.14.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:12:03,462 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:03,466 - Quant - INFO - Correcting layer features.15.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:04,683 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:04,688 - Quant - INFO - Correcting layer features.15.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:05,942 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:05,946 - Quant - INFO - Correcting layer features.15.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:12:07,136 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:07,142 - Quant - INFO - Correcting layer features.16.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:08,296 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:08,300 - Quant - INFO - Correcting layer features.16.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:09,425 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:09,429 - Quant - INFO - Correcting layer features.16.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:12:10,596 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:10,600 - Quant - INFO - Correcting layer features.17.conv.0.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:11,794 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:11,798 - Quant - INFO - Correcting layer features.17.conv.1.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:12,972 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:12,976 - Quant - INFO - Correcting layer features.17.conv.2 using Empirical Bias Correction\n",
      "2024-09-09 08:12:14,153 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:14,156 - Quant - INFO - Correcting layer features.18.0 using Empirical Bias Correction\n",
      "2024-09-09 08:12:15,332 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:15,337 - Quant - INFO - Correcting layer classifier.1 using Empirical Bias Correction\n",
      "2024-09-09 08:12:16,409 - Quant - INFO - Corrected bias for the layer\n",
      "2024-09-09 08:12:16,420 - Quant - INFO - Completed bias correction\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.quantsim import QuantParams\n",
    "from aimet_torch.bias_correction import correct_bias\n",
    "\n",
    "data_loader = ImageNetDataPipeline.get_val_dataloader()\n",
    "\n",
    "bc_params = QuantParams(weight_bw=8, act_bw=8, round_mode=\"nearest\",\n",
    "                        quant_scheme=QuantScheme.post_training_tf_enhanced)\n",
    "\n",
    "correct_bias(model, bc_params, num_quant_samples=16,\n",
    "             data_loader=data_loader, num_bias_correct_samples=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, we can determine the simulated quantized accuracy of the bias-corrected model. We again create a simulation model like before and evaluate to determine simulated quantized accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:12:18,447 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-09-09 08:12:18,500 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-09-09 08:12:18,501 - Quant - INFO - Unsupported op type Mean\n",
      "2024-09-09 08:12:18,509 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "2024-09-09 08:12:20,191 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:12:41,083 - Dataloader - INFO - Dataset consists of 10000 images in 1000 classes\n",
      "2024-09-09 08:12:41,086 - Eval - INFO - No value of iteration is provided, running evaluation on complete dataset.\n",
      "2024-09-09 08:12:41,086 - Eval - INFO - Evaluating nn.Module for 313 iterations with batch_size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (0 of 313) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "  0% (2 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:37\n",
      "  0% (3 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:50\n",
      "  1% (4 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:54\n",
      "  1% (5 of 313) |                        | Elapsed Time: 0:00:00 ETA:   0:00:57\n",
      "  1% (6 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:00:58\n",
      "  2% (7 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:00:57\n",
      "  2% (8 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:00:58\n",
      "  2% (9 of 313) |                        | Elapsed Time: 0:00:01 ETA:   0:00:56\n",
      "  3% (10 of 313) |                       | Elapsed Time: 0:00:01 ETA:   0:00:57\n",
      "  3% (11 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:00:56\n",
      "  3% (12 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:00:57\n",
      "  4% (13 of 313) |                       | Elapsed Time: 0:00:02 ETA:   0:00:57\n",
      "  4% (14 of 313) |#                      | Elapsed Time: 0:00:02 ETA:   0:00:57\n",
      "  4% (15 of 313) |#                      | Elapsed Time: 0:00:02 ETA:   0:00:57\n",
      "  5% (16 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:00:56\n",
      "  5% (17 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:00:56\n",
      "  5% (18 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:00:57\n",
      "  6% (19 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:00:56\n",
      "  6% (20 of 313) |#                      | Elapsed Time: 0:00:03 ETA:   0:00:56\n",
      "  6% (21 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:00:56\n",
      "  7% (22 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:00:57\n",
      "  7% (23 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:00:58\n",
      "  7% (24 of 313) |#                      | Elapsed Time: 0:00:04 ETA:   0:00:58\n",
      "  7% (25 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:00:59\n",
      "  8% (26 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:00\n",
      "  8% (27 of 313) |#                      | Elapsed Time: 0:00:05 ETA:   0:01:00\n",
      "  8% (28 of 313) |##                     | Elapsed Time: 0:00:06 ETA:   0:01:01\n",
      "  9% (29 of 313) |##                     | Elapsed Time: 0:00:06 ETA:   0:01:02\n",
      "  9% (30 of 313) |##                     | Elapsed Time: 0:00:06 ETA:   0:01:02\n",
      "  9% (31 of 313) |##                     | Elapsed Time: 0:00:06 ETA:   0:01:02\n",
      " 10% (32 of 313) |##                     | Elapsed Time: 0:00:07 ETA:   0:01:03\n",
      " 10% (33 of 313) |##                     | Elapsed Time: 0:00:07 ETA:   0:01:03\n",
      " 10% (34 of 313) |##                     | Elapsed Time: 0:00:07 ETA:   0:01:04\n",
      " 11% (35 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:04\n",
      " 11% (36 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:05\n",
      " 11% (37 of 313) |##                     | Elapsed Time: 0:00:08 ETA:   0:01:05\n",
      " 12% (38 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:05\n",
      " 12% (39 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:07\n",
      " 12% (40 of 313) |##                     | Elapsed Time: 0:00:09 ETA:   0:01:07\n",
      " 13% (41 of 313) |###                    | Elapsed Time: 0:00:10 ETA:   0:01:07\n",
      " 13% (42 of 313) |###                    | Elapsed Time: 0:00:10 ETA:   0:01:07\n",
      " 13% (43 of 313) |###                    | Elapsed Time: 0:00:10 ETA:   0:01:07\n",
      " 14% (44 of 313) |###                    | Elapsed Time: 0:00:11 ETA:   0:01:07\n",
      " 14% (45 of 313) |###                    | Elapsed Time: 0:00:11 ETA:   0:01:07\n",
      " 14% (46 of 313) |###                    | Elapsed Time: 0:00:11 ETA:   0:01:07\n",
      " 15% (47 of 313) |###                    | Elapsed Time: 0:00:11 ETA:   0:01:07\n",
      " 15% (48 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:07\n",
      " 15% (49 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:06\n",
      " 15% (50 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:06\n",
      " 16% (51 of 313) |###                    | Elapsed Time: 0:00:12 ETA:   0:01:06\n",
      " 16% (52 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:06\n",
      " 16% (53 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:06\n",
      " 17% (54 of 313) |###                    | Elapsed Time: 0:00:13 ETA:   0:01:06\n",
      " 17% (55 of 313) |####                   | Elapsed Time: 0:00:14 ETA:   0:01:06\n",
      " 17% (56 of 313) |####                   | Elapsed Time: 0:00:14 ETA:   0:01:05\n",
      " 18% (57 of 313) |####                   | Elapsed Time: 0:00:14 ETA:   0:01:05\n",
      " 18% (58 of 313) |####                   | Elapsed Time: 0:00:14 ETA:   0:01:05\n",
      " 18% (59 of 313) |####                   | Elapsed Time: 0:00:15 ETA:   0:01:05\n",
      " 19% (60 of 313) |####                   | Elapsed Time: 0:00:15 ETA:   0:01:05\n",
      " 19% (61 of 313) |####                   | Elapsed Time: 0:00:15 ETA:   0:01:05\n",
      " 19% (62 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:05\n",
      " 20% (63 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:05\n",
      " 20% (64 of 313) |####                   | Elapsed Time: 0:00:16 ETA:   0:01:05\n",
      " 20% (65 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:04\n",
      " 21% (66 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:04\n",
      " 21% (67 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:04\n",
      " 21% (68 of 313) |####                   | Elapsed Time: 0:00:17 ETA:   0:01:04\n",
      " 22% (69 of 313) |#####                  | Elapsed Time: 0:00:18 ETA:   0:01:04\n",
      " 22% (70 of 313) |#####                  | Elapsed Time: 0:00:18 ETA:   0:01:03\n",
      " 22% (71 of 313) |#####                  | Elapsed Time: 0:00:18 ETA:   0:01:03\n",
      " 23% (72 of 313) |#####                  | Elapsed Time: 0:00:18 ETA:   0:01:03\n",
      " 23% (73 of 313) |#####                  | Elapsed Time: 0:00:19 ETA:   0:01:03\n",
      " 23% (74 of 313) |#####                  | Elapsed Time: 0:00:19 ETA:   0:01:03\n",
      " 23% (75 of 313) |#####                  | Elapsed Time: 0:00:19 ETA:   0:01:02\n",
      " 24% (76 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:02\n",
      " 24% (77 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:02\n",
      " 24% (78 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:01\n",
      " 25% (79 of 313) |#####                  | Elapsed Time: 0:00:20 ETA:   0:01:01\n",
      " 25% (80 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:01\n",
      " 25% (81 of 313) |#####                  | Elapsed Time: 0:00:21 ETA:   0:01:01\n",
      " 26% (82 of 313) |######                 | Elapsed Time: 0:00:21 ETA:   0:01:01\n",
      " 26% (83 of 313) |######                 | Elapsed Time: 0:00:21 ETA:   0:01:00\n",
      " 26% (84 of 313) |######                 | Elapsed Time: 0:00:22 ETA:   0:01:01\n",
      " 27% (85 of 313) |######                 | Elapsed Time: 0:00:22 ETA:   0:01:00\n",
      " 27% (86 of 313) |######                 | Elapsed Time: 0:00:22 ETA:   0:01:00\n",
      " 27% (87 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:00\n",
      " 28% (88 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:00\n",
      " 28% (89 of 313) |######                 | Elapsed Time: 0:00:23 ETA:   0:01:00\n",
      " 28% (90 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:01:00\n",
      " 29% (91 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:00:59\n",
      " 29% (92 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:00:59\n",
      " 29% (93 of 313) |######                 | Elapsed Time: 0:00:24 ETA:   0:00:58\n",
      " 30% (94 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:00:59\n",
      " 30% (95 of 313) |######                 | Elapsed Time: 0:00:25 ETA:   0:00:59\n",
      " 30% (96 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:58\n",
      " 30% (97 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:58\n",
      " 31% (98 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:58\n",
      " 31% (99 of 313) |#######                | Elapsed Time: 0:00:26 ETA:   0:00:57\n",
      " 31% (100 of 313) |#######               | Elapsed Time: 0:00:27 ETA:   0:00:57\n",
      " 32% (101 of 313) |#######               | Elapsed Time: 0:00:27 ETA:   0:00:57\n",
      " 32% (102 of 313) |#######               | Elapsed Time: 0:00:27 ETA:   0:00:57\n",
      " 32% (103 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:57\n",
      " 33% (104 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:56\n",
      " 33% (105 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:56\n",
      " 33% (106 of 313) |#######               | Elapsed Time: 0:00:28 ETA:   0:00:56\n",
      " 34% (107 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:56\n",
      " 34% (108 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:55\n",
      " 34% (109 of 313) |#######               | Elapsed Time: 0:00:29 ETA:   0:00:55\n",
      " 35% (110 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:55\n",
      " 35% (111 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:55\n",
      " 35% (112 of 313) |#######               | Elapsed Time: 0:00:30 ETA:   0:00:54\n",
      " 36% (113 of 313) |#######               | Elapsed Time: 0:00:31 ETA:   0:00:54\n",
      " 36% (114 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:54\n",
      " 36% (115 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:54\n",
      " 37% (116 of 313) |########              | Elapsed Time: 0:00:31 ETA:   0:00:54\n",
      " 37% (117 of 313) |########              | Elapsed Time: 0:00:32 ETA:   0:00:54\n",
      " 37% (118 of 313) |########              | Elapsed Time: 0:00:32 ETA:   0:00:53\n",
      " 38% (119 of 313) |########              | Elapsed Time: 0:00:32 ETA:   0:00:53\n",
      " 38% (120 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:53\n",
      " 38% (121 of 313) |########              | Elapsed Time: 0:00:33 ETA:   0:00:53\n",
      " 38% (122 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:00:53\n",
      " 39% (123 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:00:53\n",
      " 39% (124 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:00:52\n",
      " 39% (125 of 313) |########              | Elapsed Time: 0:00:34 ETA:   0:00:52\n",
      " 40% (126 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:52\n",
      " 40% (127 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:51\n",
      " 40% (128 of 313) |########              | Elapsed Time: 0:00:35 ETA:   0:00:51\n",
      " 41% (129 of 313) |#########             | Elapsed Time: 0:00:35 ETA:   0:00:51\n",
      " 41% (130 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:50\n",
      " 41% (131 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:50\n",
      " 42% (132 of 313) |#########             | Elapsed Time: 0:00:36 ETA:   0:00:50\n",
      " 42% (133 of 313) |#########             | Elapsed Time: 0:00:37 ETA:   0:00:50\n",
      " 42% (134 of 313) |#########             | Elapsed Time: 0:00:37 ETA:   0:00:49\n",
      " 43% (135 of 313) |#########             | Elapsed Time: 0:00:37 ETA:   0:00:49\n",
      " 43% (136 of 313) |#########             | Elapsed Time: 0:00:37 ETA:   0:00:49\n",
      " 43% (137 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:48\n",
      " 44% (138 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:48\n",
      " 44% (139 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:48\n",
      " 44% (140 of 313) |#########             | Elapsed Time: 0:00:38 ETA:   0:00:48\n",
      " 45% (141 of 313) |#########             | Elapsed Time: 0:00:39 ETA:   0:00:47\n",
      " 45% (142 of 313) |#########             | Elapsed Time: 0:00:39 ETA:   0:00:47\n",
      " 45% (143 of 313) |##########            | Elapsed Time: 0:00:39 ETA:   0:00:47\n",
      " 46% (144 of 313) |##########            | Elapsed Time: 0:00:40 ETA:   0:00:47\n",
      " 46% (145 of 313) |##########            | Elapsed Time: 0:00:40 ETA:   0:00:46\n",
      " 46% (146 of 313) |##########            | Elapsed Time: 0:00:40 ETA:   0:00:46\n",
      " 46% (147 of 313) |##########            | Elapsed Time: 0:00:41 ETA:   0:00:46\n",
      " 47% (148 of 313) |##########            | Elapsed Time: 0:00:41 ETA:   0:00:46\n",
      " 47% (149 of 313) |##########            | Elapsed Time: 0:00:41 ETA:   0:00:46\n",
      " 47% (150 of 313) |##########            | Elapsed Time: 0:00:42 ETA:   0:00:45\n",
      " 48% (151 of 313) |##########            | Elapsed Time: 0:00:42 ETA:   0:00:45\n",
      " 48% (152 of 313) |##########            | Elapsed Time: 0:00:42 ETA:   0:00:45\n",
      " 48% (153 of 313) |##########            | Elapsed Time: 0:00:42 ETA:   0:00:44\n",
      " 49% (154 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:44\n",
      " 49% (155 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:44\n",
      " 49% (156 of 313) |##########            | Elapsed Time: 0:00:43 ETA:   0:00:43\n",
      " 50% (157 of 313) |###########           | Elapsed Time: 0:00:44 ETA:   0:00:43\n",
      " 50% (158 of 313) |###########           | Elapsed Time: 0:00:44 ETA:   0:00:43\n",
      " 50% (159 of 313) |###########           | Elapsed Time: 0:00:44 ETA:   0:00:43\n",
      " 51% (160 of 313) |###########           | Elapsed Time: 0:00:44 ETA:   0:00:42\n",
      " 51% (161 of 313) |###########           | Elapsed Time: 0:00:45 ETA:   0:00:42\n",
      " 51% (162 of 313) |###########           | Elapsed Time: 0:00:45 ETA:   0:00:42\n",
      " 52% (163 of 313) |###########           | Elapsed Time: 0:00:45 ETA:   0:00:41\n",
      " 52% (164 of 313) |###########           | Elapsed Time: 0:00:45 ETA:   0:00:41\n",
      " 52% (165 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:41\n",
      " 53% (166 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:41\n",
      " 53% (167 of 313) |###########           | Elapsed Time: 0:00:46 ETA:   0:00:40\n",
      " 53% (168 of 313) |###########           | Elapsed Time: 0:00:47 ETA:   0:00:40\n",
      " 53% (169 of 313) |###########           | Elapsed Time: 0:00:47 ETA:   0:00:40\n",
      " 54% (170 of 313) |###########           | Elapsed Time: 0:00:47 ETA:   0:00:40\n",
      " 54% (171 of 313) |############          | Elapsed Time: 0:00:47 ETA:   0:00:39\n",
      " 54% (172 of 313) |############          | Elapsed Time: 0:00:48 ETA:   0:00:39\n",
      " 55% (173 of 313) |############          | Elapsed Time: 0:00:48 ETA:   0:00:39\n",
      " 55% (174 of 313) |############          | Elapsed Time: 0:00:48 ETA:   0:00:38\n",
      " 55% (175 of 313) |############          | Elapsed Time: 0:00:49 ETA:   0:00:38\n",
      " 56% (176 of 313) |############          | Elapsed Time: 0:00:49 ETA:   0:00:38\n",
      " 56% (177 of 313) |############          | Elapsed Time: 0:00:49 ETA:   0:00:38\n",
      " 56% (178 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:37\n",
      " 57% (179 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:37\n",
      " 57% (180 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:37\n",
      " 57% (181 of 313) |############          | Elapsed Time: 0:00:50 ETA:   0:00:37\n",
      " 58% (182 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:36\n",
      " 58% (183 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:36\n",
      " 58% (184 of 313) |############          | Elapsed Time: 0:00:51 ETA:   0:00:36\n",
      " 59% (185 of 313) |#############         | Elapsed Time: 0:00:52 ETA:   0:00:36\n",
      " 59% (186 of 313) |#############         | Elapsed Time: 0:00:52 ETA:   0:00:35\n",
      " 59% (187 of 313) |#############         | Elapsed Time: 0:00:52 ETA:   0:00:35\n",
      " 60% (188 of 313) |#############         | Elapsed Time: 0:00:53 ETA:   0:00:35\n",
      " 60% (189 of 313) |#############         | Elapsed Time: 0:00:53 ETA:   0:00:35\n",
      " 60% (190 of 313) |#############         | Elapsed Time: 0:00:53 ETA:   0:00:34\n",
      " 61% (191 of 313) |#############         | Elapsed Time: 0:00:53 ETA:   0:00:34\n",
      " 61% (192 of 313) |#############         | Elapsed Time: 0:00:54 ETA:   0:00:34\n",
      " 61% (193 of 313) |#############         | Elapsed Time: 0:00:54 ETA:   0:00:33\n",
      " 61% (194 of 313) |#############         | Elapsed Time: 0:00:54 ETA:   0:00:33\n",
      " 62% (195 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:33\n",
      " 62% (196 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:33\n",
      " 62% (197 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:32\n",
      " 63% (198 of 313) |#############         | Elapsed Time: 0:00:55 ETA:   0:00:32\n",
      " 63% (199 of 313) |#############         | Elapsed Time: 0:00:56 ETA:   0:00:32\n",
      " 63% (200 of 313) |##############        | Elapsed Time: 0:00:56 ETA:   0:00:31\n",
      " 64% (201 of 313) |##############        | Elapsed Time: 0:00:56 ETA:   0:00:31\n",
      " 64% (202 of 313) |##############        | Elapsed Time: 0:00:56 ETA:   0:00:31\n",
      " 64% (203 of 313) |##############        | Elapsed Time: 0:00:57 ETA:   0:00:31\n",
      " 65% (204 of 313) |##############        | Elapsed Time: 0:00:57 ETA:   0:00:30\n",
      " 65% (205 of 313) |##############        | Elapsed Time: 0:00:57 ETA:   0:00:30\n",
      " 65% (206 of 313) |##############        | Elapsed Time: 0:00:58 ETA:   0:00:30\n",
      " 66% (207 of 313) |##############        | Elapsed Time: 0:00:58 ETA:   0:00:29\n",
      " 66% (208 of 313) |##############        | Elapsed Time: 0:00:58 ETA:   0:00:29\n",
      " 66% (209 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:29\n",
      " 67% (210 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:29\n",
      " 67% (211 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:28\n",
      " 67% (212 of 313) |##############        | Elapsed Time: 0:00:59 ETA:   0:00:28\n",
      " 68% (213 of 313) |##############        | Elapsed Time: 0:01:00 ETA:   0:00:28\n",
      " 68% (214 of 313) |###############       | Elapsed Time: 0:01:00 ETA:   0:00:27\n",
      " 68% (215 of 313) |###############       | Elapsed Time: 0:01:00 ETA:   0:00:27\n",
      " 69% (216 of 313) |###############       | Elapsed Time: 0:01:01 ETA:   0:00:27\n",
      " 69% (217 of 313) |###############       | Elapsed Time: 0:01:01 ETA:   0:00:27\n",
      " 69% (218 of 313) |###############       | Elapsed Time: 0:01:01 ETA:   0:00:26\n",
      " 69% (219 of 313) |###############       | Elapsed Time: 0:01:01 ETA:   0:00:26\n",
      " 70% (220 of 313) |###############       | Elapsed Time: 0:01:02 ETA:   0:00:26\n",
      " 70% (221 of 313) |###############       | Elapsed Time: 0:01:02 ETA:   0:00:26\n",
      " 70% (222 of 313) |###############       | Elapsed Time: 0:01:02 ETA:   0:00:25\n",
      " 71% (223 of 313) |###############       | Elapsed Time: 0:01:03 ETA:   0:00:25\n",
      " 71% (224 of 313) |###############       | Elapsed Time: 0:01:03 ETA:   0:00:25\n",
      " 71% (225 of 313) |###############       | Elapsed Time: 0:01:03 ETA:   0:00:24\n",
      " 72% (226 of 313) |###############       | Elapsed Time: 0:01:04 ETA:   0:00:24\n",
      " 72% (227 of 313) |###############       | Elapsed Time: 0:01:04 ETA:   0:00:24\n",
      " 72% (228 of 313) |################      | Elapsed Time: 0:01:04 ETA:   0:00:24\n",
      " 73% (229 of 313) |################      | Elapsed Time: 0:01:04 ETA:   0:00:23\n",
      " 73% (230 of 313) |################      | Elapsed Time: 0:01:05 ETA:   0:00:23\n",
      " 73% (231 of 313) |################      | Elapsed Time: 0:01:05 ETA:   0:00:23\n",
      " 74% (232 of 313) |################      | Elapsed Time: 0:01:05 ETA:   0:00:22\n",
      " 74% (233 of 313) |################      | Elapsed Time: 0:01:05 ETA:   0:00:22\n",
      " 74% (234 of 313) |################      | Elapsed Time: 0:01:06 ETA:   0:00:22\n",
      " 75% (235 of 313) |################      | Elapsed Time: 0:01:06 ETA:   0:00:22\n",
      " 75% (236 of 313) |################      | Elapsed Time: 0:01:06 ETA:   0:00:21\n",
      " 75% (237 of 313) |################      | Elapsed Time: 0:01:06 ETA:   0:00:21\n",
      " 76% (238 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:21\n",
      " 76% (239 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:20\n",
      " 76% (240 of 313) |################      | Elapsed Time: 0:01:07 ETA:   0:00:20\n",
      " 76% (241 of 313) |################      | Elapsed Time: 0:01:08 ETA:   0:00:20\n",
      " 77% (242 of 313) |#################     | Elapsed Time: 0:01:08 ETA:   0:00:20\n",
      " 77% (243 of 313) |#################     | Elapsed Time: 0:01:08 ETA:   0:00:19\n",
      " 77% (244 of 313) |#################     | Elapsed Time: 0:01:08 ETA:   0:00:19\n",
      " 78% (245 of 313) |#################     | Elapsed Time: 0:01:09 ETA:   0:00:19\n",
      " 78% (246 of 313) |#################     | Elapsed Time: 0:01:09 ETA:   0:00:18\n",
      " 78% (247 of 313) |#################     | Elapsed Time: 0:01:09 ETA:   0:00:18\n",
      " 79% (248 of 313) |#################     | Elapsed Time: 0:01:10 ETA:   0:00:18\n",
      " 79% (249 of 313) |#################     | Elapsed Time: 0:01:10 ETA:   0:00:18\n",
      " 79% (250 of 313) |#################     | Elapsed Time: 0:01:10 ETA:   0:00:17\n",
      " 80% (251 of 313) |#################     | Elapsed Time: 0:01:10 ETA:   0:00:17\n",
      " 80% (252 of 313) |#################     | Elapsed Time: 0:01:11 ETA:   0:00:17\n",
      " 80% (253 of 313) |#################     | Elapsed Time: 0:01:11 ETA:   0:00:16\n",
      " 81% (254 of 313) |#################     | Elapsed Time: 0:01:11 ETA:   0:00:16\n",
      " 81% (255 of 313) |#################     | Elapsed Time: 0:01:12 ETA:   0:00:16\n",
      " 81% (256 of 313) |#################     | Elapsed Time: 0:01:12 ETA:   0:00:16\n",
      " 82% (257 of 313) |##################    | Elapsed Time: 0:01:12 ETA:   0:00:15\n",
      " 82% (258 of 313) |##################    | Elapsed Time: 0:01:12 ETA:   0:00:15\n",
      " 82% (259 of 313) |##################    | Elapsed Time: 0:01:13 ETA:   0:00:15\n",
      " 83% (260 of 313) |##################    | Elapsed Time: 0:01:13 ETA:   0:00:14\n",
      " 83% (261 of 313) |##################    | Elapsed Time: 0:01:13 ETA:   0:00:14\n",
      " 83% (262 of 313) |##################    | Elapsed Time: 0:01:13 ETA:   0:00:14\n",
      " 84% (263 of 313) |##################    | Elapsed Time: 0:01:14 ETA:   0:00:14\n",
      " 84% (264 of 313) |##################    | Elapsed Time: 0:01:14 ETA:   0:00:13\n",
      " 84% (265 of 313) |##################    | Elapsed Time: 0:01:14 ETA:   0:00:13\n",
      " 84% (266 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:13\n",
      " 85% (267 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:12\n",
      " 85% (268 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:12\n",
      " 85% (269 of 313) |##################    | Elapsed Time: 0:01:15 ETA:   0:00:12\n",
      " 86% (270 of 313) |##################    | Elapsed Time: 0:01:16 ETA:   0:00:12\n",
      " 86% (271 of 313) |###################   | Elapsed Time: 0:01:16 ETA:   0:00:11\n",
      " 86% (272 of 313) |###################   | Elapsed Time: 0:01:16 ETA:   0:00:11\n",
      " 87% (273 of 313) |###################   | Elapsed Time: 0:01:16 ETA:   0:00:11\n",
      " 87% (274 of 313) |###################   | Elapsed Time: 0:01:17 ETA:   0:00:10\n",
      " 87% (275 of 313) |###################   | Elapsed Time: 0:01:17 ETA:   0:00:10\n",
      " 88% (276 of 313) |###################   | Elapsed Time: 0:01:17 ETA:   0:00:10\n",
      " 88% (277 of 313) |###################   | Elapsed Time: 0:01:18 ETA:   0:00:10\n",
      " 88% (278 of 313) |###################   | Elapsed Time: 0:01:18 ETA:   0:00:09\n",
      " 89% (279 of 313) |###################   | Elapsed Time: 0:01:18 ETA:   0:00:09\n",
      " 89% (280 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:09\n",
      " 89% (281 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:09\n",
      " 90% (282 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:08\n",
      " 90% (283 of 313) |###################   | Elapsed Time: 0:01:19 ETA:   0:00:08\n",
      " 90% (284 of 313) |###################   | Elapsed Time: 0:01:20 ETA:   0:00:08\n",
      " 91% (285 of 313) |####################  | Elapsed Time: 0:01:20 ETA:   0:00:07\n",
      " 91% (286 of 313) |####################  | Elapsed Time: 0:01:20 ETA:   0:00:07\n",
      " 91% (287 of 313) |####################  | Elapsed Time: 0:01:21 ETA:   0:00:07\n",
      " 92% (288 of 313) |####################  | Elapsed Time: 0:01:21 ETA:   0:00:07\n",
      " 92% (289 of 313) |####################  | Elapsed Time: 0:01:21 ETA:   0:00:06\n",
      " 92% (290 of 313) |####################  | Elapsed Time: 0:01:21 ETA:   0:00:06\n",
      " 92% (291 of 313) |####################  | Elapsed Time: 0:01:22 ETA:   0:00:06\n",
      " 93% (292 of 313) |####################  | Elapsed Time: 0:01:22 ETA:   0:00:05\n",
      " 93% (293 of 313) |####################  | Elapsed Time: 0:01:22 ETA:   0:00:05\n",
      " 93% (294 of 313) |####################  | Elapsed Time: 0:01:22 ETA:   0:00:05\n",
      " 94% (295 of 313) |####################  | Elapsed Time: 0:01:23 ETA:   0:00:05\n",
      " 94% (296 of 313) |####################  | Elapsed Time: 0:01:23 ETA:   0:00:04\n",
      " 94% (297 of 313) |####################  | Elapsed Time: 0:01:23 ETA:   0:00:04\n",
      " 95% (298 of 313) |####################  | Elapsed Time: 0:01:24 ETA:   0:00:04\n",
      " 95% (299 of 313) |##################### | Elapsed Time: 0:01:24 ETA:   0:00:03\n",
      " 95% (300 of 313) |##################### | Elapsed Time: 0:01:24 ETA:   0:00:03\n",
      " 96% (301 of 313) |##################### | Elapsed Time: 0:01:24 ETA:   0:00:03\n",
      " 96% (302 of 313) |##################### | Elapsed Time: 0:01:25 ETA:   0:00:03\n",
      " 96% (303 of 313) |##################### | Elapsed Time: 0:01:25 ETA:   0:00:02\n",
      " 97% (304 of 313) |##################### | Elapsed Time: 0:01:25 ETA:   0:00:02\n",
      " 97% (305 of 313) |##################### | Elapsed Time: 0:01:26 ETA:   0:00:02\n",
      " 97% (306 of 313) |##################### | Elapsed Time: 0:01:26 ETA:   0:00:01\n",
      " 98% (307 of 313) |##################### | Elapsed Time: 0:01:26 ETA:   0:00:01\n",
      " 98% (308 of 313) |##################### | Elapsed Time: 0:01:26 ETA:   0:00:01\n",
      " 98% (309 of 313) |##################### | Elapsed Time: 0:01:27 ETA:   0:00:01\n",
      " 99% (310 of 313) |##################### | Elapsed Time: 0:01:27 ETA:   0:00:00\n",
      " 99% (311 of 313) |##################### | Elapsed Time: 0:01:27 ETA:   0:00:00\n",
      " 99% (312 of 313) |##################### | Elapsed Time: 0:01:28 ETA:   0:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:28 ETA:  00:00:00\n",
      "100% (313 of 313) |######################| Elapsed Time: 0:01:28 Time:  0:01:28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:14:09,939 - Eval - INFO - Avg accuracy Top 1: 70.956470 Avg accuracy Top 5: 89.586661 on validation Dataset\n",
      "70.9564696485623\n"
     ]
    }
   ],
   "source": [
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8)\n",
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=use_cuda)\n",
    "\n",
    "accuracy = ImageNetDataPipeline.evaluate(sim.model, use_cuda)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Depending on your settings you may have observed a slight gain in accuracy after applying CLE ad BC. Ofcourse, this was just an example. Please try this against the model of your choice and play with the number of samples to get the best results.\n",
    "\n",
    "Now the next step would be to take this model to target. For this purpose, we need to export the model with the updated weights without the fake quant ops. And also to export the encodings (scale/offset quantization parameters). AIMET QuantizationSimModel provides an export API for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:14:09,956 - Quant - WARNING - Exporting encodings to yaml will be deprecated in a future release. Ensure that your code can work with the exported files ending in \".encodings\" which are saved using json format. For the time being, if yaml export is needed, set aimet_common.utils.SAVE_TO_YAML to True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n",
      "[W shape_type_inference.cpp:1973] Warning: The shape inference of aimet_torch::CustomMarker type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 08:14:12,182 - Utils - INFO - successfully created onnx model with 99/100 node names updated\n",
      "2024-09-09 08:14:12,224 - Quant - WARNING - number of input quantizers: 1 available for layer: features.0.0 doesn't match with number of input tensors: 3\n",
      "2024-09-09 08:14:12,258 - Quant - INFO - Layers excluded from quantization: []\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./output/', exist_ok=True)\n",
    "dummy_input = dummy_input.cpu()\n",
    "sim.export(path='./output/', filename_prefix='resnet18_after_cle_bc', dummy_input=dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Hope this notebook was useful for you to understand how to use AIMET for performing Cross Layer Equalization (CLE) and Bias Correction (BC).\n",
    "\n",
    "Few additional resources\n",
    "- Refer to the AIMET API docs to know more details of the APIs and optional parameters\n",
    "- Refer to the other example notebooks to understand how to use AIMET post-training quantization techniques and QAT techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
