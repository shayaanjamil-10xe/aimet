{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import onnx\n",
    "\n",
    "import onnxruntime\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 8, 16, 32, 64, 128, 256]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0, 1] + [2**i for i in range(2, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-24 14:12:21,781] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2024-09-24 14:12:21,784] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "Loads checkpoint by local backend from path: /teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n",
      "09/24 14:12:27 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.53s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmcv.transforms import Compose\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip() for x in data]\n",
    "    return data\n",
    "\n",
    "def preprocess(test_pipeline, image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Calling this method across libraries will result\n",
    "        # in module unregistered error if not prefixed with mmdet.\n",
    "        test_pipeline[0].type = 'mmdet.LoadImageFromNDArray'\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    return test_pipeline(dict(img=image))\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, annotations_json_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_json = read_json(annotations_json_path)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_json['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_dict = self.annotations_json['images'][idx]\n",
    "        image_path = os.path.join(self.images_dir, image_dict['file_name'])\n",
    "        image_id = image_dict['id']\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            transformed_images = self.transform(image)\n",
    "        else:\n",
    "            transformed_images = image\n",
    "\n",
    "        return image_id, image_path, transformed_images\n",
    "\n",
    "\n",
    "# calibrationDataloader = DataLoader(calibrationDataset, batch_size=32, shuffle=True)\n",
    "import torch\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([640, 640]),  # Resize\n",
    "])\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIG_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco.py'\n",
    "WEIGHTS_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "EVAL_DATASET_SIZE = 5000\n",
    "CALIBRATION_DATASET_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ROOT_DATASET_DIR = '/teamspace/studios/this_studio/COCO'\n",
    "IMAGES_DIR = os.path.join(ROOT_DATASET_DIR, 'images')\n",
    "ANNOTATIONS_JSON_PATH = os.path.join(ROOT_DATASET_DIR, 'annotations/instances_val2017.json')\n",
    "# ANNOTATIONS_JSON_PATH = \"/home/shayaan/Desktop/aimet/my_mmdet/temp.json\"\n",
    "\n",
    "model = DetInferencer(model=CONFIG_PATH, weights=WEIGHTS_PATH, device=DEVICE)\n",
    "evalDataset = CustomImageDataset(images_dir=IMAGES_DIR, annotations_json_path=ANNOTATIONS_JSON_PATH, transform=transform)\n",
    "eval_data_loader = DataLoader(evalDataset, batch_size=BATCH_SIZE)\n",
    "calibration_images = read_txt('/teamspace/studios/this_studio/aimet/Examples/torch/quantization/calibration_image_ids.txt')\n",
    "calibration_data_loader = DataLoader(calibration_images, batch_size=BATCH_SIZE)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "from mmdet.models.utils import samplelist_boxtype2tensor\n",
    "from mmengine.registry import MODELS\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "test_evaluator = model.cfg.test_evaluator\n",
    "test_evaluator.type = 'mmdet.evaluation.CocoMetric' \n",
    "test_evaluator.dataset_meta = model.model.dataset_meta\n",
    "test_evaluator.ann_file = ANNOTATIONS_JSON_PATH\n",
    "test_evaluator = Compose(test_evaluator)\n",
    "\n",
    "collate_preprocessor = model.preprocess\n",
    "predict_by_feat = model.model.bbox_head.predict_by_feat\n",
    "rescale = True\n",
    "\n",
    "preprocessor = MODELS.build(model.cfg.model.data_preprocessor)\n",
    "def add_pred_to_datasample(data_samples, results_list):\n",
    "    for data_sample, pred_instances in zip(data_samples, results_list):\n",
    "        data_sample.pred_instances = pred_instances\n",
    "    samplelist_boxtype2tensor(data_samples)\n",
    "    return data_samples\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names: ['input_tensor.1']\n",
      "Output names: ['1965', '2085', '2203', '2027', '2147', '2266']\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"/teamspace/studios/this_studio/aimet/exported_models/bn_folded_int8_embedded/rtm_det_embedded.onnx\"\n",
    "\n",
    "# load onnx model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "# get the model's input and output names\n",
    "input_names = [input.name for input in onnx_model.graph.input]\n",
    "output_names = [output.name for output in onnx_model.graph.output]\n",
    "# print the input and output names\n",
    "print(\"Input names:\", input_names)\n",
    "print(\"Output names:\", output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_ignore = ['backbone.stage2.1.blocks.0.conv2.depthwise_conv', 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_7', 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.conv', 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_21', 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_30', 'neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_37', 'neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_44', 'neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_51', 'neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_58']\n",
    "modules_to_ignore\n",
    "\n",
    "modules = [\"module_batch_norm_14\", \"module_batch_norm_7\", \"module_batch_norm_21\", \"module_batch_norm_30\", \"module_batch_norm_37\", \"module_batch_norm_44\", \"module_batch_norm_51\", \"module_batch_norm_58\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdedded_modules = [\"/conv/_module_to_wrap_14/Conv\", \"/conv_14/QuantizeLinear_1\", \"/conv_14/DequantizeLinear_1\", \"/conv/_module_to_wrap_7/Conv\", \"/conv_7/QuantizeLinear_1\", \"/conv_7/DequantizeLinear_1\", \"/conv/_module_to_wrap_21/Conv\", \"/conv_21/QuantizeLinear_1\", \"/conv_21/DequantizeLinear_1\", \"/conv/_module_to_wrap_30/Conv\", \"/conv_30/QuantizeLinear_1\", \"/conv_30/DequantizeLinear_1\", \"/conv/_module_to_wrap_37/Conv\", \"/conv_37/QuantizeLinear_1\", \"/conv_37/DequantizeLinear_1\", \"/conv/_module_to_wrap_44/Conv\", \"/conv_44/QuantizeLinear_1\", \"/conv_44/DequantizeLinear_1\", \"/conv/_module_to_wrap_51/Conv\", \"/conv_51/QuantizeLinear_1\", \"/conv_51/DequantizeLinear_1\", \"/conv/_module_to_wrap_58/Conv\", \"/conv_58/QuantizeLinear_1\", \"/conv_58/DequantizeLinear_1\", \"/conv/_module_to_wrap_15/Conv\",  \"/conv_15/QuantizeLinear_1\", \"/conv_15/DequantizeLinear_1\", ]\n",
    "len(embdedded_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a681c4c7aa4b59a8e8f49391d7232b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.name='/conv/_module_to_wrap_21/Conv' node.output=['/conv/_module_to_wrap_21/Conv_output_0']\n",
      "node.name='/conv_21/Constant_2' node.output=['/conv_21/Constant_2_output_0']\n",
      "node.name='/conv_21/Constant_3' node.output=['/conv_21/Constant_3_output_0']\n",
      "node.name='/conv_21/QuantizeLinear_1' node.output=['/conv_21/QuantizeLinear_1_output_0']\n",
      "node.name='/conv_21/DequantizeLinear_1' node.output=['/conv_21/DequantizeLinear_1_output_0']\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./temp.onnx\"\n",
    "b = False\n",
    "idx = 0\n",
    "for node in tqdm(onnx_model.graph.node):\n",
    "    if \"/conv/_module_to_wrap_21/Conv\" in node.name:\n",
    "        print(f\"{node.name=} {node.output=}\")\n",
    "        b = True\n",
    "        continue\n",
    "    if b:\n",
    "        print(f\"{node.name=} {node.output=}\")\n",
    "        idx += 1\n",
    "        if idx == 4:\n",
    "            break\n",
    "# output_names = [\"/conv_14/QuantizeLinear_1_output_0\"]\n",
    "# onnx.utils.extract_model(onnx_model_path, output_path, input_names, output_names)\n",
    "\n",
    "# ort_session = onnxruntime.InferenceSession(output_path, providers=[\"CPUExecutionProvider\"])\n",
    "# output_name = ort_session.get_outputs()[0].name\n",
    "# input_name = ort_session.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./temp.onnx\"\n",
    "obj = {}\n",
    "\n",
    "image_path = [\"/teamspace/studios/this_studio/COCO/images/000000000139.jpg\"]\n",
    "\n",
    "for node in tqdm(onnx_model.graph.node):\n",
    "    print(node.name)\n",
    "    if any([m in node.name for m in modules_to_ignore]):\n",
    "        print(node.output[0])\n",
    "        output_names = node.output\n",
    "        onnx.utils.extract_model(onnx_model_path, output_path, input_names, output_names)\n",
    "        \n",
    "        ort_session = onnxruntime.InferenceSession(output_path)\n",
    "        output_name = ort_session.get_outputs()[0].name\n",
    "        input_name = ort_session.get_inputs()[0].name\n",
    "        \n",
    "        # pre_processed = collate_preprocessor(inputs=image_path, batch_size=1)\n",
    "        # _, data = list(pre_processed)[0]\n",
    "        # data = preprocessor(data, False)\n",
    "        # input_data = data['inputs'].numpy()\n",
    "\n",
    "        # outputs = ort_session.run([output_name], {input_name: input_data})[0]\n",
    "        # obj[node.output[0]] = outputs\n",
    "        # break\n",
    "    \n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import torch\n",
    "\n",
    "input_path = onnx_model_path\n",
    "output_path = \"./temp.onnx\"\n",
    "output_names = [\"/module_batch_norm_14/BatchNormalization_output_0\"]\n",
    "\n",
    "onnx.utils.extract_model(input_path, output_path, input_names, output_names)\n",
    "\n",
    "temp_onnx_model = onnx.load(\"./temp.onnx\")\n",
    "# run inference\n",
    "ort_session = onnxruntime.InferenceSession(\"./temp.onnx\")\n",
    "# get the output name\n",
    "output_name = ort_session.get_outputs()[0].name\n",
    "# get the input name\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "input_data = torch.randn(1, 3, 640, 640).numpy()\n",
    "\n",
    "ort_session.run([output_name], {input_name: input_data})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
